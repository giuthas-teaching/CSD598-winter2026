@book{--,
  type = {Book}
}

@misc{--a,
  url = {https://www.internationalphoneticassociation.org/icphs-proceedings/ICPhS2023/FINAL-PROCEEDINGS_TOC_HTML.html},
  urldate = {2024-09-04},
  file = {/home/jpalo/Zotero/storage/AMR94SWP/FINAL-PROCEEDINGS_TOC_HTML.html}
}

@misc{-Analysisbysynthesis-2013,
  title = {Analysis-by-Synthesis},
  year = 2013,
  url = {http://www.glottopedia.org/index.php/Analysis-by-synthesis}
}

@misc{-ArticulatoryUniformityArticulatory-,
  title = {Articulatory {{Uniformity}} through {{Articulatory Reuse}}: {{Insights}} from an {{Ultrasound Study}} of {{S\=uzh\=ou Chinese}} - {{ProQuest}}},
  url = {https://www.proquest.com/openview/f6e14fa5fec803d1052890c57ac98121/1?pq-origsite=gscholar&cbl=18750},
  urldate = {2025-05-28},
  file = {/home/jpalo/Zotero/storage/SENBRXCI/1.html}
}

@misc{-ArticulographyElectromagneticSystems-2015,
  title = {Articulography - Electromagnetic Systems for Visualization of Speech Movement inside the Mouth},
  year = 2015
}

@misc{-ClinicalAudiologySpeech-2015,
  title = {Clinical {{Audiology}}, {{Speech}} and {{Language Research Centre}}},
  year = 2015,
  annotation = {Published: Website, accessed 11 Feb 2015}
}

@misc{-FrontiersRoleTemporal-,
  title = {Frontiers \textbar{} {{The Role}} of {{Temporal Modulation}} in {{Sensorimotor Interaction}}},
  url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2019.02608/full},
  urldate = {2024-04-16},
  file = {/home/jpalo/Zotero/storage/FGDFH9CV/full.html}
}

@misc{-Presentation-2015,
  title = {{{Presentation}}\textregistered},
  year = 2015,
  howpublished = {Neurobehavioral systems}
}

@book{-ProceedingsUltrafestVI-2013,
  title = {Proceedings of {{Ultrafest VI}}},
  year = 2013,
  publisher = {Queen Margaret University}
}

@misc{-ResearchExcellenceFramework-2014,
  title = {Research {{Excellence Framework}} 2014},
  year = 2014,
  annotation = {Published: Website, accessed 11 Feb 2015}
}

@misc{-SpectrogramSegmentingPractice-,
  title = {Spectrogram Segmenting Practice},
  url = {https://home.cc.umanitoba.ca/~krussll/phonetics/practice/spectrogram-segmentation.html},
  urldate = {2025-08-17},
  file = {/home/jpalo/Zotero/storage/NCVIFURH/spectrogram-segmentation.html}
}

@article{2020SciPy-NMeth,
  title = {{{SciPy}} 1.0: {{Fundamental}} Algorithms for Scientific Computing in Python},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  year = 2020,
  journal = {Nature Methods},
  volume = {17},
  pages = {261--272},
  doi = {10.1038/s41592-019-0686-2}
}

@inproceedings{AaltoEtAl-EffectsUltrasoundBiofeedback-2024,
  title = {Effects of an Ultrasound Biofeedback Session on Maximal Tongue Movements},
  booktitle = {Proceedings of {{ISSP}} 2024},
  author = {Aalto, Eija M. A. and Yoshida, Minory and M{\'e}nard, Lucie and Cardoso, Walcir and Laporte, Catherine},
  year = 2024,
  pages = {63--66},
  address = {Autrans, France},
  file = {/home/jpalo/Zotero/storage/JLRG3E5V/Aalto et al. - 2024 - Effects of an ultrasound biofeedback session on maximal tongue movements.pdf}
}

@inproceedings{AaltoEtAl-EstimatesMeasurementArticulatory-2011,
  title = {Estimates for the Measurement and Articulatory Error in {{MRI}} Data from Sustained Phonation},
  booktitle = {Proceedings of {{ICPhS}} 2011},
  author = {Aalto, D. and Malinen, J. and Palo, P. and Saunavaara, J. and Vainio, M.},
  year = 2011,
  pages = {180--183},
  address = {Hong Kong, China}
}

@inproceedings{AaltoEtAl-LFpulseSimpleGlottal-2009,
  title = {A {{LF-pulse}} from a Simple Glottal Flow Model},
  booktitle = {{{MAVEBA}} 2009},
  author = {Aalto, A. and Alku, P. and Malinen, J.},
  year = 2009,
  pages = {199--202},
  address = {Florence, Italy}
}

@inproceedings{AaltoEtAl-RecordingSpeechSound-2011,
  title = {Recording Speech Sound and Articulation in {{MRI}}},
  booktitle = {Biodevices 2011},
  author = {Aalto, D. and Malinen, J. and Palo, P. and Aaltonen, O. and Vainio, M. and Happonen, R.-P. and Parkkola, R. and Saunavaara, J.},
  year = 2011,
  pages = {168--173},
  address = {Rome, Italy}
}

@unpublished{AaltoPalo-ComparisonTongueMovement-2023,
  title = {Comparison of Tongue Movement Recording Methods -- a Phonetical View},
  author = {Aalto, D. and Palo, P.},
  year = 2023
}

@article{AbbsGracco-SensorimotorActionsControl-1983,
  title = {Sensorimotor Actions in the Control of Multi-Movement Speech Gestures},
  author = {Abbs, J. H. and Gracco, V. L.},
  year = 1983,
  journal = {Trends in Neurosciences},
  volume = {6},
  number = {9},
  pages = {391--395}
}

@book{Adams-ElamaMaailmankaikkeusJa-1982,
  title = {El\"am\"a, Maailmankaikkeus -- Ja Kaikki},
  author = {Adams, Douglas},
  year = 1982,
  series = {Linnunradan K\"asikirja Liftareille},
  volume = {3}
}

@book{Adams-EnimmakseenHarmiton-1992,
  title = {Enimm\"akseen Harmiton},
  author = {Adams, Douglas},
  year = 1992,
  series = {Linnunradan K\"asikirja Liftareille},
  volume = {5}
}

@book{Adams-MaailmanlopunRavintola-1980,
  title = {Maailmanlopun Ravintola},
  author = {Adams, Douglas},
  year = 1980,
  series = {Linnunradan K\"asikirja Liftareille},
  volume = {2}
}

@book{Adams-TerveJaKiitos-1984,
  title = {Terve, Ja Kiitos Kaloista},
  author = {Adams, Douglas},
  year = 1984,
  series = {Linnunradan K\"asikirja Liftareille},
  volume = {4}
}

@article{AdlerEtAl-UseUltrasoundRemediation-2007,
  title = {The {{Use}} of {{Ultrasound}} in {{Remediation}} of {{North American English}} /r/ in 2 {{Adolescents}}},
  author = {Adler, -Bock Marcy and Bernhardt, Barbara May and Gick, Bryan and Bacsfalvi, Penelope},
  year = 2007,
  month = may,
  journal = {American Journal of Speech-Language Pathology},
  volume = {16},
  number = {2},
  pages = {128--139},
  publisher = {American Speech-Language-Hearing Association},
  doi = {10.1044/1058-0360(2007/017)},
  url = {https://pubs.asha.org/doi/full/10.1044/1058-0360%282007/017%29},
  urldate = {2024-09-16},
  abstract = {Purpose  Ultrasound can provide images of the tongue during speech production. The present study set out to examine the potential utility of ultrasound in remediation of North American English /r/. Method  The participants were 2 Canadian English-speaking adolescents who had not yet acquired /r/. The study included an initial period without ultrasound and 13 treatment sessions, each 1 hr long, using ultrasound. Speech samples were recorded at screening and immediately before and after treatment. Samples were analyzed acoustically and with listener judgments. Ultrasound images were obtained before, during, and after the treatment period. Results  Three speech-language pathologists unfamiliar with the participants rated significantly more posttreatment tokens as accurate [r]s in single words and some phrases. Acoustic analyses showed an expected lowering of the third formant after treatment. A qualitative observation of posttreatment ultrasound images for accurate [r] tokens showed tongue shapes to be more similar to those of typical adults than had been observed before treatment. Participants needed continued practice of their newly acquired skills in sentences and conversation. Conclusion  Two-dimensional dynamic ultrasound appears to have potential utility for remediation of /r/ in speakers with residual /r/ impairment. Further research is needed with larger numbers of participants to establish the relative efficacy of ultrasound in treatment.},
  keywords = {remediation of /r/,residual articulation disorder,ultrasound}
}

@inproceedings{Al-TamimiPalo-DynamicsTongueContour-2023,
  title = {Dynamics of the Tongue Contour in the Production of Guttural Consonants in Levantine Arabic},
  booktitle = {International Conference of Phonetic Sciences ({{ICPhS}} 2023)},
  author = {{Al-Tamimi}, J. and Palo, P.},
  year = 2023,
  address = {Prague}
}

@unpublished{Al-TamimiPalo-QuantifyingGradienceEpilaryngeal-2020,
  title = {Quantifying Gradience of Epilaryngeal Constriction in {{Levantine Arabic}} ``Gutturals'': {{A Generalized Additive Modelling}} Approach to Ultrasound Tongue Contours},
  author = {{Al-Tamimi}, J. and Palo, P.},
  year = 2020
}

@inproceedings{Al-TamimiPalo-RetractionWholeTongue-2024,
  title = {Retraction of the Whole Tongue Induced by Pharyngealisation in {{Levantine Arabic}}: {{A}} between-Subject Account Using {{Static}} and {{Dynamic PCA}} and {{GAMMs}}},
  booktitle = {Ultrafest 2024},
  author = {{Al-Tamimi}, J. and Palo, Pertti},
  year = 2024,
  address = {Autrans, France}
}

@unpublished{Al-TamimiPalo-TongueContoursGuttural-2024,
  title = {Tongue Contours in Guttural Consonants in Levantine Arabic: {{A}} Generalised Additive Modelling Approach (Provisional Title)},
  author = {{Al-Tamimi}, J. and Palo, P.},
  year = 2024
}

@article{AlkuEtAl-PerformanceGlottalInverse-2006,
  title = {Performance of Glottal Inverse Filtering as Tested by Aeroelastic Modelling of Phonation and {{FE}} Modelling of Vocal Tract},
  author = {{Alku} and {Hor\'a\v cek} and {Airas} and {Griffond-Boitier} and {Laukkanen}},
  year = 2006,
  journal = {Acta Acustica}
}

@article{AllenStrong-ModelSynthesisNatural-1985,
  title = {A Model for the Synthesis of Natural Sounding Vowels},
  author = {Allen, D. R. and Strong, W. J.},
  year = 1985,
  journal = {Journal of the Acoustical Society of America},
  volume = {78},
  number = {1},
  pages = {58--69}
}

@article{AnanthapadmanabhaFant-CalculationTrueGlottal-1982,
  title = {Calculation of True Glottal Flow and Its Components},
  author = {Ananthapadmanabha, T. V. and Fant, G.},
  year = 1982,
  journal = {Speech Communication},
  volume = {1},
  pages = {167--184}
}

@misc{ArticulateInstruments-ArticulateAssistantAdvanced-2012,
  title = {Articulate {{Assistant Advanced User Guide}}: {{Version}} 2.14},
  author = {Articulate Instruments},
  year = 2012,
  address = {Edinburgh, UK},
  howpublished = {Articulate Instruments Ltd}
}

@misc{ArticulateInstrumentsLtd-UltrasoundStabilisationHeadset-2008,
  title = {Ultrasound {{Stabilisation Headset Users Manual}}: {{Revision}} 1.4},
  author = {{\textbraceleft}Articulate Instruments Ltd{\textbraceright}},
  year = 2008,
  address = {Edinburgh, UK},
  howpublished = {Articulate Instruments Ltd}
}

@inproceedings{AtalSchroeder-PredictiveCodingSpeech-1967,
  title = {Predictive Coding of Speech Signals},
  booktitle = {Proc. {{IEEE Conference}} on {{Communication}} and {{Processing}}},
  author = {Atal, B. S. and Schroeder, M. R.},
  year = 1967,
  pages = {360--361}
}

@inproceedings{AubinMenard-CompensationLabialPerturbation-2006,
  title = {Compensation for a Labial Perturbation: {{An}} Acoustic and Articulatory Study of Child and Adult {{French}} Speakers},
  booktitle = {Proceedings of the 7th International Seminar on Speech Production},
  author = {Aubin, J{\'e}r{\^o}me and M{\'e}nard, Lucie},
  editor = {Yehia, H. C. and Demolin, D. and Laboissi{\`e}re, R.},
  year = 2006,
  pages = {209--216},
  address = {Ubatuba, Brazil},
  file = {/home/jpalo/Zotero/storage/2UBFGMR6/Aubin and Ménard - 2006 - Compensation for a labial perturbation An acoustic and articulatory study of child and adult French.pdf}
}

@book{Baayen-AnalyzingLinguisticData-2008,
  title = {Analyzing {{Linguistic Data}}, {{A Practical Introduction}} to {{Statistics}} Using {{R}}},
  author = {Baayen, R. H.},
  year = 2008,
  publisher = {Cambridge University Press},
  address = {Cambridge}
}

@article{BaayenEtAl-MixedeffectsModelingCrossed-2008,
  title = {Mixed-Effects Modeling with Crossed Random Effects for Subjects and Items},
  author = {Baayen, R.H. and Davidson, D.J. and Bates, D.M.},
  year = 2008,
  journal = {Journal of Memory and Language},
  volume = {59},
  pages = {390--412}
}

@article{BacsfalviBernhardt-LongtermOutcomesSpeech-2011,
  title = {Long-Term Outcomes of Speech Therapy for Seven Adolescents with Visual Feedback Technologies: Ultrasound and Electropalatography},
  shorttitle = {Long-Term Outcomes of Speech Therapy for Seven Adolescents with Visual Feedback Technologies},
  author = {Bacsfalvi, Penelope and Bernhardt, Barbara May},
  year = 2011,
  journal = {Clinical Linguistics \& Phonetics},
  volume = {25},
  number = {11-12},
  pages = {1034--1043},
  issn = {1464-5076},
  doi = {10.3109/02699206.2011.618236},
  abstract = {This follow-up study investigated the speech production of seven adolescents and young adults with hearing impairment 2-4 years after speech intervention with ultrasound and electropalatography. Perceptual judgments by seven expert listeners revealed that five out of seven speakers either continued to generalize post-treatment or maintained their level of performance post-treatment. Targets included fricatives, vowels and the rhotic /ɹ/. Speakers ranged in age from 14 to 19 years. Listeners were considered to be expert listeners. All listeners had extensive backgrounds in phonetics and phonology and were speech-language pathologists. This long-term investigation revealed that speech habilitation with visual feedback tools as adjuncts to therapy appeared to have lasting effects. The implications for habilitation include reduced therapy times and outcomes not previously possible.},
  langid = {english},
  pmid = {22106893},
  keywords = {Adolescent,Adult,Cochlear Implantation,Electrodiagnosis,Feedback Sensory,Female,Follow-Up Studies,Hearing Loss,Humans,Male,Observer Variation,Palate,Random Allocation,Rehabilitation,Speech Disorders,Speech Intelligibility,Speech Therapy,Treatment Outcome,Ultrasonography,Young Adult}
}

@article{BadinEtAl-ThreedimensionalLinearArticulatory-2002,
  title = {Three-Dimensional Linear Articulatory Modeling of Tongue, Lips and Face Based on Mri Data and Video Images},
  author = {Badin, P. and Bailly, G. and Raybaudi, M. and Segebarth, C.},
  year = 2002,
  journal = {Journal of Phonetics},
  volume = {30},
  pages = {533--553}
}

@article{BaerEtAl-AnalysisVocalTract-1991,
  title = {Analysis of Vocal Tract Shape and Dimensions Using Magnetic Resonance Imaging},
  author = {Baer, T. and Gore, J. C. and Gracco, L. W. and Nye, P. W.},
  year = 1991,
  journal = {Journal of the Acoustical Society of America},
  volume = {90},
  number = {2},
  pages = {799--828}
}

@article{BaerEtAl-ApplicationMRIAnalysis-1987,
  title = {Application of {{MRI}} to the {{Analysis}} of {{Speech Production}}},
  author = {Baer, T. and Gore, J. C. and Boyce, S. and Nye, P. W.},
  year = 1987,
  journal = {Magnetic Resonance Imaging},
  volume = {5},
  pages = {1--7}
}

@article{Bakst-PalateShapeInfluence-2021,
  title = {Palate Shape Influence Depends on the Segment: {{Articulatory}} and Acoustic Variability in {{American English}} /ɹ/ and /s/},
  shorttitle = {Palate Shape Influence Depends on the Segment},
  author = {Bakst, Sarah},
  year = 2021,
  journal = {The Journal of the Acoustical Society of America},
  volume = {149},
  number = {2},
  pages = {960--971},
  issn = {0001-4966},
  doi = {10.1121/10.0003379},
  url = {https://doi.org/10.1121/10.0003379},
  urldate = {2024-05-17},
  abstract = {This ultrasound and acoustics study of American English /ɹ/ and /s/ investigates whether variability in production as measured in the midsagittal plane is related to individual differences in the shape of the hard palate in the coronal plane. Both token-to-token variability and variability between different phonetic contexts were investigated. While no direct relationship was found between palate flatness and articulatory variability, a secondary analysis revealed that speakers' articulatory variability for one segment was related to their variability in the other. Speakers with flatter palates tended towards lower articulatory variability scores, but speakers with more domed palates showed both high and low variability scores.},
  file = {/home/jpalo/Zotero/storage/ZTXR9CMB/Palate-shape-influence-depends-on-the-segment.html}
}

@book{Banks-Bridge-1986,
  title = {The {{Bridge}}},
  author = {Banks, Iain},
  year = 1986
}

@book{Banks-Business-1999,
  title = {The {{Business}}},
  author = {Banks, Iain},
  year = 1999
}

@book{Banks-CanalDreams-1989,
  title = {Canal {{Dreams}}},
  author = {Banks, Iain},
  year = 1989
}

@book{Banks-CrowRoad-1993,
  title = {The {{Crow Road}}},
  author = {Banks, Iain},
  year = 1993
}

@book{Banks-SteepApproachGarbadale-2007,
  title = {The {{Steep Approach}} to {{Garbadale}}},
  author = {Banks, Iain},
  year = 2007
}

@book{Banks-WaspFactory-1984,
  title = {The {{Wasp Factory}}},
  author = {Banks, Iain},
  year = 1984
}

@book{Banks-Whit-1995,
  title = {Whit},
  author = {Banks, Iain},
  year = 1995
}

@inproceedings{BarbosaEtAl-LinguisticallyValidMovement-2008,
  title = {Linguistically {{Valid Movement Behavior Measured Non-Invasively}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Auditory-Visual Speech Processing}}},
  author = {Barbosa, A.V. and Yehia, H.C. and {Vatikiotis-Bateson}, E.},
  editor = {G{\"o}cke, R. and Lucey, P. and Lucey, S.},
  year = 2008,
  pages = {173--177}
}

@article{BatesEtAl-FittingLinearMixedEffects-2015,
  title = {Fitting {{Linear Mixed-Effects Models Using}} Lme4},
  author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
  year = 2015,
  journal = {Journal of Statistical Software},
  volume = {67},
  number = {1},
  pages = {1--48}
}

@article{BernhardtEtAl-UltrasoundSpeechTherapy-2005,
  title = {Ultrasound in Speech Therapy with Adolescents and Adults},
  author = {Bernhardt, B. and Gick, B. and Bacsfalvi, P. and {Adler-Bock}, M.},
  year = 2005,
  journal = {Clinical Linguistics \& Phonetics},
  volume = {19},
  number = {6-7},
  pages = {605--617}
}

@article{BeverPoeppel-AnalysisSynthesisRe-2010,
  title = {Analysis by {{Synthesis}}: {{A}} ({{Re-}}){{Emerging Program}} of {{Research}} for {{Language}} and {{Vision}}},
  author = {Bever, T. G. and Poeppel, D.},
  year = 2010,
  journal = {Biolinguistics},
  volume = {4},
  number = {2--3},
  pages = {174--200}
}

@inproceedings{BirdEtAl-OpticFlowAnalysis-2010,
  title = {An Optic Flow Analysis of Tongue Movement in {{SENCOTEN}} /{{qV}}/ and /{{Vq}}/ Sequences},
  booktitle = {Ultrafest {{V}}},
  author = {Bird, S. and Moisik, S. R. and Leonard, J. and Smith, S.},
  year = 2010,
  month = mar,
  address = {Ultrafest V, Haskins Lab, New Haven, Connecticut}
}

@phdthesis{Birkholz-3DArtikulatorischeSprachsynthese-2005,
  title = {{{3D-Artikulatorische Sprachsynthese}}},
  author = {Birkholz, P.},
  year = 2005,
  address = {Berlin},
  school = {University of Rostock}
}

@article{BirkholzEtAl-ModelBasedReproductionArticulatory-2010,
  title = {Model-{{Based Reproduction}} of {{Articulatory Trajectories}} for {{Consonant-Vowel Sequences}}},
  author = {Birkholz, P. and Kr{\"o}ger, B. J. and {Neuschaefer-Rube}, C.},
  year = 2010,
  journal = {IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING},
  volume = {19},
  number = {5},
  pages = {1422--1433}
}

@phdthesis{Blackburn-ArticulatoryMethodsSpeech-1996,
  title = {Articulatory {{Methods}} for {{Speech Production}} and {{Recognition}}},
  author = {Blackburn, C. S.},
  year = 1996,
  school = {Trinity College Cambridge \& Cambridge University Engineering Department}
}

@phdthesis{Boersma-FunctionalPhonology-1998,
  title = {Functional {{Phonology}}},
  author = {Boersma, P.},
  year = 1998,
  school = {University of Amsterdam, Netherlands}
}

@misc{BoersmaWeenink-PraatDoingPhonetics-2010,
  title = {Praat: Doing Phonetics by Computer [{{Computer}} Program]},
  author = {Boersma, Paul and Weenink, David},
  year = 2010
}

@article{BogelsLevinson-UltrasoundMeasurementsInteractive-2023,
  title = {Ultrasound Measurements of Interactive Turn-Taking in Question-Answer Sequences: {{Articulatory}} Preparation Is Delayed but Not Tied to the Response},
  author = {B{\"o}gels, Sara and Levinson, Stephen C.},
  year = 2023,
  journal = {PLoS ONE},
  volume = {18},
  number = {7},
  pages = {e0276470},
  doi = {10.1371/journal.pone.0276470}
}

@article{BohlandEtAl-NeuralRepresentationsMechanisms-2010,
  title = {Neural Representations and Mechanisms for the Performance of Simple Speech Sequences},
  author = {Bohland, J. W. and Bullock, D. and Guenther, F. H.},
  year = 2010,
  journal = {Journal of Cognitive Neuroscience},
  volume = {22},
  number = {7},
  pages = {1504--1529}
}

@misc{Boyd-Wallis-ParkParkinson-2018,
  title = {Round the {{Park}} for {{Parkinson}}'s},
  author = {{Boyd-Wallis}, Will},
  year = 2018,
  url = {https://www.cureparkinsons.org.uk/willboydwallis},
  urldate = {2020-06-23}
}

@article{BreschEtAl-SynchronizedNoiserobustAudio-2006,
  title = {Synchronized and Noise-Robust Audio Recordings during Realtime Magnetic Resonance Imaging Scans ({{L}})},
  author = {Bresch, E. and Nielsen, J. and Nayak, K. and Narayanan, S.},
  year = 2006,
  journal = {Journal of the Acoustical Society of America},
  volume = {120},
  number = {4},
  pages = {1791--1794}
}

@article{BrowmanGoldstein-ArticulatoryGesturesPhonological-1990,
  title = {Articulatory Gestures as Phonological Units.},
  author = {Browman, C. P. and Goldstein, L.},
  year = 1990,
  journal = {Phonology},
  volume = {6},
  pages = {201--251}
}

@article{BrowmanGoldstein-ArticulatoryPhonologyOverview-1992,
  title = {Articulatory Phonology: {{An}} Overview},
  author = {Browman, C. P. and Goldstein, L.},
  year = 1992,
  journal = {Phonetica},
  volume = {49},
  pages = {155--180}
}

@article{BrowmanGoldstein-NotesSyllableStructure-1988,
  title = {Some Notes on Syllable Structure in {{Articulatory Phonology}}},
  author = {Browman, C. P. and Goldstein, L.},
  year = 1988,
  journal = {Phonetica},
  volume = {45},
  pages = {140--155}
}

@article{ByunEtAl-RetroflexBunchedTreatment-2014,
  title = {Retroflex {{Versus Bunched}} in {{Treatment}} for {{Rhotic Misarticulation}}: {{Evidence From Ultrasound Biofeedback Intervention}}},
  shorttitle = {Retroflex {{Versus Bunched}} in {{Treatment}} for {{Rhotic Misarticulation}}},
  author = {Byun, Tara McAllister and Hitchcock, Elaine R. and Swartz, Michelle T.},
  year = 2014,
  journal = {Journal of speech, language, and hearing research : JSLHR},
  volume = {57},
  number = {6},
  pages = {2116--2130},
  issn = {1092-4388},
  doi = {10.1044/2014_JSLHR-S-14-0034},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4294189/},
  urldate = {2024-04-09},
  abstract = {Purpose To document the efficacy of ultrasound biofeedback treatment for misarticulation of the North American English rhotic in children. Because of limited progress in the first cohort, a series of two closely related studies was conducted in place of a single study. The studies differed primarily in the nature of tongue-shape targets (e.g., retroflex, bunched) cued during treatment. Method Eight participants received 8 weeks of individual ultrasound biofeedback treatment targeting rhotics. In Study 1, all 4 participants were cued to match a bunched tongue-shape target. In Study 2, participants received individualized cues aimed at eliciting the tongue shape most facilitative of perceptually correct rhotics. Results Participants in Study 1 showed only minimal treatment effects. In Study 2, all participants demonstrated improved production of rhotics in untreated words produced without biofeedback, with large to very large effect sizes. Conclusions The results of Study 2 indicate that with proper parameters of treatment, ultrasound biofeedback can be a highly effective intervention for children with persistent rhotic errors. In addition, qualitative comparison of Studies 1 and 2 suggests that treatment for the North American English rhotic should include opportunities to explore different tongue shapes, to find the most facilitative variant for each individual speaker.},
  pmcid = {PMC4294189},
  pmid = {25088034},
  file = {/home/jpalo/Zotero/storage/BRAAF2V2/Byun et al. - 2014 - Retroflex Versus Bunched in Treatment for Rhotic M.pdf}
}

@misc{Campbell-MairiCampbellPulse-,
  title = {Mairi {{Campbell}}: {{Pulse}}},
  author = {Campbell, Mairi},
  url = {https://mairicampbell.scot/pulse/},
  urldate = {2020-07-16}
}

@article{CarlsenEtAl-PreparationVoluntaryMovement-2012,
  title = {Preparation for Voluntary Movement in Healthy and Clinical Populations: Evidence from Startle},
  author = {Carlsen, A. N. and Maslovat, D. and Franks, I. M.},
  year = 2012,
  journal = {Clinical Neurophysiology},
  volume = {123},
  pages = {21--33}
}

@phdthesis{Carr-VideofluorographicInvestigationTongue-1978,
  title = {A Videofluorographic Investigation of Tongue and Throat Positions in Playing Flute, Oboe, Clarinet, Bassoon, and Saxophone},
  author = {Carr, W. E., {\relax Jr}.},
  year = 1978,
  school = {UNIVERSITY OF SOUTHERN CALIFORNIA}
}

@article{Cattell-TIMETAKENCEREBRAL-1886,
  title = {{{THE TIME TAKEN UP BY CEREBRAL OPERATIONS}}},
  author = {Cattell, James McKeen},
  year = 1886,
  journal = {Mind},
  volume = {11},
  pages = {220--242}
}

@book{ChibaKajiyama-VowelItsNature-1941,
  title = {The {{Vowel}}, {{Its Nature}} and {{Structure}}},
  author = {Chiba, T. and Kajiyama, M.},
  year = 1941,
  publisher = {Phonetic Society of Japan}
}

@book{ChibaKajiyama-VowelItsNature-1958,
  title = {The {{Vowel}}, {{Its Nature}} and {{Structure}}},
  author = {Chiba, T. and Kajiyama, M.},
  year = 1958,
  edition = {2nd edition},
  publisher = {Phonetic Society of Japan}
}

@article{ChiuGick-StartlingSpeechEliciting-2014,
  title = {Startling Speech: Eliciting Prepared Speech Using Startling Auditory Stimulus},
  author = {Chiu, C and Gick, B.},
  year = 2014,
  journal = {Frontiers in Psychology},
  volume = {5},
  number = {1082}
}

@article{ChoKeating-EffectsInitialPosition-2009,
  title = {Effects of Initial Position versus Prominence in {{English}}},
  author = {Cho, T. and Keating, P.},
  year = 2009,
  journal = {Journal of Phonetics},
  volume = {37},
  number = {4},
  pages = {466--485}
}

@inproceedings{ClelandEtAl-ComparingArticulatoryImages-2011,
  title = {Comparing Articulatory Images: {{An MRI}} / {{Ultrasound Tongue Image}} Database},
  booktitle = {Proceedings of the 9th {{International Seminar}} on {{Speech Production}}},
  author = {Cleland, J. and Wrench, A. A. and Scobbie, J. M. and Semple, S.},
  year = 2011,
  pages = {163--170}
}

@article{ClelandEtAl-CovertContrastCovert-2016,
  title = {Covert Contrast and Covert Errors in Persistent Velar Fronting},
  author = {Cleland, J. and Scobbie, J. M. and Heyde, C. J. and Roxburgh, Z. and Wrench, A. A.},
  year = 2016,
  journal = {Clinical Linguistics \& Phonetics},
  volume = {Early online},
  pages = {1--21}
}

@article{ClelandEtAl-EditorialInsightsUltrasound-2016,
  title = {Editorial: {{Insights}} from Ultrasound: {{Enhancing Our Understanding}} of {{Clinical Phonetics}}},
  author = {Cleland, J. and Scobbie, J. M. and Zharkova, N.},
  year = 2016,
  journal = {Clinical Linguistics \& Phonetics: Special Issue "Insights from Ultrasound"},
  volume = {30},
  number = {3--5},
  pages = {171--173}
}

@article{ClelandEtAl-UsingUltrasoundVisual-2015,
  title = {Using Ultrasound Visual Biofeedback to Treat Persistent Primary Speech Sound Disorders},
  author = {Cleland, J. and Scobbie, J. M. and Wrench, A. A.},
  year = 2015,
  journal = {Clinical Linguistics \& Phonetics},
  volume = {29},
  number = {8--10},
  pages = {575--597}
}

@book{Cline-ReadyPlayerOne-2011,
  title = {Ready {{Player One}}},
  author = {Cline, Ernest},
  year = 2011
}

@incollection{CollinsMees-ApproachesArticulatorySetting-1995,
  title = {Approaches to Articulatory Setting in Foreign-Language Teaching},
  booktitle = {Studies in {{General}} and {{English Phonetics}}: {{Essays}} in {{Honour}} of {{Professor J}}. {{D}}. {{O}}'{{Connor}}},
  author = {Collins, B. and Mees, I. M.},
  editor = {Lewis, J. W.},
  year = 1995,
  pages = {415--424},
  publisher = {Routledge},
  address = {New York}
}

@article{Craik-TheoryHumanOperator-1948,
  title = {Theory of the {{Human Operator}} in {{Control Systems}}},
  author = {Craik, K. J. W.},
  year = 1948,
  journal = {British Journal of Psychology},
  volume = {38},
  pages = {56--61}
}

@article{CranenBoves-PressureMeasurementsSpeech-1985,
  title = {Pressure Measurements during Speech Production Using Semiconductor Miniature Pressure Transducers: {{Impact}} on Models for Speech Production},
  author = {Cranen, B. and Boves, L.},
  year = 1985,
  journal = {Journal of the Acoustical Society of America},
  volume = {77},
  number = {4},
  pages = {1543--1551}
}

@inproceedings{CsapóEtAl-TransducerMisalignmentUltrasound-2020,
  title = {Transducer {{Misalignment}} in {{Ultrasound Tongue Imaging}}},
  booktitle = {{{ISSP}} 2020},
  author = {Csap{\'o}, T. and Xu, Kele and Deme, A. and Gr{\'a}czi, T. E. and Mark{\'o}, Alexandra},
  year = 2020,
  address = {Yale},
  url = {https://www.semanticscholar.org/paper/Transducer-Misalignment-in-Ultrasound-Tongue-Csap%C3%B3-Xu/206843eeb059e81428b6fcfb84e3ccc4616c104f},
  urldate = {2024-09-03},
  abstract = {Introduction In order to fix head movement during the Ultrasound Tongue Imaging (UTI), various solutions have been proposed, e.g. the metal headset of Articulate Instruments Ltd. [1, 2], UltraFit [3], and others listed in [4]. Despite these substantial efforts, it is a question whether the use of a headset itself is enough to ensure that the transducer is not moving during the recordings. Even if a transducer fixing system is used, large jaw movements during speech production can cause the ultrasound transducer to move, and misalignment or full displacement might occur. This way the recordings from the same session will not be directly comparable, which can be a serious issue during analysis of tongue contours.}
}

@inproceedings{CsapoEtAl-TransducerMisalignmentUltrasound-2020a,
  title = {Transducer {{Misalignment}} in {{Ultrasound Tongue Imaging}}},
  booktitle = {Proceedings of the 12th {{International Seminar}} on {{Speech Production}} ({{ISSP}} 2020)},
  author = {Csap{\'o}, T. G. and Xu, K. and Deme, A. and Gr{\'a}czi, T. E. and Mark{\'o}, A.},
  year = 2020,
  pages = {166--169},
  address = {Online / New Haven, CT}
}

@inproceedings{CsapoLulich-ErrorAnalysisExtracted-2015,
  title = {Error Analysis of Extracted Tongue Contours from {{2D}} Ultrasound Images},
  booktitle = {{{INTERSPEECH-2015}}},
  author = {Csap{\'o}, T. G. and Lulich, S. M.},
  year = 2015,
  pages = {2157--2161}
}

@inproceedings{CsapóXu-QuantificationTransducerMisalignment-2020,
  title = {Quantification of {{Transducer Misalignment}} in {{Ultrasound Tongue Imaging}}},
  author = {Csap{\'o}, Tam{\'a}s and Xu, Kele},
  year = 2020,
  abstract = {In speech production research, different imaging modalities have been employed to obtain accurate information about the movement and shaping of the vocal tract. Ultrasound is an affordable and non-invasive imaging modality with relatively high temporal and spatial resolution to study the dynamic behavior of tongue during speech production. However, a long-standing problem for ultrasound tongue imaging is the transducer misalignment during longer data recording sessions. In this paper, we propose a simple, yet effective, misalignment quantification approach. The analysis employs MSE distance and two similarity measurement metrics to identify the relative displacement between the chin and the transducer. We visualize these measures as a function of the timestamp of the utterances. Extensive experiments are conducted on a Hungarian and Scottish English child dataset. The results suggest that large values of Mean Square Error (MSE) and small values of Structural Similarity Index (SSIM) and Complex Wavelet SSIM indicate corruptions or issues during the data recordings, which can either be caused by transducer misalignment or lack of gel.},
  file = {/home/jpalo/Zotero/storage/D3EBNMUH/Csapó and Xu - 2020 - Quantification of Transducer Misalignment in Ultrasound Tongue Imaging.pdf}
}

@inproceedings{CsapoXu-QuantificationTransducerMisalignment-2020a,
  title = {Quantification of {{Transducer Misalignment}} in {{Ultrasound Tongue Imaging}}},
  booktitle = {Interspeech 2020},
  author = {Csap{\'o}, Tam{\'a}s G{\'a}bor and Xu, Kele},
  year = 2020,
  pages = {3735--3739},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2020-1672},
  url = {https://www.isca-archive.org/interspeech_2020/csapo20d_interspeech.html},
  urldate = {2024-09-03},
  abstract = {In speech production research, different imaging modalities have been employed to obtain accurate information about the movement and shaping of the vocal tract. Ultrasound is an affordable and non-invasive imaging modality with relatively high temporal and spatial resolution to study the dynamic behavior of tongue during speech production. However, a long-standing problem for ultrasound tongue imaging is the transducer misalignment during longer data recording sessions. In this paper, we propose a simple, yet effective, misalignment quantification approach. The analysis employs MSE distance and two similarity measurement metrics to identify the relative displacement between the chin and the transducer. We visualize these measures as a function of the timestamp of the utterances. Extensive experiments are conducted on a Hungarian and Scottish English child dataset. The results suggest that large values of Mean Square Error (MSE) and small values of Structural Similarity Index (SSIM) and Complex Wavelet SSIM indicate corruptions or issues during the data recordings, which can either be caused by transducer misalignment or lack of gel.},
  langid = {english},
  file = {/home/jpalo/Zotero/storage/VX4ZRNQY/Csapó and Xu - 2020 - Quantification of Transducer Misalignment in Ultrasound Tongue Imaging.pdf}
}

@article{Dahlgren-CopticVowelReduction-2020,
  title = {Coptic Vowel Reduction in a Cross-Linguistic Context: {{Evidence}} from {{L2 Greek}} Usage},
  author = {Dahlgren, S.},
  year = 2020,
  journal = {Italian Journal of Linguistics}
}

@unpublished{DahlgrenEtAl-INVESTIGATINGLANGUAGESPECIFICCOARTICULATORY-2022,
  title = {{{INVESTIGATING LANGUAGE-SPECIFIC COARTICULATORY PATTERNS}}: {{A COMPARATIVE ULTRASOUND STUDY OF FINNISH AND ENGLISH}}},
  author = {Dahlgren, S. and Palo, P. and Toivola, M.},
  year = 2022
}

@unpublished{DahlgrenPalo-StudyingLanguagespecificCoarticulatory-2024,
  title = {Studying Language-Specific Coarticulatory Patterns with Tongue Ultrasound: {{The}} Case of {{Finnish}}},
  author = {Dahlgren, S. and Palo, P.},
  year = 2024
}

@article{DalstonKeefe-DigitalLabialVelopharyngeal-1988,
  title = {Digital, {{Labial}}, and {{Velopharyngeal Reaction Times}} in {{Normal Speakers}}},
  author = {Dalston, Rodger M. and Keefe, Michael J.},
  year = 1988,
  journal = {Cleft Palate Journal},
  volume = {25},
  number = {3},
  pages = {203--209}
}

@article{DamerjianEtAl-SpeckleCharacterizationMethods-2014,
  title = {Speckle Characterization Methods in Ultrasound Images~--~{{A}} Review},
  author = {Damerjian, V. and Tankyevych, O. and Souag, N. and Petit, E.},
  year = 2014,
  month = sep,
  journal = {IRBM},
  volume = {35},
  number = {4},
  pages = {202--213},
  issn = {1959-0318},
  doi = {10.1016/j.irbm.2014.05.003},
  url = {https://www.sciencedirect.com/science/article/pii/S1959031814000797},
  urldate = {2024-12-11},
  abstract = {Speckle has been widely considered a noisy feature in ultrasound images, thus it is intended to be suppressed and eliminated. On the other hand, speckle can be studied as a signal modeled by various statistical distributions or by analyzing its intensity with spatial relations in image space that characterize its nature, and hence, the nature of the underlying tissue. This knowledge can then be used in order to classify the different speckle regions into anatomical structures. In fact, speckle characterization in echocardiography and other ultrasonic images is important for motion tracking, tissue characterization, image segmentation, registration, and other medical applications for diagnosis, therapy planning and decision making. In this paper, we review and discuss various speckle characterization methods, which are often applied to confirm the speckle nature of the elements.}
}

@article{DamerjianEtAl-SpeckleCharacterizationMethods-2014a,
  title = {Speckle Characterization Methods in Ultrasound Images~--~{{A}} Review},
  author = {Damerjian, V. and Tankyevych, O. and Souag, N. and Petit, E.},
  year = 2014,
  journal = {IRBM},
  volume = {35},
  number = {4},
  pages = {202--213},
  issn = {1959-0318},
  doi = {10.1016/j.irbm.2014.05.003},
  url = {https://www.sciencedirect.com/science/article/pii/S1959031814000797},
  urldate = {2025-05-14},
  abstract = {Speckle has been widely considered a noisy feature in ultrasound images, thus it is intended to be suppressed and eliminated. On the other hand, speckle can be studied as a signal modeled by various statistical distributions or by analyzing its intensity with spatial relations in image space that characterize its nature, and hence, the nature of the underlying tissue. This knowledge can then be used in order to classify the different speckle regions into anatomical structures. In fact, speckle characterization in echocardiography and other ultrasonic images is important for motion tracking, tissue characterization, image segmentation, registration, and other medical applications for diagnosis, therapy planning and decision making. In this paper, we review and discuss various speckle characterization methods, which are often applied to confirm the speckle nature of the elements.},
  file = {/home/jpalo/Zotero/storage/8737WMBL/Damerjian et al. - 2014 - Speckle characterization methods in ultrasound images – A review.pdf;/home/jpalo/Zotero/storage/EYGPTPI3/S1959031814000797.html}
}

@article{Davidson-ComparingTongueShapes-2006,
  title = {Comparing Tongue Shapes from Ultrasound Imaging Using Smoothing Spline Analysis of Variance},
  author = {Davidson, L.},
  year = 2006,
  journal = {Journal of the Acoustical Society of America},
  volume = {120},
  number = {1},
  pages = {407--415}
}

@misc{Davis-WhatAppalachianTrail-2014,
  title = {What Is an {{Appalachian Trail}} Thru-Hike?},
  author = {Davis, Zach},
  year = 2014,
  url = {https://thetrek.co/definition-thru-hike/},
  urldate = {2020-07-14}
}

@article{DawsonEtAl-MethodsQuantifyingTongue-2016,
  title = {Methods for Quantifying Tongue Shape and Complexity Using Ultrasound Imaging.},
  author = {Dawson, Katherine M. and Tiede, Mark K. and Whalen, D. H.},
  year = 2016,
  journal = {Clinical linguistics \& phonetics},
  volume = {30},
  number = {3-5},
  pages = {328--344},
  address = {England},
  issn = {1464-5076 0269-9206},
  doi = {10.3109/02699206.2015.1099164},
  abstract = {Quantification of tongue shape is potentially useful for indexing articulatory strategies arising from intervention, therapy and development. Tongue shape  complexity is a parameter that can be used to reflect regional functional  independence of the tongue musculature. This paper considers three different  shape quantification methods - based on Procrustes analysis, curvature  inflections and Fourier coefficients - and uses a linear discriminant analysis to  test how well each method is able to classify tongue shapes from different  phonemes. Test data are taken from six native speakers of American English  producing 15 phoneme types. Results classify tongue shapes accurately when  combined across quantification methods. These methods hold promise for extending  the use of ultrasound in clinical assessments of speech deficits.},
  langid = {english},
  pmcid = {PMC4914365},
  pmid = {26587871},
  keywords = {*Phonetics,*Ultrasonography,Adult,Articulation,complexity,Humans,Middle Aged,speech production measurement,Speech Production Measurement/methods,Speech/*physiology,tongue shape,Tongue/*anatomy & histology/diagnostic imaging,ultrasound}
}

@article{DearyEtAl-FreeEasytouseComputerbased-2011,
  title = {A Free, Easy-to-Use, Computer-Based Simple and Four-Choice Reaction Time Programme: {{The Deary-Liewald}} Reaction Time Task},
  author = {Deary, Ian J. and Liewald, David and Nissan, Jack},
  year = 2011,
  journal = {Behavior Research Methods},
  volume = {43},
  number = {1},
  pages = {258--268},
  publisher = {Springer-Verlag}
}

@book{DeBoer-HistoryPhilosophyIslam-2003,
  title = {History of {{Philosophy}} in {{Islam}}},
  author = {De Boer, T. J.},
  year = 2003,
  publisher = {Kessinger Publishing}
}

@inproceedings{DedouchEtAl-AcousticModalAnalysis-2002,
  title = {Acoustic Modal Analysis of Male Vocal Tract for {{Czech}} Vowels},
  booktitle = {Proceedings {{Interaction}} and {{Feedbacks}} '2002},
  author = {Dedouch, K. and Hor{\'a}{\v c}ek, J. and Vampola, T. and {\v S}vec, J.G. and Kr{\v s}ek, P. and Havl{\'i}k, R.},
  year = 2002,
  pages = {13--19}
}

@inproceedings{DedouchEtAl-FiniteElementModelling-2002,
  title = {Finite Element Modelling of a Male Vocal Tract with Consideration of Cleft Palate},
  booktitle = {Forum {{Acusticum}}},
  author = {Dedouch, K. and Hor{\'a}{\v c}ek, J. and Vampola, T. and {\v C}ern{\'y}, L.},
  year = 2002,
  address = {Sevilla, Spain}
}

@book{DenesPinson-SpeechChainPhysics-1993,
  title = {Speech {{Chain}}: {{The Physics}} and {{Biology}} of {{Spoken Language}}},
  author = {Denes, P. B. and Pinson, E. N.},
  year = 1993,
  publisher = {{W.H. Freeman and Company}},
  address = {New York}
}

@article{DerrickEtAl-ThreedimensionalPrintableUltrasound-2018,
  title = {Three-Dimensional Printable Ultrasound Transducer Stabilization System},
  author = {Derrick, Donald and Carignan, Christopher and Chen, Wei-Rong and Shujau, Muawiyath and Best, Catherine T.},
  year = 2018,
  month = nov,
  journal = {The Journal of the Acoustical Society of America},
  volume = {144},
  number = {5},
  pages = {EL392},
  issn = {1520-8524},
  doi = {10.1121/1.5066350},
  abstract = {When using ultrasound imaging of the tongue for speech recording/research, submental transducer stabilization is required to prevent the ultrasound transducer from translating or rotating in relation to the tongue. An iterative prototype of a lightweight three-dimensional-printable wearable ultrasound transducer stabilization system that allows flexible jaw motion and free head movement is presented. The system is completely non-metallic, eliminating interference with co-recorded signals, thus permitting co-collection and co-registration with articulometry systems. A motion study of the final version demonstrates that transducer rotation is limited to 1.25{$^\circ$} and translation to 2.5\,mm-well within accepted tolerances.},
  langid = {english},
  pmcid = {PMC6221822},
  pmid = {30522328},
  keywords = {Germany,Head Movements,Humans,Jaw,Male,Printing Three-Dimensional,Speech,Speech Articulation Tests,Tongue,Transducers,Ultrasonography},
  file = {/home/jpalo/Zotero/storage/V7MQUELG/Derrick et al. - 2018 - Three-dimensional printable ultrasound transducer stabilization system.pdf}
}

@article{DevarajuEtAl-ComparisonMcGurkEffect-2019,
  title = {Comparison of {{McGurk Effect}} across {{Three Consonant-Vowel Combinations}} in {{Kannada}}},
  author = {Devaraju, Dhatri S and U, Ajith Kumar and Maruthy, Santosh},
  year = 2019,
  month = jan,
  journal = {Journal of Audiology \& Otology},
  volume = {23},
  number = {1},
  pages = {39--48},
  issn = {2384-1621},
  doi = {10.7874/jao.2018.00234},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6348306/},
  urldate = {2025-01-23},
  abstract = {Background and Objectives The influence of visual stimulus on the auditory component in the perception of auditory-visual (AV) consonant-vowel syllables has been demonstrated in different languages. Inherent properties of unimodal stimuli are known to modulate AV integration. The present study investigated how the amount of McGurk effect (an outcome of AV integration) varies across three different consonant combinations in Kannada language. The importance of unimodal syllable identification on the amount of McGurk effect was also seen. Subjects and Methods Twenty-eight individuals performed an AV identification task with ba/ ga, pa/ka and ma/n{$\cdot$} a consonant combinations in AV congruent, AV incongruent (McGurk combination), audio alone and visual alone condition. Cluster analysis was performed using the identification scores for the incongruent stimuli, to classify the individuals into two groups; one with high and the other with low McGurk scores. The differences in the audio alone and visual alone scores between these groups were compared. Results The results showed significantly higher McGurk scores for ma/n{$\cdot$} a compared to ba/ga and pa/ka combinations in both high and low McGurk score groups. No significant difference was noted between ba/ga and pa/ka combinations in either group. Identification of /n{$\cdot$} a/ presented in the visual alone condition correlated negatively with the higher McGurk scores. Conclusions The results suggest that the final percept following the AV integration is not exclusively explained by the unimodal identification of the syllables. But there are other factors which may also contribute to making inferences about the final percept.},
  pmcid = {PMC6348306},
  pmid = {30518196},
  file = {/home/jpalo/Zotero/storage/LQMKETFC/Devaraju et al. - 2019 - Comparison of McGurk Effect across Three Consonant-Vowel Combinations in Kannada.pdf}
}

@article{DokovovaEtAl-TongueShapeComplexity-2023,
  title = {Tongue {{Shape Complexity}} in {{Children With}} and {{Without Speech Sound Disorders}}},
  author = {Dokovova, Marie and Sugden, Ellie and Cartney, Gemma and Schaeffler, Sonja and Cleland, Joanne},
  year = 2023,
  journal = {Journal of Speech, Language, and Hearing Research},
  volume = {66},
  number = {7},
  pages = {2164--2183},
  publisher = {American Speech-Language-Hearing Association},
  doi = {10.1044/2023_JSLHR-22-00472},
  url = {https://pubs.asha.org/doi/10.1044/2023_JSLHR-22-00472},
  urldate = {2024-06-18},
  abstract = {Purpose:  This study investigates the hypothesis that younger speakers and speakers with more severe speech sound disorders are more likely to use simpler (undifferentiated) tongue gestures due to difficulties with, or immaturity of, lingual motor control. Method:  The hypothesis is tested using cross-sectional secondary data analysis of synchronous audio and high-speed ultrasound recordings from children with idiopathic speech sound disorders (n = 30, aged 5;0--12;11 [years;months]) and typically developing children (n = 29, aged 5;8--12;10), producing /a/, /t/, /ɹ/, /l/, /s/, and /ʃ/ in an intervocalic /aCa/ environment. Tongue shape complexity is measured using NINFL (Number of INFLections) and modified curvature index (MCI) from splines fitted to ultrasound images at the point of maximal lingual gesture. Age, perceived accuracy, and consonant are used as predictors. Results:  The results suggest that as age increases, children with speech sound disorders have lower MCI compared to typically developing children. Increase in age also led to decrease of MCI for the typically developing group. In the group of children with speech sound disorders, perceptually incorrect /ɹ/ productions have lower MCI than correct productions, relative to /a/. Conclusions:  There is some evidence of systematic tongue shape complexity differences between typically developing children and children with speech sound disorders when accounting for increase in age. Among children with speech sound disorders, increase in age and perceptually incorrect consonant realizations are associated with decreasing tongue shape complexity.},
  file = {/home/jpalo/Zotero/storage/FF4UW3FV/Dokovova et al. - 2023 - Tongue Shape Complexity in Children With and Witho.pdf}
}

@inproceedings{DrakeEtAl-ARTICULATORYEVIDENCEINVOLVEMENT-2013,
  title = {{{ARTICULATORY EVIDENCE FOR THE INVOLVEMENT OF THE SPEECH PRODUCTION SYSTEM IN THE GENERATION OF PREDICTIONS DURING COMPREHENSION}}},
  booktitle = {Architectures and {{Mechanisms}} for {{Language Processing}} ({{AMLaP}})},
  author = {Drake, E. and Schaeffler, S. and Corley, M.},
  year = 2013,
  address = {Marseille}
}

@inproceedings{DrakeEtAl-DoesPredictionComprehension-2013,
  title = {Does Prediction in Comprehension Involve Articulation? {{Evidence}} from Speech Imaging},
  booktitle = {11th {{Symposium}} of {{Psycholinguistics}} ({{SCOPE}})},
  author = {Drake, E. and Schaeffler, S. and Corley, M.},
  year = 2013,
  address = {Tenerife}
}

@article{DraperEtAl-ExpiratoryPressuresAir-1960,
  title = {Expiratory Pressures and Air Flow during Speech},
  author = {Draper, M. H. and Ladefoged, P. and Whitteridge, D.},
  year = 1960,
  journal = {British Medical Journal},
  volume = {1},
  number = {5189},
  pages = {1837--1843}
}

@misc{DrGeoffLindsey-PhoneticAdventuresPrague-2023,
  title = {Phonetic Adventures in {{Prague}}},
  author = {{Dr Geoff Lindsey}},
  year = 2023,
  month = aug,
  url = {https://www.youtube.com/watch?v=uDpVPj49R8w},
  urldate = {2025-01-28},
  abstract = {Join my Discord: ~~/~discord~~ -- It's rare for a language to have a 'fricative trill' sound in addition to its regular /r/, but Czech is one such language. Hear it on my trip to the once-every-4-years Phonetics Congress in stunning Prague.  20th International Congress of Phonetics Sciences, 7-11 Aug 2023 was hosted by the Institute of Phonetics, Charles University, Prague (Radek Skarntizl, Congress Chair) Presentations mentioned: 'Timing of laughter in conversation: better late than never?' Tamara Rathcke (University of Konstanz), Eleni Kapogianni and Lucy Page 'Some measures of phonetic similarity for use in legal trade mark disputes' Sandra Ferrari Disner (USC) and Vincent J. van Heuven 'Game of phones: a socio-phonetic analysis of stylised media performance of Yorkshire English' Lucy Jackson (University of Glasgow) 'Dentofacial Disharmony: patients' sibilants differ from controls' more in source than filter properties'         Madeleine Oakley (North Carolina State University), Auvi Tran, Ciana Paye, Emma Trudan, Timothy Turve   'Glottalization of voiceless stops in Multicultural London English'        Chong Adam (Queen Mary University of London) and Garellek Marc    'The devil is in the detail: An interactional-phonetic study of G-word interjections and some methodological implications '  Marina Cantarutti (University of York) 'Beyond accent, attitudes, and native speakers: What might socially responsible second language speech research look like?'           Pavel Trofimovich (Concordia University) 'What can speakers tell us about speech?'          Jane Stuart-Smith (University of Glasgow) If you want to speak British English clearly and confidently, I recommend this course from accent coach Luke Nicholson: info: https://improveyouraccent.co.uk/engli... sign up: https://course.improveyouraccent.co.u...}
}

@article{Dunn-CalculationVowelResonances-1950,
  title = {The {{Calculation}} of {{Vowel Resonances}}, and an {{Electrical Vocal Tract}}},
  author = {Dunn, H. K.},
  year = 1950,
  journal = {J. Acoust. Soc. Am.},
  volume = {22},
  pages = {740--753}
}

@article{ElMasriEtAl-DevelopmentTransmissionLine-1998,
  title = {Development of the {{Transmission Line Matrix}} Method in Acoustics. {{Applications}} to Higher Modes in the Vocal Tract and Other Complex Ducts},
  author = {El Masri, S. and Pelorson, X. and Saguet, P. and Badin, P.},
  year = 1998,
  journal = {Int. J. of Numerical Modelling},
  volume = {11},
  pages = {133--151}
}

@inproceedings{ElMasriEtAl-VocalTractAcoustics-1996,
  title = {Vocal Tract Acoustics Using the Transmission Line Matrix ({{TLM}}) Method},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Spoken Language Processing}}},
  author = {El Masri, S. and Pelorson, X. and Saguet, P. and Badin, P.},
  year = 1996,
  pages = {953--956}
}

@inproceedings{Engwall-3DTongueModel-2000,
  title = {A {{3D}} Tongue Model Based on {{MRI}} Data},
  booktitle = {In {{Proceedings}} of {{International Conference}} on {{Spoken Language Processing}} 2000 ({{ICSLP}} 2000)},
  author = {Engwall, O.},
  year = 2000,
  pages = {III: 901--904}
}

@inproceedings{Engwall-AreStaticalMRI-2000,
  title = {Are Statical {{MRI}} Data Representative of Dynamic Speech? {{Results}} from a Comparative Study Using {{MRI}}, {{EMA}} and {{EPG}}},
  booktitle = {In {{Proceedings}} of {{International Conference}} on {{Spoken Language Processing}} 2000 ({{ICSLP}} 2000)},
  author = {Engwall, O.},
  year = 2000,
  pages = {I: 17--20}
}

@inproceedings{Engwall-RevisitApplicationMRI-2003,
  title = {A Revisit to the {{Application}} of {{MRI}} to the {{Analysis}} of {{Speech Production}} - {{Testing}} Our Assumptions},
  booktitle = {In {{Proceedings}} of the 6th {{Int Seminar}} on {{Speech Production}}},
  author = {Engwall, O.},
  year = 2003
}

@inproceedings{Engwall-SpeakerAdaptationThreedimensional-2004,
  title = {Speaker Adaptation of a Three-Dimensional Tongue Model},
  booktitle = {{{ICSLP}} 2004},
  author = {Engwall, O.},
  editor = {Kim, Soon Hyob and Youn, Dae Hee},
  year = 2004,
  month = oct,
  volume = {I},
  pages = {465--468},
  address = {Jeju Island, Korea}
}

@incollection{Engwall-SpeechProductionModels-2006,
  title = {Speech Production: {{Models}}, {{Phonetic Processes}} and {{Techniques}}},
  author = {Engwall, O.},
  editor = {Harrington, J. and Tabain, M.},
  year = 2006,
  pages = {301--314},
  publisher = {Psychology Press},
  address = {New York},
  chapter = {Assessing MRI measurements: Effects of sustenation, gravitation and coarticulation.}
}

@phdthesis{Engwall-TongueTalkingStudies-2002,
  title = {Tongue {{Talking}} - {{Studies}} in {{Intraoral Speech Synthesis}}},
  author = {Engwall, O.},
  year = 2002,
  school = {Kungliga Tekniska H\"ogskolan, Stockholm, Sweden}
}

@article{EngwallBadin-CollectingAnalysingTwo-1999,
  title = {Collecting and Analysing Two- and Three-Dimensional {{MRI}} Data for {{Swedish}}},
  author = {Engwall, O. and Badin, P.},
  year = 1999,
  journal = {TMH-QPSR},
  volume = {3-4/1999},
  pages = {11--38}
}

@misc{EoinReardon-FranticChairMaking-2025,
  title = {Frantic {{Chair Making}}},
  author = {{Eoin Reardon}},
  year = 2025,
  month = feb,
  url = {https://www.youtube.com/watch?v=7LpVlXxEEL4},
  urldate = {2025-08-20},
  abstract = {In this video I try and make two chairs in one day.}
}

@phdthesis{Ericsdotter-ArticulatoryAcousticRelationshipsSwedish-2005,
  title = {Articulatory-{{Acoustic Relationships}} in {{Swedish Vowel Sounds}}},
  author = {Ericsdotter, C.},
  year = 2005,
  school = {Stockholm University, Stockholm, Sweden}
}

@book{Euclid-ElementsBooksIXIII-2006,
  title = {The {{Elements}}: {{Books I-XIII}} - {{Complete}} and {{Unabridged}}},
  author = {{Euclid}},
  year = 2006,
  publisher = {Barnes \& Noble}
}

@book{Fant-AcousticTheorySpeech-1960,
  title = {Acoustic {{Theory}} of {{Speech Production}}},
  author = {Fant, G.},
  year = 1960,
  publisher = {Mouton, The Hague}
}

@inproceedings{Fant-SoundSpectrography-1961,
  title = {Sound {{Spectrography}}},
  booktitle = {Proceedings of 4th {{ICPhS}}},
  author = {Fant, Gunnar},
  year = 1961,
  pages = {14--33},
  address = {Helsinki, Finland},
  file = {/home/jpalo/Zotero/storage/UJ5N8IK6/Fant - 1961 - Sound Spectrography.pdf}
}

@incollection{FarnetaniRecasens-CoarticulationModelsRecent-1999,
  title = {Coarticulation Models in Recent Speech Production Theories},
  booktitle = {Coarticulation: {{Theory}}, {{Data}} and {{Techniques}}},
  author = {Farnetani, E. and Recasens, D.},
  editor = {Hardcastle, W. J. and Hewlett, N.},
  year = 1999,
  pages = {31--68},
  publisher = {Cambridge University Press}
}

@inproceedings{FaselBerry-DeepBeliefNetworks-2010,
  title = {Deep {{Belief Networks}} for {{Real-Time Extraction}} of {{Tongue Contours}} from {{Ultrasound During Speech}}},
  booktitle = {2010 20th {{International Conference}} on {{Pattern Recognition}}},
  author = {Fasel, I. and Berry, J.},
  year = 2010,
  month = aug,
  pages = {1493--1496}
}

@phdthesis{Faytak-ArticulatoryUniformityArticulatory-2018,
  title = {Articulatory {{Uniformity}} through {{Articulatory Reuse}}: {{Insights}} from an {{Ultrasound Study}} of {{S\=uzh\=ou Chinese}} - {{ProQuest}}},
  shorttitle = {Articulatory {{Uniformity}} through {{Articulatory Reuse}}},
  author = {Faytak, Matthew D.},
  year = 2018,
  url = {https://www.proquest.com/openview/f6e14fa5fec803d1052890c57ac98121/1?pq-origsite=gscholar&cbl=18750},
  urldate = {2025-05-28},
  langid = {english},
  file = {/home/jpalo/Zotero/storage/XZSVHUWG/Faytak - 2018 - Articulatory Uniformity through Articulatory Reuse Insights from an Ultrasound Study of Sūzhōu Chin.pdf;/home/jpalo/Zotero/storage/YSXMMHCV/1.html}
}

@inproceedings{FaytakEtAl-SpeechArticulationToolkit-2020,
  title = {The Speech Articulation Toolkit ({{SATKit}}): {{Ultrasound}} Image Analysis in Python},
  booktitle = {Proceedings of the 12th International Seminar on Speech Production ({{ISSP}} 2020)},
  author = {Faytak, M. and Moisik, S. R. and Palo, P.},
  year = 2020,
  pages = {234--237},
  address = {Online / New Haven, CT}
}

@book{Feher-QuantitativeHumanPhysiology-2012,
  title = {Quantitative {{Human Physiology}}},
  author = {Feher, J.},
  year = 2012,
  publisher = {Academic Press}
}

@book{Feist-DarknessSethanon-1987,
  title = {A {{Darkness}} at {{Sethanon}}},
  author = {Feist, Raymond E.},
  year = 1987,
  series = {The {{Riftwar Saga}}},
  number = {4}
}

@article{Feldman-OnceMoreEquilibriumpoint-1986,
  title = {Once More on the Equilibrium-Point Hypothesis (Lambda Model) for Motor Control},
  author = {Feldman, A.},
  year = 1986,
  journal = {Journal of Motor Behavior},
  volume = {18},
  number = {1},
  pages = {17--56}
}

@book{FetterWalecka-TheoreticalMechanicsParticles-1980,
  title = {Theoretical {{Mechanics}} of {{Particles}} and {{Continua}}},
  author = {Fetter, A. and Walecka, J.},
  year = 1980,
  publisher = {McGraw--Hill, New York}
}

@misc{Fitt-UnisynLexiconRelease-2014,
  title = {Unisyn {{Lexicon Release}}},
  author = {Fitt, Susan},
  year = 2014
}

@book{FoiasFrazho-CommutantLiftingApproach-1990,
  title = {The {{Commutant Lifting Approach}} to {{Interpolation Problems}}},
  author = {Foias, C. and Frazho, A. E.},
  year = 1990,
  series = {Operator {{Theory}}: {{Advances}} and Applications},
  volume = {44},
  publisher = {Birkh\"auser Verlag, Basel}
}

@misc{Ford-InstagramPost-2025,
  title = {Instagram Post},
  author = {Ford, Emily},
  year = 2025,
  url = {https://www.instagram.com/p/DFpXx9VREWv/},
  urldate = {2025-02-05},
  annotation = {aka emilyontrail},
  file = {/home/jpalo/Zotero/storage/6872LECF/DFpXx9VREWv.html}
}

@article{FougeronEtAl-ReducedCoarticulationAging-2018,
  title = {Reduced Coarticulation and Aging},
  author = {Fougeron, C. and D'Alessandro, D. and Lancia, L.},
  year = 2018,
  journal = {The Journal of the Acoustical Society of America},
  volume = {144},
  number = {3},
  pages = {1905--1905}
}

@misc{FourKeysBookArts-MakeYourOwn-2024,
  title = {Make {{Your Own Piercing Cradle}} - {{Indispendable Bookbinding Tool}} - {{Two Versions}}: {{Easy}} and {{Easier}}!},
  shorttitle = {Make {{Your Own Piercing Cradle}} - {{Indispendable Bookbinding Tool}} - {{Two Versions}}},
  author = {{Four Keys Book Arts}},
  year = 2024,
  month = jan,
  url = {https://www.youtube.com/watch?v=i9CMYHRnbeQ},
  urldate = {2025-09-25},
  abstract = {Hello everyone, welcome back to my bindery!  In this video I present to you two easy-to-make versions of a tool I use for virtually every bookbinding project: the piercing cradle! If you're moving beyon}
}

@article{Fowler-PerceptualCentersSpeech-1979,
  title = {''{{Perceptual}} Centers`` in Speech Production and Perception},
  author = {Fowler, C.A.},
  year = 1979,
  journal = {Perception \& Psychophysics},
  volume = {25},
  number = {5},
  pages = {375--388}
}

@incollection{FowlerTassinary-NaturalMeasurementCriteria-1981,
  title = {Natural {{Measurement Criteria}} for {{Speech}}: {{The Anisochrony Illusion}}},
  booktitle = {Attention and {{Performance IX}}},
  author = {Fowler, C.A. and Tassinary, L.G.},
  editor = {Long, J. and Baddeley, A.},
  year = 1981,
  pages = {521--535},
  publisher = {{The International Association For The Study of Attention and Performance}}
}

@article{FuchsEtAl-AcousticRespiratoryEvidence-2013,
  title = {Acoustic and Respiratory Evidence for Utterance Planning in {{German}}},
  author = {Fuchs, S. and Petrone, C. and Krivokapi{\'c}, J. and Hoole, P.},
  year = 2013,
  journal = {Journal of Phonetics},
  volume = {41},
  number = {1},
  pages = {29--47},
  issn = {0095-4470}
}

@unpublished{FuchsÜnal-Logacev-TonguePalatalContacts-2017,
  title = {Tongue Palatal Contacts during Speech Preparation in 7 Languages},
  author = {Fuchs, S. and {\"U}nal-Logacev, {\"O}.},
  year = 2017
}

@book{FullerEtAl-AppliedAnatomyPhysiology-2012,
  title = {Applied Anatomy \& Physiology for Speech-Language Pathology \& Audiology},
  author = {Fuller, D. R. and Pimentel, J. T. and Peregoy, B. M.},
  year = 2012,
  publisher = {Wolters Kluwer-Lippincott Williams \& Wilkins},
  address = {Baltimore, MD}
}

@article{Gabor-TheoryCommunicationPart-1946,
  title = {Theory of Communication. {{Part}} 1: {{The}} Analysis of Information},
  shorttitle = {Theory of Communication. {{Part}} 1},
  author = {Gabor, D.},
  year = 1946,
  journal = {Journal of the Institution of Electrical Engineers - Part III: Radio and Communication Engineering},
  volume = {93},
  number = {26},
  pages = {429--441},
  publisher = {{The Institution of Engineering and Technology}},
  doi = {10.1049/ji-3-2.1946.0074},
  url = {https://digital-library.theiet.org/doi/abs/10.1049/ji-3-2.1946.0074},
  urldate = {2025-05-14},
  abstract = {Hitherto communication theory was based on two alternative methods of signal analysis. One is the description of the signal as a function of time; the other is Fourier analysis. Both are idealizations, as the first method operates with sharply defined instants of time, the second with infinite wave-trains of rigorously defined frequencies. But our everyday experiences---especially our auditory sensations---insist on a description in terms of both time and frequency. In the present paper this point of view is developed in quantitative language. Signals are represented in two dimensions, with time and frequency as co-ordinates. Such two-dimensional representations can be called ``information diagrams,'' as areas in them are proportional to the number of independent data which they can convey. This is a consequence of the fact that the frequency of a signal which is not of infinite duration can be defined only with a certain inaccuracy, which is inversely proportional to the duration, and vice versa. This ``uncertainty relation'' suggests a new method of description, intermediate between the two extremes of time analysis and spectral analysis. There are certain ``elementary signals'' which occupy the smallest possible area in the information diagram. They are harmonic oscillations modulated by a ``probability pulse.'' Each elementary signal can be considered as conveying exactly one datum, or one ``quantum of information.'' Any signal can be expanded in terms of these by a process which includes time analysis and Fourier analysis as extreme cases. These new methods of analysis, which involve some of the mathematical apparatus of quantum theory, are illustrated by application to some problems of transmission theory, such as direct generation of single sidebands, signals transmitted in minimum time through limited frequency channels, frequency modulation and time-division multiplex telephony.},
  file = {/home/jpalo/Zotero/storage/ZET83XGS/Gabor - 1946 - Theory of communication. Part 1 The analysis of information.pdf}
}

@article{Gick-UseUltrasoundLinguistic-2002,
  title = {The Use of Ultrasound for Linguistic Phonetic Fieldwork},
  author = {Gick, Bryan},
  year = 2002,
  month = dec,
  journal = {Journal of the International Phonetic Association},
  volume = {32},
  number = {2},
  pages = {113--121},
  issn = {1475-3502, 0025-1003},
  doi = {10.1017/S0025100302001007},
  url = {https://www.cambridge.org/core/journals/journal-of-the-international-phonetic-association/article/abs/use-of-ultrasound-for-linguistic-phonetic-fieldwork/0604F35DE0FB40C5B8C5923E346B498B},
  urldate = {2024-09-18},
  abstract = {An increasingly wide variety of imaging and tracking technologies have been applied to measuring speech articulation in recent years. However, virtually all of these techniques have been restricted to laboratory or clinical settings. Portable ultrasound technology offers a way to conduct easy, non-invasive dynamic imaging of the whole tongue for less commonly studied languages, both in the field and in the laboratory.},
  langid = {english}
}

@article{Gick-UseUltrasoundLinguistic-2002a,
  title = {The Use of Ultrasound for Linguistic Phonetic Fieldwork},
  author = {Gick, Bryan},
  year = 2002,
  journal = {Journal of the International Phonetic Association},
  volume = {32},
  number = {2},
  pages = {113--121},
  issn = {1475-3502, 0025-1003},
  doi = {10.1017/S0025100302001007},
  url = {https://www.cambridge.org/core/journals/journal-of-the-international-phonetic-association/article/use-of-ultrasound-for-linguistic-phonetic-fieldwork/0604F35DE0FB40C5B8C5923E346B498B},
  urldate = {2024-09-19},
  abstract = {An increasingly wide variety of imaging and tracking technologies have been applied to measuring speech articulation in recent years. However, virtually all of these techniques have been restricted to laboratory or clinical settings. Portable ultrasound technology offers a way to conduct easy, non-invasive dynamic imaging of the whole tongue for less commonly studied languages, both in the field and in the laboratory.},
  langid = {english},
  file = {/home/jpalo/Zotero/storage/MDK3YSLK/Gick - 2002 - The use of ultrasound for linguistic phonetic fieldwork.pdf}
}

@book{GickEtAl-ArticulatoryPhonetics-2013,
  title = {Articulatory {{Phonetics}}},
  author = {Gick, B. and Wilson, I. and Derrick, D.},
  year = 2013,
  publisher = {Wiley-Blackwell}
}

@article{Goldstein-DomainQuantalTheory-1989,
  title = {On the Domain of the Quantal Theory},
  author = {Goldstein, Louis},
  year = 1989,
  month = jan,
  journal = {Journal of Phonetics},
  volume = {17},
  number = {1},
  pages = {91--97},
  issn = {0095-4470},
  doi = {10.1016/S0095-4470(19)31527-X},
  url = {https://www.sciencedirect.com/science/article/pii/S009544701931527X},
  urldate = {2025-05-31},
  abstract = {Stevens' quantal theory is examined from the point of view of accounting for the types of gestural structures that are employed phonologically in languages. It is argued that the domain of the theory is both wider and narrower than Stevens proposes. It is wider in that it could be extended to explain, not only what gestures languages may employ contrastively, but also how those gestures may be organized. It is narrower in that quantal considerations do not seem to actively constrain the precision of talkers in speech production.},
  file = {/home/jpalo/Zotero/storage/QYYVCPPP/S009544701931527X.html}
}

@article{Goldstein-RoleTemporalModulation-2019,
  title = {The {{Role}} of {{Temporal Modulation}} in {{Sensorimotor Interaction}}},
  author = {Goldstein, Louis},
  year = 2019,
  journal = {Frontiers in Psychology},
  volume = {10},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2019.02608},
  url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2019.02608/full},
  urldate = {2024-04-16},
  abstract = {{$<$}p{$>$}How do we align the distinct neural patterns associated with the articulation and the acoustics of the same utterance in order to guide behaviors that demand sensorimotor interaction, such as vocal learning and the use of feedback during speech production? One hypothesis is that while the representations are distinct, their patterns of change over time (temporal modulation) are systematically related. This hypothesis is pursued in the exploratory study described here, using paired articulatory and acoustic data from the X-ray microbeam corpus. The results show that modulation in both articulatory movement and in the changing acoustics has the form of a pulse-like structure related to syllable structure. The pulses are aligned with each other in time, and the modulation functions are robustly correlated. These results encourage further investigation and testing of the hypothesis.{$<$}/p{$>$}},
  langid = {english},
  keywords = {articulation,sensorimotor interaction,Speech Acoustics,speech production,Syllable structure,temporal modulation},
  file = {/home/jpalo/Zotero/storage/ZFJP9RCZ/Goldstein - 2019 - The Role of Temporal Modulation in Sensorimotor In.pdf}
}

@phdthesis{GomesdosSantosJunior-FluteInsideOutTracking-2017,
  title = {The {{Flute Inside-Out}}: {{Tracking Internal Movements}} in {{Flute Playing}}},
  author = {{Gomes dos Santos Junior}, O.},
  year = 2017,
  school = {Sydney Conservatorium of Music, University of Sydney}
}

@article{GomiEtAl-CompensatoryArticulationBilabial-2002,
  title = {Compensatory Articulation during Bilabial Fricative Production by Regulating Muscle Stiffness},
  author = {Gomi, H. and Honda, M. and Ito, T. and Murano, E. Z.},
  year = 2002,
  journal = {Journal of Phonetics},
  volume = {30},
  number = {3},
  pages = {261--279}
}

@inproceedings{GranströmHouse-OUTACOUSTICVISUAL-2007,
  title = {{{INSIDE OUT}} - {{ACOUSTIC AND VISUAL ASPECTS OF VERBAL AND NON-VERBAL COMMUNICATION}}},
  booktitle = {Proceedings of {{ICPhS}} 2007},
  author = {Granstr{\"o}m, B. and House, D.},
  year = 2007,
  pages = {11--18}
}

@inproceedings{GrimaldiEtAl-NewTechnologiesSimultaneous-2008,
  title = {New {{Technologies}} for {{Simultaneous Acquisition}} of {{Speech Articulatory Data}}: {{3D Articulograph}}, {{Ultrasound}} and {{Electroglottograph}}},
  booktitle = {{{LangTech}}},
  author = {Grimaldi, M. and Fivela, B. G. and Sigona, F. and Tavella, M. and Fitzpatrick, P. and Craighero, L. and Fadiga, L. and Sandini, G. and Metta, G.},
  year = 2008
}

@article{GriswoldEtAl-GeneralizedAutocalibratingPartially-2002,
  title = {Generalized Autocalibrating Partially Parallel Acquisitions ({{GRAPPA}})},
  author = {Griswold, M. A. and Jakob, P. M. and Heidemann, R. M. and Nittka, M. and Jellus, V. and Wang, J. and Kiefer, B. and Haase, A.},
  year = 2002,
  journal = {Magnetic Resonance in Medicine},
  volume = {47},
  pages = {1202}
}

@book{Gros-PhilosophyWalking-2015,
  title = {A {{Philosophy}} of {{Walking}}},
  author = {Gros, Fr{\'e}d{\'e}ric},
  year = 2015,
  edition = {Paberback},
  publisher = {Verso},
  address = {London}
}

@book{Guenther-NeuralControlSpeech-2016,
  title = {Neural {{Control}} of {{Speech}}},
  author = {Guenther, F. H.},
  year = 2016,
  publisher = {The MIT Press},
  address = {Cambridge, MA}
}

@article{GuentherEtAl-NeuralModelingImaging-2006,
  title = {Neural {{Modeling}} and {{Imaging}} of the {{Cortical Interactions Underlying Syllable Production}}},
  author = {Guenther, F. H. and Ghosh, S. S. and Tourville, J. A.},
  year = 2006,
  journal = {Brain \& Language},
  volume = {96},
  pages = {280--301}
}

@article{GuentherEtAl-TheoreticalInvestigationReference-1998,
  title = {A Theoretical Investigation of Reference Frames for the Planning of Speech Movements},
  author = {Guenther, F. H. and Hampson, M. and Johnson, D.},
  year = 1998,
  journal = {Psychological Review},
  volume = {105},
  number = {4},
  pages = {611--633}
}

@book{Haila-RetkeilynRikkaus-2004,
  title = {Retkeilyn {{Rikkaus}}},
  author = {Haila, Yrj{\"o}},
  year = 2004,
  publisher = {Kustannus Oy Taide},
  address = {Helsinki}
}

@inproceedings{HalleStevens-AnalysisSynthesis-1959,
  title = {Analysis by Synthesis},
  booktitle = {Proceedings of the {{Seminar}} on {{Speech Compression}} and {{Processing}}},
  author = {Halle, Morris and Stevens, Kenneth},
  editor = {{Wathen-Dunn}, W. and Woods, L. E.},
  year = 1959,
  volume = {II},
  pages = {paper D7}
}

@article{HalleStevens-SpeechRecognitionModel-1962,
  title = {Speech Recognition: {{A}} Model and a Program for Research},
  author = {Halle, M. and Stevens, K.},
  year = 1962,
  month = feb,
  journal = {Information Theory, IRE Transactions on},
  volume = {8},
  number = {2},
  pages = {155--159}
}

@inproceedings{HannukainenEtAl-FormantsVowelSounds-2006,
  title = {Formants and Vowel Sounds by Finite Element Method},
  booktitle = {The Phonetics Symposium 2006},
  author = {Hannukainen, A. and Lukkari, T. and Malinen, J. and Palo, P.},
  year = 2006,
  pages = {24--33},
  address = {Helsinki, Finland}
}

@article{HannukainenEtAl-VowelFormantsWave-2007,
  title = {Vowel Formants from the Wave Equation},
  author = {Hannukainen, A. and Lukkari, T. and Malinen, J. and Palo, P.},
  year = 2007,
  journal = {Journal of the Acoustical Society of America Express Letters},
  volume = {122},
  number = {1},
  pages = {EL1--EL7}
}

@inproceedings{HarandiEtAl-SubjectSpecificBiomechanicalModelling-2014,
  title = {Subject-{{Specific Biomechanical Modelling}} of the {{Tongue}}: {{Analysis}} of {{Muscle Activations During Speech}}},
  booktitle = {Proceedings of the 10th {{International Seminar}} on {{Speech Production}}},
  author = {Harandi, N. M. and Woo, J. and Stone, M. and Abugharbieh, R. and Fels, S.},
  year = 2014,
  month = apr,
  address = {Cologne}
}

@article{Hardcastle-UseElectropalatographyPhonetic-1972,
  title = {The {{Use}} of {{Electropalatography}} in {{Phonetic Research}}},
  author = {Hardcastle, W. J.},
  year = 1972,
  journal = {Phonetica},
  volume = {25},
  pages = {197--215}
}

@book{HardcastleHewlett-CoarticulationTheoryData-2000,
  title = {Coarticulation: {{Theory}}, Data and Techniques},
  editor = {Hardcastle, William J. and Hewlett, Nigel},
  year = 2000,
  publisher = {Cambridge University Press},
  address = {Cambridge}
}

@book{HardyEaston-EthicalSlut-2017,
  title = {The {{Ethical Slut}}},
  author = {Hardy, Janet W. and Easton, Dossie},
  year = 2017,
  edition = {3rd edition},
  publisher = {Ten Speed Press}
}

@article{HavuMalinen-CayleyTransformTime-2007,
  title = {The {{Cayley}} Transform as a Time Discretization Scheme},
  author = {Havu, V. and Malinen, J.},
  year = 2007,
  journal = {Numerical Functional Analysis and Optimization},
  volume = {28},
  number = {7 \& 8},
  pages = {825--851}
}

@book{Heffner-GeneralPhonetics-1950,
  title = {General Phonetics},
  author = {Heffner, R-M. S.},
  year = 1950,
  publisher = {The University of Wisconsin Press},
  address = {Madison, WI}
}

@inproceedings{HejnaEtAl-GlottalSqueaksCV-2016,
  title = {Glottal Squeaks in {{CV}} Sequences},
  booktitle = {Proceedings of Interspeech 2016},
  author = {Hejna, M. and Palo, P. and Moisik, S.},
  year = 2016,
  pages = {1136--1140},
  address = {San Francisco}
}

@article{HeldnerEdlund-PausesGapsOverlaps-2010,
  title = {Pauses, Gaps and Overlaps in Conversations},
  author = {Heldner, M. and Edlund, J.},
  year = 2010,
  journal = {Journal of Phonetics},
  volume = {38},
  number = {4},
  pages = {555--568},
  issn = {0095-4470}
}

@book{Helmholtz-LehreTonempfindungenAls-1863,
  title = {Die {{Lehre}} von Den {{Tonempfindungen}} Als Physiologische {{Grundlage}} F\"ur Die {{Theorie}} Der {{Musik}}},
  author = {Helmholtz, H. L. F.},
  year = 1863,
  publisher = {Braunschweig: F. Vieweg}
}

@book{HewlettBeck-IntroductionSciencePhonetics-2006,
  title = {An {{Introduction}} to the {{Science}} of {{Phonetics}}},
  author = {Hewlett, N. and Beck, J.},
  year = 2006,
  publisher = {Routledge}
}

@phdthesis{Heyne-InfluenceFirstLanguage-2016,
  title = {The Influence of {{First Language}} on Playing Brass Instruments: {{An}} Ultrasound Study of {{Tongan}} and {{New Zealand}} Trombonists},
  author = {Heyne, M.},
  year = 2016,
  school = {University of Cantebury, New Zealand}
}

@article{HeyneEtAl-NativeLanguageInfluence-2019,
  title = {Native {{Language Influence}} on {{Brass Instrument Performance}}: {{An Application}} of {{Generalized Additive Mixed Models}} ({{GAMMs}}) to {{Midsagittal Ultrasound Images}} of the {{Tongue}}},
  author = {Heyne, M. and Derrick, D. and {Al-Tamimi}, J.},
  year = 2019,
  journal = {Frontiers in Psychology},
  volume = {10},
  pages = {2597}
}

@book{HixonEtAl-PreclinicalSpeechScience-2018,
  title = {Preclinical {{Speech Science}}: {{Anatomy}}, {{Physiology}}, {{Acoustics}}, and {{Perception}}, {{Third Edition}}},
  shorttitle = {Preclinical {{Speech Science}}},
  author = {Hixon, Thomas J. and Weismer, Gary and Hoit, Jeannette D.},
  year = 2018,
  month = aug,
  publisher = {Plural Publishing},
  abstract = {Preclinical Speech Science: Anatomy, Physiology, Acoustics, and Perception, Third Edition~is a high-quality text for undergraduate and graduate courses in speech and hearing science. Written in a user-friendly style by distinguished scientists/clinicians who have taught the course to thousands of students at premier academic programs, it is the text of choice for instructors and students. Additionally, it is applicable to a broad range of courses that cover the anatomy and physiology of speech production, speech acoustics, and swallowing as well as those that cover the hearing mechanism, psychoacoustics, and speech perception.The material in this book is designed to help future speech-language pathologists and audiologists to understand the science that underpins their work and provide a framework for the evaluation and management of their future clients. It provides all the information students need to be fully ready for their clinical practicum training.KEY FEATURES:Describes scientific principles explicitly and in translational terms that emphasize their relevance to clinical practice.Features beautiful original, full-color illustrations designed to be instructive learning tools.Incorporates analogies that aid thinking about processes from different perspectives.Features \&quot;sidetracks\&quot; that contain clinical insights and relate interesting historical and contemporary facts to the discipline of speech and hearing science.Provides a framework for conceptualizing the uses, subsystems, and levels of observation of speech production, hearing, and swallowing.Includes material that is ideal for preparing both undergraduates and graduates for clinical study.NEW TO THE THIRD EDITION:Three new, up-to-date, and comprehensive chapters on auditory anatomy and physiology, auditory psychophysics, and speech physiology measurement and analysis.All chapters fully revised, including updated references and new full-color, detailed images.*Disclaimer: Please note that ancillary content (such as documents,     audio, and video, etc.) may not be included as published in the     original print version of this book.},
  googlebooks = {SDFtDwAAQBAJ},
  isbn = {978-1-63550-062-2},
  langid = {english},
  keywords = {Medical / Audiology & Speech Pathology}
}

@incollection{HooleNguyen-ElectromagneticArticulographyCoarticulation-1999,
  title = {Electromagnetic Articulography in Coarticulation Research},
  booktitle = {Coarticulation: {{Theory}}, {{Data}} and {{Techniques}}},
  author = {Hoole, P. and Nguyen, N.},
  editor = {Hardcastle, W. J. and Hewlett, N.},
  year = 1999,
  pages = {260--269},
  publisher = {Cambridge University Press}
}

@article{HooleZierdt-FivedimensionalArticulography-2010,
  title = {Five-Dimensional Articulography},
  author = {Hoole, P. and Zierdt, A.},
  year = 2010,
  journal = {Speech motor control},
  pages = {331--349}
}

@mastersthesis{Horbatiuk-FourierWaveletMethods-2011,
  title = {Fourier \& {{Wavelet Methods}} for {{Finding Speech Onset Latencies}}},
  author = {Horbatiuk, I.},
  year = 2011,
  school = {McMaster University}
}

@article{HornSchunck-DeterminingOpticalFlow-1981,
  title = {Determining {{Optical Flow}}},
  author = {Horn, Berthold K. P. and Schunck, Brian G.},
  year = 1981,
  journal = {Artificial Intelligence},
  volume = {17},
  pages = {185--203}
}

@article{HoudeJordan-SensorimotorAdaptationSpeech-1998,
  title = {Sensorimotor Adaptation in Speech Production},
  author = {Houde, J. and Jordan, M.},
  year = 1998,
  journal = {Science},
  volume = {279},
  pages = {1213}
}

@misc{Hueber-UltraspeechtoolsAcquisitionProcessing-2013,
  title = {Ultraspeech-Tools {{Acquisition}}, Processing and Visualization of Ultrasound Speech Data for Phonetics and Speech Therapy},
  author = {Hueber, Thomas},
  year = 2013,
  address = {Edinburgh, UK}
}

@misc{HueberEtAl-AcquisitionUltrasoundVideo-2008,
  title = {Acquisition of Ultrasound, Video and Acoustic Speech Data for a Silent-Speech Interface Application},
  author = {Hueber, Thomas and Chollet, G{\'e}rard and Denby, Bruce and Stone, Maureen},
  year = 2008,
  url = {https://www.academia.edu/download/42421083/issp2008-85.pdf},
  urldate = {2024-09-04},
  file = {/home/jpalo/Zotero/storage/VVDILC97/Hueber et al. - 2008 - Acquisition of ultrasound, video and acoustic speech data for a silent-speech interface application.pdf}
}

@article{HueberEtAl-AcquisitionUltrasoundVideo-2008a,
  title = {Acquisition of Ultrasound, Video and Acoustic Speech Data for a Silent-Speech Interface Application},
  author = {Hueber, Thomas and Chollet, G{\'e}rard and Denby, Bruce and Stone, Maureen},
  year = 2008,
  journal = {Proc. of ISSP},
  pages = {365--369},
  url = {https://www.academia.edu/download/42421083/issp2008-85.pdf},
  urldate = {2024-09-04},
  file = {/home/jpalo/Zotero/storage/D8MZ9E7B/Hueber et al. - 2008 - Acquisition of ultrasound, video and acoustic speech data for a silent-speech interface application.pdf}
}

@article{HueberEtAl-DevelopmentSilentSpeech-2010,
  title = {Development of a Silent Speech Interface Driven by Ultrasound and Optical Images of the Tongue and Lips},
  author = {Hueber, T. and Benaroya, E.-L. and Chollet, G. and Denby, B. and Dreyfus, G. and Stone, M.},
  year = 2010,
  journal = {Speech Communication},
  volume = {52},
  number = {4},
  pages = {288--300},
  issn = {0167-6393},
  keywords = {Visual phone recognition}
}

@patent{HueberEtAl-DeviceReconstructingSpeech-2011,
  title = {Device for Reconstructing Speech by Ultrasonically Probing the Vocal Apparatus},
  author = {Hueber, Thomas and Denby, Bruce and DREYFUS, G{\'e}rald and Dubois, R{\'e}mi and Roussel, Pierre},
  year = 2011,
  month = mar,
  number = {WO2011032688A1},
  url = {https://patents.google.com/patent/WO2011032688A1/en},
  urldate = {2024-09-04},
  abstract = {The invention relates to a portable device for speech recognition and/or reconstruction by ultrasonically probing the vocal apparatus, wherein the device comprises at least one ultrasonic transducer (20) for generating an ultrasonic wave and receiving a wave reflected by the vocal apparatus of the user, and means for analyzing a signal generated by the ultrasonic transducer, characterized in that the device comprises locating means (21; 23) for determining a relative position of the ultrasonic transducer in relation to the skull of the user.},
  assignee = {Universite Pierre Et Marie Curie (Paris 6), Centre National De La Recherche Scientifique},
  nationality = {WO},
  keywords = {relative,skull,speech,ultrasonic,user}
}

@inproceedings{HueberEtAl-EIGENTONGUEFEATUREEXTRACTION-2007,
  title = {{{EIGENTONGUE FEATURE EXTRACTION FOR AN ULTRASOUND-BASED SILENT SPEECH INTERFACE}}},
  booktitle = {Proceedings of {{ICASSP}} 2007},
  author = {Hueber, T. and Aversano, G. and Chollet, G. and Denby, B. and Dreyfus, G. and Oussar, Y. and Roussel, P. and Stone, M.},
  year = 2007,
  pages = {I: 1245 -- 1248}
}

@article{IndefreyLevelt-SpatialTemporalSignatures-2004,
  title = {The Spatial and Temporal Signatures of Word Production Components},
  author = {Indefrey, P. and Levelt, W.J.M.},
  year = 2004,
  journal = {Cognition},
  volume = {92},
  number = {1},
  pages = {101--144}
}

@misc{InternationalPhoneticAssociation-LinksPhoneticsResources-2025,
  title = {Links to {{Phonetics Resources}} \textbar{} {{International Phonetic Association}}},
  author = {International Phonetic Association},
  year = 2025,
  url = {https://www.internationalphoneticassociation.org/content/links-phonetics-resources},
  urldate = {2025-08-17},
  file = {/home/jpalo/Zotero/storage/AXWYFG9C/links-phonetics-resources.html}
}

@article{IzdebskiShipp-MinimalReactionTimes-1978,
  title = {Minimal Reaction Times for Phonatory Initiation},
  author = {Izdebski, K. and Shipp, T.},
  year = 1978,
  journal = {Journal of Speech and Hearing Research},
  volume = {21},
  number = {4},
  pages = {638--651}
}

@article{JacewiczEtAl-BetweenspeakerWithinspeakerVariation-2010,
  title = {Between-Speaker and within-Speaker Variation in Speech Tempo of {{American English}}},
  author = {Jacewicz, Ewa and Fox, Robert Allen and Wei, Lai},
  year = 2010,
  journal = {The Journal of the Acoustical Society of America},
  volume = {128},
  number = {2},
  pages = {839--850},
  issn = {0001-4966},
  doi = {10.1121/1.3459842},
  abstract = {This study characterizes the speech tempo (articulation rate, excluding pauses) of two distinct varieties of American English taking into account both between-speaker and within-speaker variation. Each of 192 speakers from Wisconsin (the northern variety) and from North Carolina (the southern variety), men and women, ranging in age from children to old adults, read a set of sentences and produced a spontaneous unconstrained talk. Articulation rate in spontaneous speech was modeled using fixed-mixed effects analyses. The models explored the effects of the between-speaker factors dialect, age and gender and included each phrase and its length as a source of both between- and within-speaker variation. The major findings are: (1) Wisconsin speakers speak significantly faster and produce shorter phrases than North Carolina speakers; (2) speech tempo changes across the lifespan, being fastest for individuals in their 40s; (3) men speak faster than women and this effect is not related to the length of phrases they produce. Articulation rate in reading was slower than in speaking and the effects of gender and age also differed in reading and spontaneous speech. The effects of dialect in reading remained the same, showing again that Wisconsin speakers had faster articulation rates than did North Carolina speakers.},
  pmcid = {PMC2933259},
  pmid = {20707453},
  file = {/home/jpalo/Zotero/storage/F2382CEJ/Jacewicz et al. - 2010 - Between-speaker and within-speaker variation in sp.pdf}
}

@misc{Jakosuo-UKKreittiKoliltaKiilopaalle-2018,
  title = {{{UKK-reitti}} ({{Kolilta Kiilop\"a\"alle}})},
  author = {Jakosuo, Minna},
  year = 2018,
  url = {https://aloneinthewild.net/2018/05/30/ukk-reitti-kolilta-kiilopaalle/},
  urldate = {2020-07-13}
}

@incollection{JannedyEtAl-ArticulationUsualEvaluating-2010,
  title = {Articulation {{Beyond}} the {{Usual}}: {{Evaluating}} the {{Fastest German Speaker}} under {{Laboratory Conditions}}},
  booktitle = {Between the Regular and the Particular in Speech and Language},
  author = {Jannedy, S. and Fuchs, S. and Weirich, M.},
  editor = {Fuchs, S. and Hoole, P. and Mooshammer, C. and Zygis, M.},
  year = 2010,
  pages = {205--234},
  publisher = {Peter Lang Verlag}
}

@book{Janson-MoominsummerMadness-2019,
  title = {Moominsummer {{Madness}}},
  author = {Janson, Tove},
  year = 2019
}

@article{Jenner-ArticulatorySettingGenealogies-2001,
  title = {'{{Articulatory}} Setting': Genealogies of an Idea},
  author = {Jenner, B.},
  year = 2001,
  journal = {Historiographica Linguistica},
  pages = {121--141}
}

@article{Jensen-GaltonLegacyResearch-2002,
  title = {Galton's {{Legacy}} to {{Research}} on {{Intelligence}}},
  author = {Jensen, A. R.},
  year = 2002,
  journal = {Journal of Biosocial Science},
  volume = {34},
  number = {2},
  pages = {145--172}
}

@article{JescheniakLevelt-WordFrequencyEffects-1994,
  title = {Word Frequency Effects in Speech Production: {{Retrieval}} of Syntactic Information and of Phonological Form},
  author = {Jescheniak, J. D. and Levelt, W. J. M.},
  year = 1994,
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {20},
  pages = {824--843}
}

@book{Johnson-NumericalSolutionPartial-1987,
  title = {Numerical {{Solution}} of {{Partial Differential Equations}} by the {{Finite Element Method}}},
  author = {Johnson, C.},
  year = 1987,
  publisher = {Cambridge University Press}
}

@article{Jones-RadiographyPronunciation-1929,
  title = {Radiography and Pronunciation},
  author = {Jones, S.},
  year = 1929,
  journal = {The British Journal of Radiology},
  volume = {2},
  number = {2},
  pages = {149--150}
}

@article{JouenEtAl-MicrostateERPAnalyses-2021,
  title = {Microstate {{ERP Analyses}} to {{Pinpoint}} the {{Articulatory Onset}} in {{Speech Production}}},
  author = {Jouen, Anne-Lise and Lancheros, Monica and Laganaro, Marina},
  year = 2021,
  journal = {Brain Topography},
  volume = {34},
  pages = {29--40}
}

@article{KabakoffEtAl-ComparingMetricsQuantification-2023,
  title = {Comparing Metrics for Quantification of Children's Tongue Shape Complexity Using Ultrasound Imaging},
  author = {Kabakoff, Heather and Beames, Sam Pearl and Tiede, Mark and Whalen, D. H. and Preston, Jonathan L. and McAllister, Tara},
  year = 2023,
  month = feb,
  journal = {Clinical Linguistics \& Phonetics},
  volume = {37},
  number = {2},
  pages = {169--195},
  publisher = {Taylor \& Francis},
  issn = {0269-9206},
  doi = {10.1080/02699206.2022.2039300},
  url = {https://doi.org/10.1080/02699206.2022.2039300},
  urldate = {2024-06-18},
  abstract = {Speech sound disorders can pose a challenge to communication in children that may persist into adulthood. As some speech sounds are known to require differential control of anterior versus posterior regions of the tongue body, valid measurement of the degree of differentiation of a given tongue shape has the potential to shed light on development of motor skill in typical and disordered speakers. The current study sought to compare the success of multiple techniques in quantifying tongue shape complexity as an index of degree of lingual differentiation in child and adult speakers. Using a pre-existing data set of ultrasound images of tongue shapes from adult speakers producing a variety of phonemes, we compared the extent to which three metrics of tongue shape complexity differed across phonemes/phoneme classes that were expected to differ in articulatory complexity. We then repeated this process with ultrasound tongue shapes produced by a sample of young children. The results of these comparisons suggested that a modified curvature index and a metric representing the number of inflection points best reflected small changes in tongue shapes across individuals differing in vocal tract size. Ultimately, these metrics have the potential to reveal delays in motor skill in young children, which could inform assessment procedures and treatment decisions for children with speech delays and disorders.},
  pmid = {35243947},
  keywords = {lingual differentiation,Speech development,ultrasound imaging},
  file = {/home/jpalo/Zotero/storage/5SHL9DIJ/Kabakoff et al. - 2023 - Comparing metrics for quantification of children’s.pdf}
}

@article{KadambiEtAl-TunableForcedAlignment-2025,
  title = {A {{Tunable Forced Alignment System Based}} on {{Deep Learning}}: {{Applications}} to {{Child Speech}}},
  shorttitle = {A {{Tunable Forced Alignment System Based}} on {{Deep Learning}}},
  author = {Kadambi, Prad and Mahr, Tristan J. and Hustad, Katherine C. and Berisha, Visar},
  year = 2025,
  journal = {Journal of Speech, Language, and Hearing Research},
  publisher = {American Speech-Language-Hearing Association},
  doi = {10.1044/2024_JSLHR-24-00347},
  url = {https://pubs.asha.org/doi/abs/10.1044/2024_JSLHR-24-00347},
  urldate = {2025-05-17},
  abstract = {Purpose: Phonetic forced alignment has a multitude of applications in automated analysis of speech, particularly in studying nonstandard speech such as children's speech. Manual alignment is tedious but serves as the gold standard for clinical-grade alignment. Current tools do not support direct training on manual alignments. Thus, a trainable speaker adaptive phonetic forced alignment system, Wav2TextGrid, was developed for children's speech. The source code for the method is publicly available along with a graphical user interface at https://github.com/pkadambi/Wav2TextGrid. Method: We propose a trainable, speaker-adaptive, neural forced aligner developed using a corpus of 42 neurotypical children from 3 to 6 years of age. Evaluation on both child speech and on the TIMIT corpus was performed to demonstrate aligner performance across age and dialectal variations. Results: The trainable alignment tool markedly improved accuracy over baseline for several alignment quality metrics, for all phoneme categories. Accuracy for plosives and affricates in children's speech improved more than 40\% over baseline. Performance matched existing methods using approximately 13 min of labeled data, while approximately 45--60 min of labeled alignments yielded significant improvement. Conclusion: The Wav2TextGrid tool allows alternate alignment workflows where the forced alignments, via training, are directly tailored to match clinical-grade, manually provided alignments. Supplemental Material: https://doi.org/10.23641/asha.28593971},
  file = {/home/jpalo/Zotero/storage/MZGBJR5L/Kadambi et al. - 2025 - A Tunable Forced Alignment System Based on Deep Learning Applications to Child Speech.pdf}
}

@inproceedings{KapusinskiRosenquist-BriefHistoryVoice-1973,
  title = {A Brief History of the Voice Key},
  booktitle = {Proceedings of the {{Annual Convention}} of the {{American Psychological Association}}},
  author = {Kapusinski, D. A. and Rosenquist, H. S.},
  year = 1973,
  volume = {8},
  pages = {945--946}
}

@article{KarthikEtAl-AutomaticTongueSurface-2020,
  title = {Automatic Tongue Surface Extraction from Three-Dimensional Ultrasound Vocal Tract Images},
  author = {Karthik, Enamundram M. V. Naga and Karimi, Elham and Lulich, Steven M. and Laporte, Catherine},
  year = 2020,
  journal = {Journal of the Acoustical Society of America},
  volume = {147},
  number = {3},
  pages = {1623--1633}
}

@incollection{Kasper-Politeness-2009,
  title = {Politeness},
  booktitle = {The {{Pragmatics}} of {{Interaction}}},
  author = {Kasper, G.},
  editor = {D'hondt, S. and {\"O}stman, J.-O. and Verschueren, J.},
  year = 2009,
  pages = {157--173},
  publisher = {John Bemjamins Publishing Co}
}

@article{KasperEtAl-InformationalAccountAnticipatory-2026,
  title = {An Informational Account of Anticipatory Coarticulation -- Theoretical Considerations and Empirical Plausibility},
  author = {Kasper, Johannes and Kell, Christian A. and Perrier, Pascal},
  year = 2026,
  month = may,
  journal = {Journal of Neurolinguistics},
  volume = {78},
  pages = {101298},
  issn = {0911-6044},
  doi = {10.1016/j.jneuroling.2025.101298},
  url = {https://www.sciencedirect.com/science/article/pii/S0911604425000545},
  urldate = {2025-12-08},
  abstract = {In speech, the realization of a phoneme is influenced by its preceding and forthcoming neighbors, a phenomenon called coarticulation. While influences from past actions (carryover coarticulation) can be explained by a combination of physical causalities and motor planning, influences from future action goals can only be fully explained by active anticipatory processes in speech motor control. Current modeling accounts view coarticulation as a side effect of efficiency objectives of control or properties of the planning algorithm. Here, we propose that anticipatory coarticulation provides prospective sensory cues usable for feedback-dependent speech control. To support this notion, we characterized the dynamics of carryover and anticipatory coarticulation in three French speakers in both acoustic and articulator data as measured by Electromagnetic Articulography using General Additive Models. Our data show that anticipatory coarticulation effects dominate over carryover coarticulation effects and allow to reliably differentiate between upcoming phonemes at least 200 ms before the canonical phoneme onset. These findings are compatible with an anticipatory feedback-dependent control strategy, as sensory information about future actions is available early enough to influence ongoing actions in spite of sensorimotor delays. Such considerations weaken the necessity for fully fledged internal predictive forward models for the control of fast actions.},
  keywords = {Auditory and Somatosensory Feedback Control,Electromagnetic articulography,Formants,General additive model,Speech motor control,Speech production},
  file = {/home/jpalo/Zotero/storage/FX9KYZCB/Kasper et al. - 2026 - An informational account of anticipatory coarticulation – theoretical considerations and empirical p.pdf;/home/jpalo/Zotero/storage/Y7Z4M5I8/S0911604425000545.html}
}

@article{KawamotoEtAl-ArticulatoryPreparationDelayed-2008,
  title = {Articulatory Preparation in the Delayed Naming Task},
  author = {Kawamoto, Alan H. and Liu, Qiang and Mura, Keith and Sanchez, Adrianna},
  year = 2008,
  journal = {Journal of Memory and Language},
  volume = {58},
  number = {2},
  pages = {347--365}
}

@article{KawamotoEtAl-InitialPhonemeWhole-1998,
  title = {Initial Phoneme versus Whole Word Criterion to Initiate Pronunciation: {{Evidence}} Based on Response Latency and Initial Phoneme Duration},
  author = {Kawamoto, A. H. and Kello, C. T. and Jones, R. and Bame, K.},
  year = 1998,
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {24},
  pages = {862--885}
}

@article{KawanishiEtAl-ConsiderationAcousticCharacteristics-1996,
  title = {Consideration of the Acoustic Characteristics of the Pyriform Fossa in Transmission Line Model, {{3D-FEM}} Model and Realistic Model},
  author = {Kawanishi, Y. and Obuchi, M. and Dang, J. and Nakai, T. and Suzuki, H.},
  year = 1996,
  journal = {Tech. Rep. IEICE},
  volume = {EA96-12},
  number = {1},
  pages = {1--8}
}

@book{Kay-DarkestRoad-1987,
  title = {The {{Darkest Road}}},
  author = {Kay, Guy Gavriel},
  year = 1987,
  series = {The {{Fionavar Tapestry}}},
  volume = {3}
}

@book{Kay-SummerTree-1984,
  title = {The {{Summer Tree}}},
  author = {Kay, Guy Gavriel},
  year = 1984,
  series = {The {{Fionavar Tapestry}}},
  volume = {3}
}

@book{Kay-Tigana-1990,
  title = {Tigana},
  author = {Kay, Guy Gavriel},
  year = 1990
}

@book{Kay-WanderingFire-1986,
  title = {The {{Wandering Fire}}},
  author = {Kay, Guy Gavriel},
  year = 1986,
  series = {The {{Fionavar Tapestry}}},
  volume = {2}
}

@article{KelleyEtAl-MasonAlbertaPhoneticSegmenter-2024,
  title = {The {{Mason-Alberta Phonetic Segmenter}}: A Forced Alignment System Based on Deep Neural Networks and Interpolation},
  shorttitle = {The {{Mason-Alberta Phonetic Segmenter}}},
  author = {Kelley, Matthew C. and Perry, Scott James and Tucker, Benjamin V.},
  year = 2024,
  month = oct,
  journal = {Phonetica},
  volume = {81},
  number = {5},
  pages = {451--508},
  publisher = {De Gruyter Mouton},
  issn = {1423-0321},
  doi = {10.1515/phon-2024-0015},
  url = {https://www.degruyter.com/document/doi/10.1515/phon-2024-0015/html},
  urldate = {2024-12-09},
  abstract = {Given an orthographic transcription, forced alignment systems automatically determine boundaries between segments in speech, facilitating the use of large corpora. In the present paper, we introduce a neural network-based forced alignment system, the Mason-Alberta Phonetic Segmenter (MAPS). MAPS serves as a testbed for two possible improvements we pursue for forced alignment systems. The first is treating the acoustic model as a tagger, rather than a classifier, motivated by the common understanding that segments are not truly discrete and often overlap. The second is an interpolation technique to allow more precise boundaries than the typical 10\,ms limit in modern systems. During testing, all system configurations we trained significantly outperformed the state-of-the-art Montreal Forced Aligner in the 10\,ms boundary placement tolerance threshold. The greatest difference achieved was a 28.13\,\% relative performance increase. The Montreal Forced Aligner began to slightly outperform our models at around a 30\,ms tolerance. We also reflect on the training process for acoustic modeling in forced alignment, highlighting how the output targets for these models do not match phoneticians' conception of similarity between phones and that reconciling this tension may require rethinking the task and output targets or how speech itself should be segmented.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {acoustic models,automatic speech recognition,forced alignment,neural networks,phonetics,speech technology},
  file = {/home/jpalo/Zotero/storage/7TMCWQ9Y/Kelley et al. - 2024 - The Mason-Alberta Phonetic Segmenter a forced alignment system based on deep neural networks and in.pdf}
}

@inproceedings{KellyLochbaum-SpeechSynthesis-1962,
  title = {Speech Synthesis},
  booktitle = {Proceedings of the 4th {{International Congress}} on {{Acoustics}}},
  author = {Kelly, J.L. and Lochbaum, C.C.},
  year = 1962,
  pages = {Paper G42: 1-4}
}

@article{KelsoEtAl-FunctionallySpecificArticulatory-1984,
  title = {Functionally Specific Articulatory Adaptation to Jaw Perturbations during Speech: {{Evidence}} for Coordinative Structures},
  author = {Kelso, J. and Tuller, B. and {Vatikiotis-Bateson}, E. and Fowler, C.},
  year = 1984,
  journal = {Journal of Experimental Psychology},
  volume = {10},
  number = {6},
  pages = {812--832}
}

@article{Kelz-ArticulatoryBasisSecond-1971,
  title = {Articulatory Basis and Second Language Learning},
  author = {Kelz, H. P.},
  year = 1971,
  journal = {Phonetica},
  volume = {24},
  pages = {193--211}
}

@book{Kerr-Daggerspell-1986,
  title = {Daggerspell},
  author = {Kerr, Katherine},
  year = 1986,
  series = {Deverry},
  number = {1},
  publisher = {Grafton}
}

@book{Kerr-Darkspell-1988,
  title = {Darkspell},
  author = {Kerr, Katherine},
  year = 1988,
  series = {Deverry},
  number = {2},
  publisher = {Grafton}
}

@book{Kerr-Dawnspell-1989,
  title = {Dawnspell},
  author = {Kerr, Katherine},
  year = 1989,
  series = {Deverry},
  number = {3},
  publisher = {Grafton}
}

@book{Kerr-DaysAirDarkness-1994,
  title = {Days of {{Air}} and {{Darkness}}},
  author = {Kerr, Katherine},
  year = 1994,
  series = {Westlands},
  number = {4},
  publisher = {Bantam}
}

@book{Kerr-DaysBloodFire-1993,
  title = {Days of {{Blood}} and {{Fire}}},
  author = {Kerr, Katherine},
  year = 1993,
  series = {Westlands},
  number = {3},
  publisher = {Bantam}
}

@book{Kerr-Dragonspell-1990,
  title = {Dragonspell},
  author = {Kerr, Katherine},
  year = 1990,
  series = {Deverry},
  number = {4},
  publisher = {Grafton}
}

@book{Kerr-TimeExile-1991,
  title = {A {{Time}} of {{Exile}}},
  author = {Kerr, Katherine},
  year = 1991,
  series = {Westlands},
  number = {1},
  publisher = {Bantam}
}

@book{Kerr-TimeOmens-1992,
  title = {A {{Time}} of {{Omens}}},
  author = {Kerr, Katherine},
  year = 1992,
  series = {Westlands},
  number = {2},
  publisher = {Bantam}
}

@article{KesslerEtAl-PhoneticBiasesVoice-2002,
  title = {Phonetic {{Biases}} in {{Voice Key Response Time Measurements}}},
  author = {Kessler, B. and Treiman, R. and Mullenix, J.},
  year = 2002,
  journal = {Journal of Memory and Language},
  volume = {47},
  pages = {145--171}
}

@article{KeyserStevens-EnhancementOverlapSpeech-2006,
  title = {Enhancement and {{Overlap}} in the {{Speech Chain}}},
  author = {Keyser, Samuel Jay and Stevens, Kenneth N.},
  year = 2006,
  journal = {Language},
  volume = {82},
  number = {1},
  pages = {33--63},
  publisher = {Linguistic Society of America},
  issn = {1535-0665},
  url = {https://muse.jhu.edu/pub/24/article/194981},
  urldate = {2025-05-31},
  abstract = {, A model of speech production is proposed in which the input is a planning stage at which lexical items are arrayed, accompanied by the full panoply of phonological representations from distinctive features to their attendant tree structures. A set of instructions for control of the vocal tract is calculated leading to a sound output. Two parallel processes are involved in the calculation of these instructions, both of which replace the planning-stage representation by the appropriate motoric instructions. One of these processes is universal and involves replacing each distinctive feature with an appropriate set of motoric instructions, either unmodified or modified by the process of overlap. We postulate a parallel language-specific process that is sensitive to those features in danger of losing their perceptual saliency as a consequence of the environment in which they appear. This process, referred to as ENHANCEMENT, adds additional motoric instructions to enhance the saliency of the jeopardized features. We provide a number of examples to illustrate how enhancement works. We conclude from these examples that whereas defining gestures related to distinctive features are, in many instances, weakened or even absented from the speech stream, enhancement gestures, once added to the set of motoric instructions, appear never to be subject to obliteration by overlap.}
}

@article{KeyserStevens-EnhancementOverlapSpeech-2006a,
  title = {Enhancement and {{Overlap}} in the {{Speech Chain}}},
  author = {Keyser, Samuel Jay and Stevens, Kenneth N.},
  year = 2006,
  journal = {Language},
  volume = {82},
  number = {1},
  pages = {33--63},
  publisher = {Linguistic Society of America},
  issn = {1535-0665},
  url = {https://muse.jhu.edu/pub/24/article/194981},
  urldate = {2025-12-11},
  abstract = {A model of speech production is proposed in which the input is a planning stage at which lexical items are arrayed, accompanied by the full panoply of phonological representations from distinctive features to their attendant tree structures. A set of instructions for control of the vocal tract is calculated leading to a sound output. Two parallel processes are involved in the calculation of these instructions, both of which replace the planning-stage representation by the appropriate motoric instructions. One of these processes is universal and involves replacing each distinctive feature with an appropriate set of motoric instructions, either unmodified or modified by the process of overlap. We postulate a parallel language-specific process that is sensitive to those features in danger of losing their perceptual saliency as a consequence of the environment in which they appear. This process, referred to as  enhancement, adds additional motoric instructions to enhance the saliency of the jeopardized features. We provide a number of examples to illustrate how enhancement works. We conclude from these examples that whereas defining gestures related to distinctive features are, in many instances, weakened or even absented from the speech stream, enhancement gestures, once added to the set of motoric instructions, appear never to be subject to obliteration by overlap. *}
}

@article{KiefteNearey-ModelingConsonantcontextEffects-2017,
  title = {Modeling Consonant-Context Effects in a Large Database of Spontaneous Speech Recordings},
  author = {Kiefte, Michael and Nearey, Terrance M.},
  year = 2017,
  month = jul,
  journal = {The Journal of the Acoustical Society of America},
  volume = {142},
  number = {1},
  pages = {434--443},
  issn = {0001-4966},
  doi = {10.1121/1.4991022},
  url = {https://doi.org/10.1121/1.4991022},
  urldate = {2025-11-17},
  abstract = {Given recent interest in the analysis of naturally produced spontaneous speech, a large database of speech samples from the Canadian Maritimes was collected, processed, and analyzed with the primary aim of examining vowel-inherent spectral change in formant trajectories. Although it takes few resources to collect a large sample of audio recordings, the analysis of spontaneous speech introduces a number of difficulties compared to that of laboratory citation speech: Surrounding consonants may have a large influence on vowel formant frequencies and the distribution of consonant contexts is highly unbalanced. To overcome these problems, a statistical procedure inspired by that of Broad and Clermont [(2014). J. Phon. 47, 47--80] was developed to estimate the magnitude of both onset and coda effects on vowel formant frequencies. Estimates of vowel target formant frequencies and the parameters associated with consonant-context effects were allowed to vary freely across the duration of the vocalic portion of a syllable which facilitated the examination of vowel-inherent spectral change. Thirty-five hours of recorded speech samples from 223 speakers were automatically segmented and formant-frequency values were measured for all stressed vowels in the database. Consonant effects were accounted for to produce context-normalized vowel formant frequencies that varied across time.},
  file = {/home/jpalo/Zotero/storage/3B7VT2HV/Kiefte and Nearey - 2017 - Modeling consonant-context effects in a large database of spontaneous speech recordings.pdf;/home/jpalo/Zotero/storage/XKS9IP2D/1.html}
}

@article{KingEtAl-SpeechProductionKnowledge-2007,
  title = {Speech Production Knowledge in Automatic Speech Recognition},
  author = {King, S. and Frankel, J. and Livescu, K. and McDermott, E. and Richmond, K. and Wester, M.},
  year = 2007,
  journal = {Journal of the Acoustical Society of America},
  volume = {121},
  number = {2},
  pages = {723--742}
}

@inproceedings{KirkhamEtAl-CoregistrationSimultaneousHighspeed-2023,
  title = {Co-Registration of Simultaneous High-Speed Ultrasound and Electromagnetic Articulography for Speech Production Research},
  booktitle = {Proceedings of the 20th International Congress of Phonetic Sciences},
  author = {Kirkham, Sam and Strycharczuk, Patrycja and Gorman, Emily and Nagamine, Takayuki and Wrench, Alan},
  year = 2023,
  pages = {942--946}
}

@article{KittredgeEtAl-WhereEffectFrequency-1994,
  title = {Where Is the Effect of Frequency in Word Production? {{Insights}} from Aphasic Picture-Naming Errors},
  author = {Kittredge, A. K. and Dell, G. S. and Verkuilen, J. and Schwartz, M. F.},
  year = 1994,
  journal = {Cognitive neuropsychology},
  volume = {25},
  number = {4},
  pages = {463--492}
}

@article{KlappErwin-RelationProgrammingTime-1976,
  title = {Relation between Programming Time and Duration of the Response Being Programmed},
  author = {Klapp, {\relax ST} and Erwin, {\relax CI}},
  year = 1976,
  month = nov,
  journal = {Journal of experimental psychology. Human perception and performance},
  volume = {2},
  number = {4},
  pages = {591--598},
  doi = {10.1037//0096-1523.2.4.591}
}

@article{KlappEtAl-ImplicitSpeechReading-1973,
  title = {Implicit {{Speech}} in {{Reading}}: {{Reconsidered}}},
  author = {Klapp, S. T. and Anderson, W. G. and Berrian, R. W.},
  year = 1973,
  journal = {Journal of Experimental Psychology},
  volume = {100},
  number = {2},
  pages = {368--374},
  doi = {10.1037/h0035471}
}

@article{KochetovEtAl-PilotUltrasoundStudy-2012,
  title = {A Pilot Ultrasound Study of {{Kannada}} Lingual Articulations},
  author = {Kochetov, A. and Sreedevi, N. and Kasim, M and Manjula, R.},
  year = 2012,
  journal = {Journal of Indian Speech and Hearing},
  volume = {26},
  pages = {38--49}
}

@article{KoenigEtAl-SoundSpectrograph-1946,
  title = {The {{Sound Spectrograph}}},
  author = {Koenig, W. and Dunn, H. K. and Lacy, L. Y.},
  year = 1946,
  journal = {The Journal of the Acoustical Society of America},
  volume = {18},
  number = {1},
  pages = {19--49},
  issn = {0001-4966},
  doi = {10.1121/1.1916342},
  url = {https://doi.org/10.1121/1.1916342},
  urldate = {2025-05-14},
  abstract = {The sound spectrograph is a wave analyzer which produces a permanent visual record showing the distribution of energy in both frequency and time. This paper describes the operation of this device, and shows the mechanical arrangements and the electrical circuits in a particular model. Some of the problems encountered in this type of analysis are discussed, particularly those arising from the necessity for handling and portraying a wide range of component levels in a complex wave such as speech. Spectrograms are shown for a wide variety of sounds, including voice sounds, animal and bird sounds, music, frequency modulations, and miscellaneous familiar sounds.},
  file = {/home/jpalo/Zotero/storage/MKTD5HQX/Koenig et al. - 1946 - The Sound Spectrograph.pdf;/home/jpalo/Zotero/storage/CHVDNIJB/The-Sound-Spectrograph.html}
}

@article{KoppenhaverEtAl-RehabilitativeUltrasoundImaging-2009,
  title = {Rehabilitative Ultrasound Imaging Is a Valid Measure of Trunk Muscle Size and Activation during Most Isometric Sub-Maximal Contractions: A Systematic Review},
  author = {Koppenhaver, S. L. and Hebert, J. J. and Parent, E. C. and Fritz, J. M.},
  year = 2009,
  journal = {Australian Journal of Physiotherapy},
  volume = {55},
  number = {3},
  pages = {153--169},
  issn = {0004-9514}
}

@article{Krmpotić-AnatomischhistologischeUndFunktionelle-1958,
  title = {Anatomisch-Histologische Und Funktionelle {{Verh\"altnisse}} Des Rechten Und Des Linken {{Nervus}} Recurrens Mit {{R\"ucksicht}} Auf Die {{Geschwindigkeit}} Der {{Impulsleitung}} Bei Einer {{Ursprungsanomalie}} Der Rechten {{Schl\"usselbeinarterie}}},
  author = {Krmpoti{\'c}, J.},
  year = 1958,
  journal = {Arch. Ohr. - Nas. - Kehlk. Heilk.},
  volume = {173},
  pages = {490--496}
}

@article{Krmpotić-DonneesAnatomiqueHistologiques-1959,
  title = {Donne\'es Anatomique et Histologiques Relatives Aux Effecteurs Laryngo-Pharyngo-Buccaux},
  author = {Krmpoti{\'c}, J.},
  year = 1959,
  journal = {Rev. Laryngol.},
  volume = {11},
  pages = {829--848}
}

@article{Kroos-EvaluationMeasurementPrecision-2012,
  title = {Evaluation of the Measurement Precision in Three-Dimensional Electromagnetic Articulography ({{Carstens AG500}})},
  author = {Kroos, C.},
  year = 2012,
  journal = {Journal of Phonetics},
  volume = {40},
  pages = {453--465}
}

@article{Kuehn-CineradiographicInvestigationVelar-1976,
  title = {A {{Cineradiographic Investigation}} of {{Velar Movement Variables}} in {{Two Normals}}},
  author = {Kuehn, D. P.},
  year = 1976,
  journal = {The Cleft Palate Journal},
  volume = {13},
  pages = {88--103}
}

@incollection{KühnertNolan-OriginCoarticulation-1999,
  title = {The Origin of Coarticulation},
  booktitle = {Coarticulation: {{Theory}}, {{Data}} and {{Techniques}}},
  author = {K{\"u}hnert, B. and Nolan, F.},
  editor = {Hardcastle, W. J. and Hewlett, N.},
  year = 1999,
  pages = {7--30},
  publisher = {Cambridge University Press}
}

@book{Ladefoged-CoursePhonetics-1995,
  title = {A {{Course}} in {{Phonetics}}},
  author = {Ladefoged, P.},
  year = 1995,
  publisher = {Harcourt Brace Jovanovich, Inc.},
  address = {Orlando, Florida}
}

@book{Ladefoged-VowelsConsonants-2005,
  title = {Vowels and {{Consonants}}},
  author = {Ladefoged, Peter},
  year = 2005,
  publisher = {Wiley}
}

@inproceedings{Laine-ModellingLipRadiation-1982,
  title = {Modelling of Lip Radiation Impedance in {{Z-domain}}},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}} ({{ICASSP}} -82)},
  author = {Laine, U. K.},
  year = 1982,
  volume = {3}
}

@inproceedings{LammertEtAl-VocalTractCrossdistance-2013,
  title = {Vocal Tract Cross-Distance Estimation from Real-Time {{MRI}} Using Region-of-Interest Analysis},
  booktitle = {Interspeech 2013},
  author = {Lammert, A. and Ramanarayanan, V. and Proctor, M. and Narayanan, S.},
  year = 2013,
  pages = {959--962}
}

@misc{Lampén-LahdinKavelemaanMatkapaivakirja-2000,
  title = {L\"ahdin K\"avelem\"a\"an: {{Matkap\"aiv\"akirja Helsingist\"a J\"a\"amerelle}}},
  author = {Lamp{\'e}n, Erkki},
  year = 2000,
  url = {http://www.saunalahti.fi/~elampen/lahdin_kavelemaan/lahdin_kavelemaan.html},
  urldate = {2020-06-24}
}

@book{Lampén-NeljaRetkeaLapi-2009,
  title = {Nelj\"a Retke\"a L\"api {{Suomen}}: {{K\"avellen}}, Py\"or\"aillen, Hiiht\"aen, Meloen},
  author = {Lamp{\'e}n, Erkki},
  year = 2009,
  publisher = {Tammi},
  address = {Helsinki}
}

@article{LaporteMenard-MultihypothesisTrackingTongue-2018,
  title = {Multi-Hypothesis Tracking of the Tongue Surface in Ultrasound Video Recordings of Normal and Impaired Speech},
  author = {Laporte, Catherine and M{\'e}nard, Lucie},
  year = 2018,
  journal = {Medical Image Analysis},
  volume = {44},
  pages = {98--114},
  issn = {1361-8423},
  doi = {10.1016/j.media.2017.12.003},
  abstract = {Characterizing tongue shape and motion, as they appear in real-time ultrasound (US) images, is of interest to the study of healthy and impaired speech production. Quantitative anlaysis of tongue shape and motion requires that the tongue surface be extracted in each frame of US speech recordings. While the literature proposes several automated methods for this purpose, these either require large or very well matched training sets, or lack robustness in the presence of rapid tongue motion. This paper presents a new robust method for tongue tracking in US images that combines simple tongue shape and motion models derived from a small training data set with a highly flexible active contour (snake) representation and maintains multiple possible hypotheses as to the correct tongue contour via a particle filtering algorithm. The method was tested on a database of large free speech recordings from healthy and impaired speakers and its accuracy was measured against the manual segmentations obtained for every image in the database. The proposed method achieved mean sum of distances errors of 1.69\,\textpm\,1.10~mm, and its accuracy was not highly sensitive to training set composition. Furthermore, the proposed method showed improved accuracy, both in terms of mean sum of distances error and in terms of linguistically meaningful shape indices, compared to the three publicly available tongue tracking software packages Edgetrak, TongueTrack and Autotrace.},
  langid = {english},
  pmid = {29232649},
  keywords = {Algorithms,Deep Learning,Humans,Image Interpretation Computer-Assisted,Image segmentation,Neural Networks Computer,Particle filter,Speech,Speech Disorders,Tongue,Tracking,Ultrasonography,Ultrasound,Video Recording}
}

@inproceedings{LaprieEtAl-GeometricArticulatoryModel-2014,
  title = {Geometric Articulatory Model Adapted to the Production of Consonants},
  booktitle = {Proceedings of 10th {{ISSP}}},
  author = {Laprie, Y. and Vaxelaire, B. and Cadot, M.},
  year = 2014,
  pages = {253--256}
}

@incollection{Lashley-ProblemSerialOrder-1951,
  title = {The Problem of Serial Order in Behavior},
  booktitle = {Cerebral {{Mechanisms}} in {{Behavior}}},
  author = {Lashley, K. S.},
  editor = {Jeffress, L. A.},
  year = 1951,
  pages = {112--136},
  publisher = {John Wiley \& Sons},
  address = {New York}
}

@article{LaurettaEtAl-CrossLinguisticBracingLingual-2017,
  title = {Cross-{{Linguistic Bracing}}: {{A Lingual Ultrasound Study}} of {{Six Languages}}},
  shorttitle = {Cross-{{Linguistic Bracing}}},
  author = {Lauretta, Cheng and Schellenberg, Murray and Gick, Bryan},
  year = 2017,
  month = aug,
  journal = {Canadian Acoustics},
  volume = {45},
  number = {3},
  pages = {186--187},
  issn = {2291-1391},
  url = {https://jcaa.caa-aca.ca/index.php/jcaa/article/view/3114},
  urldate = {2024-10-28},
  abstract = {Lateral bracing refers to contact of the sides of the tongue along the upper molars or palate; evidence from articulatory analysis of native English speakers as well as 3D biomechanical simulations suggests that bracing involves mechanical support which occurs consistently throughout speech [Gick et al. 2017.~J Speech Lang Hear Res. 60(3):494-506]. Release of lateral bracing occurs only during some lateral consonants and low vowels. The current study tests for the presence of active lateral bracing in seven languages: Cantonese, English, Korean, Mandarin, Portuguese, Spanish, and Turkish. Ten native speakers of these languages (2 each for English, Mandarin and Korean and one each for the other languages) read aloud passages of the North Wind and the Sun [Handbook of the IPA, 1999] while a coronal ultrasound video of their tongue was recorded. Tracings were made from still images of the M-mode ultrasound videos, and measurements of the vertical motion of the tongue midline and both edges were taken. The percentage of time the tongue is not laterally braced was calculated. Active lateral bracing is implicated if the left and right edges of the tongue are less variable in vertical motion than midline and/or positioned at a stable baseline height for a larger percentage of time than they are lowered. Preliminary analysis supports the hypothesis that tongue bracing in speech exists regardless of language.},
  copyright = {Copyright (c) 2017 Cheng Lauretta, Murray Schellenberg, Bryan Gick},
  langid = {english},
  file = {/home/jpalo/Zotero/storage/HMYHC3BF/Lauretta et al. - 2017 - Cross-Linguistic Bracing A Lingual Ultrasound Study of Six Languages.pdf}
}

@article{Laver-ConceptArticulatorySettings-1978a,
  title = {The Concept of Articulatory Settings: An Historical Survey},
  author = {Laver, John},
  year = 1978,
  journal = {Historiographica Linguistica},
  volume = {V},
  pages = {1--14}
}

@article{LawsonEtAl-BunchedPromotesVowel-2013,
  title = {Bunched /r/ Promotes Vowel Merger to Schwar: {{An Ultrasound Tongue Imaging}} Study of {{Scottish}} Sociophonetic Variation},
  author = {Lawson, E. and Scobbie, J. M. and {Stuart-Smith}, J.},
  year = 2013,
  journal = {Journal of Phonetics},
  volume = {41},
  number = {3--4},
  pages = {198--210}
}

@article{LawsonEtAl-SocialStratificationTongue-2011,
  title = {The Social Stratification of Tongue Shape for Postvocalic /r/ in {{Scottish English}}},
  author = {Lawson, E. and Scobbie, J. M. and {Stuart-Smith}, J.},
  year = 2011,
  journal = {Journal of Phonetics},
  volume = {15},
  number = {2},
  pages = {256--268}
}

@article{LeeDoherty-SpeakingRateArticulation-2017,
  title = {Speaking Rate and Articulation Rateof Native Speakers of {{Irish English}}},
  author = {Lee, A. and Doherty, R.},
  year = 2017,
  journal = {Speech, Language and Hearing},
  volume = {20},
  number = {4},
  pages = {206--211}
}

@book{Lehiste-Suprasegmentals-1970,
  title = {Suprasegmentals},
  author = {Lehiste, Ilse},
  year = 1970,
  publisher = {M.I.T. Press},
  address = {Cambridge, Massachusetts}
}

@book{Lehtola-SaamelaisetHistoriaYhteiskunta-2015,
  title = {Saamelaiset -- Historia, Yhteiskunta, Taide},
  author = {Lehtola, Veli-Pekka},
  year = 2015,
  publisher = {Kustannus-Puntsi},
  address = {Inari}
}

@book{Lenneberg-BiologicalFoundationsLanguage-1967,
  title = {Biological {{Foundations}} of {{Language}}},
  author = {Lenneberg, E. H.},
  year = 1967,
  publisher = {John Wiley \& Sons},
  address = {New York}
}

@article{LennesEtAl-ComparingPitchDistributions-2015,
  title = {Comparing Pitch Distributions Using {{Praat}} and {{R}}},
  author = {Lennes, M. and Stevanovic, M. and Aalto, D. and Palo, P.},
  year = 2015,
  journal = {The Phonetician},
  volume = {111--112},
  pages = {35--53}
}

@inproceedings{LennesEtAl-SuomenVenajanJa-2008,
  title = {{Suomen, ven\"aj\"an ja hollannin puhujien perustaajuusjakaumat: alustavia tuloksia}},
  booktitle = {{Fonetiikan p\"aiv\"at 2008}},
  author = {Lennes, M. and Aalto, D. and Palo, P.},
  year = 2008,
  pages = {147--156},
  address = {Tampere},
  langid = {finnish}
}

@book{Levelt-SpeakingIntentionArticulation-1989,
  title = {Speaking, {{From Intention}} to {{Articulation}}},
  author = {Levelt, W. J. M.},
  year = 1989,
  publisher = {The MIT Press}
}

@article{LeveltEtAl-TheoryLexicalAccess-1999,
  title = {A Theory of Lexical Access in Speech Production},
  author = {Levelt, W. J. M. and Roelofs, A. and Meyer, A. S.},
  year = 1999,
  journal = {Behavioral and Brain Sciences},
  volume = {22},
  pages = {1--75}
}

@inproceedings{LimEtAl-ImprovedDepictionTissue-2016,
  title = {Improved {{Depiction}} of {{Tissue Boundaries}} in {{Vocal Tract Real-time MRI}} Using {{Automatic Off-resonance Correction}}},
  booktitle = {Interspeech 2016},
  author = {Lim, Y. and Lingala, S. G. and Toutios, A. and Narayanan, S. and Nayak, K. S.},
  year = 2016,
  pages = {1765--1769},
  address = {San Francisco, USA}
}

@article{LingalaEtAl-FastFlexibleMRI-2017,
  title = {A Fast and Flexible {{MRI}} System for the Study of Dynamic Vocal Tract Shaping.},
  author = {Lingala, S. G. and Zhu, Y. and Kim, Y.-C. and Toutios, A. and Narayanan, S. and Nayak, K. S.},
  year = 2017,
  journal = {Magnetic resonance in medicine},
  volume = {77 1},
  pages = {112--125}
}

@article{LiuEtAl-LateralTongueBracing-2023,
  title = {Lateral Tongue Bracing as a Universal Postural Basis for Speech},
  author = {Liu, Yadong and Tong, Felicia and de Boer, Gillian and Gick, Bryan},
  year = 2023,
  month = dec,
  journal = {Journal of the International Phonetic Association},
  volume = {53},
  number = {3},
  pages = {712--727},
  issn = {0025-1003, 1475-3502},
  doi = {10.1017/S0025100321000335},
  url = {https://www.cambridge.org/core/journals/journal-of-the-international-phonetic-association/article/lateral-tongue-bracing-as-a-universal-postural-basis-for-speech/EBE7469B7D8EAE9F1EB7FA20C077835C},
  urldate = {2024-11-04},
  abstract = {Lateral bracing refers to an intentional tongue posture whereby the sides of the tongue make contact with the sides of the palate and the upper molars. While previous research on this topic has focused mostly on English, the present study tests the hypothesis that lateral bracing provides a fundamental postural basis for speech and is present across languages. We predicted that, across multiple languages, the sides of the tongue should be more stable than the center and should stay in a relatively high position in the mouth throughout most of running speech. Using coronal ultrasound imaging, we measured tongue movement produced by speakers (N = 28) of six languages (Akan, Cantonese, English, Korean, Mandarin and Spanish). Across these languages, as predicted, the sides of the tongue throughout running speech were positioned higher in the mouth than the center, and the range of movement of the sides was significantly smaller than that of the center of the tongue. These findings support the view that the sides of the tongue maintain a braced posture across languages while speaking, potentially constituting a universal, rather than language-specific, postural basis for speech.},
  langid = {english},
  file = {/home/jpalo/Zotero/storage/GTT7V552/Liu et al. - 2023 - Lateral tongue bracing as a universal postural basis for speech.pdf}
}

@inproceedings{LloydEtAl-ArtiSynthToolkitRigidDeformable-2011,
  title = {The {{ArtiSynth Toolkit For Rigid-Deformable Biomechanics}}},
  booktitle = {{{ISB Technical Group}} on {{Computer Simulation Symposium}}, {{Poster}}},
  author = {Lloyd, John and Stavness, Ian and Fels, Sidney},
  year = 2011,
  month = jun
}

@article{LloydEtAl-ImpactRealtimeArticulatory-2019,
  title = {The Impact of Real-Time Articulatory Information on Phonetic Transcription: {{Ultrasound-aided}} Transcription in Cleft Lip and Palate},
  author = {Lloyd, S. and Cleland, J. and Campbell, L. and Crampin, L. and Palo, P. and Sugden, E. and Wrench, A. and Zharkova, N.},
  year = 2019,
  journal = {Folia Phoniatrica et Logopaedica},
  pages = {1--11}
}

@article{LonnqvistEtAl-HowYoungAdults-2017,
  title = {How Young Adults with Autism Spectrum Disorder Watch and Interpret Scenes with Complex Communication},
  author = {L{\"o}nnqvist, L. and Loukusa, S. and Hurtig, T. and M{\"a}kinen, L. and Siipo, A. and V{\"a}yrynen, E. and Palo, P. and Laukka, S. and M{\"a}mmel{\"a}, L. and Mattila, M.-L. and Moilanen, I. and Ebeling, H.},
  year = 2017,
  journal = {The Quarterly Journal of Experimental Psychology},
  volume = {70},
  number = {11},
  pages = {2331--2346}
}

@article{LuEtAl-FiniteElementSimulation-1993,
  title = {Finite Element Simulation of Sound Transmission in Vocal Tract},
  author = {Lu, C. and Nakai, T. and Suzuki, H.},
  year = 1993,
  journal = {J. Acoust. Soc. Jpn. (E)},
  volume = {92},
  pages = {2577--2585}
}

@inproceedings{LukkariEtAl-PuheenAanittaminenMagneettiresonanssikuvauksen-2008,
  title = {{Puheen \"a\"anitt\"aminen magneettiresonanssikuvauksen aikana}},
  booktitle = {{Fonetiikan p\"aiv\"at 2008}},
  author = {Lukkari, T. and Malinen, J. and Palo, P.},
  year = 2008,
  pages = {57--64},
  address = {Tampere, Finland},
  langid = {finnish}
}

@inproceedings{LukkariEtAl-RecordingSpeechMagnetic-2007,
  title = {Recording Speech during Magnetic Resonance Imaging},
  booktitle = {{{MAVEBA}} 2007},
  author = {Lukkari, T. and Malinen, J. and Palo, P.},
  year = 2007,
  pages = {163--166},
  address = {Florence, Italy}
}

@article{LukkarilaEtAl-InfluenceIntentionalVoice-2012,
  title = {Influence of the Intentional Voice Quality on the Impression of Female Speaker},
  author = {Lukkarila, P. and Laukkanen, A.-M. and Palo, P.},
  year = 2012,
  journal = {Logopedics Phoniatrics Vocology},
  pages = {158--166}
}

@article{LulichEtAl-AcquiringVisualizing3D-2018,
  title = {Acquiring and Visualizing {{3D}}/{{4D}} Ultrasound Recordings of Tongue Motion},
  author = {Lulich, Steven M. and Berkson, Kelly H. and {de Jong}, Kenneth},
  year = 2018,
  month = nov,
  journal = {Journal of Phonetics},
  volume = {71},
  pages = {410--424},
  issn = {0095-4470},
  doi = {10.1016/j.wocn.2018.10.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0095447017301481},
  urldate = {2024-10-28},
  abstract = {Ultrasound is increasingly common in speech and phonetics research as technology continues to improve. The first digital 3D/4D ultrasound system was utilized for speech research nearly a decade ago, but data access, processing, and visualization were limited to (non-speech) clinical imaging applications. Access to the raw (pulse-echo or scan-converted) image data is a critical step toward making 3D/4D ultrasound an effective tool for speech research. In addition, there is a need for technical characterization of 3D/4D ultrasound systems together with a presentation of their strengths and limitations for research. This paper gives a general technical description of ultrasound systems, beginning with conventional 2D image acquisition and working up toward 3D/4D systems. Emphasis is given to one particular system -- the Philips EPIQ 7G system with xMatrix X6-1 digital 3D/4D transducer -- for which access to raw scan-converted data is now possible. For parameter settings typical of general abdominal imaging, frame rates around 20\,fps are easily achieved (with frame rates possible up to 173\,fps depending on image volume size, scan line density, and imaging depth), with spatial resolution comparable to research-grade 2D transducers. Using a modified Philips foot pedal, ultrasound recordings can be made with synchronous audio, and time-aligned with a minimum uncertainty equal to the ultrasound frame duration. In speech research, frame rates around 20\,fps -- with corresponding time-alignment uncertainty near 50\,ms -- provide good 3D coverage and enable many new phonetics questions to be posed and addressed.},
  keywords = {3D/4D ultrasound,Articulatory phonetics,Biomedical imaging,Phonetics,Speech production,Ultrasound,Volumetric imaging},
  file = {/home/jpalo/Zotero/storage/HQ7R8T3P/Lulich et al. - 2018 - Acquiring and visualizing 3D4D ultrasound recordings of tongue motion.pdf;/home/jpalo/Zotero/storage/PKNC3TE5/S0095447017301481.html}
}

@book{MacfarlaneEtAl-Holloway-2014,
  title = {Holloway},
  author = {Macfarlane, Robert and Donwood, Stanley and Richards, Dan},
  year = 2014
}

@incollection{Maeda-CompensatoryArticulationSpeech-1990,
  title = {Compensatory Articulation during Speech: {{Evidencefrom}} the Analysis and Synthesis of Vocal-Tract Shapes Using an Articulatory Model},
  booktitle = {Speech {{Production}} and {{Speech Modelling}}},
  author = {Maeda, S.},
  editor = {Hardcastle, W. J. and Marchal, A.},
  year = 1990,
  pages = {131--149},
  publisher = {Boston: Kluwer Academic Publishers}
}

@article{Maeda-DigitalSimulationMethod-1982,
  title = {A Digital Simulation Method of the Vocal-Tract System},
  author = {Maeda, S.},
  year = 1982,
  journal = {Speech Communication},
  volume = {1},
  pages = {199--229}
}

@misc{MakerGiovanni-Building200EUProduct-2025,
  title = {Building a 200\texteuro{} Product with 30\texteuro! {{ESP32}} Automatic Watering ({{NO WATER TAP}} Needed!)},
  author = {{Maker Giovanni}},
  year = 2025,
  month = jun,
  url = {https://www.youtube.com/watch?v=uRrnA6kK3FU},
  urldate = {2025-08-17},
  abstract = {Discover Easy, Affordable, and Reliable PCB manufacturing with JLCPCB! Register to get \$70 New customer coupons: https://jlcpcb.com/?from=GAO Special Deal: Get a \$30 coupon for JLCPCB premium 6-layer PCBs: https://jlcpcb.com/6-layer-pcb?from=GAO}
}

@article{Makhoul-SpectralLinearPrediction-1975,
  title = {Spectral Linear Prediction: {{Properties}} and Applications},
  author = {Makhoul, J.},
  year = 1975,
  journal = {IEEE Transactions on Acoustics, Speech and Signal Processing},
  volume = {23},
  number = {3},
  pages = {283--296}
}

@misc{Malinen-ConservativityTimeflowInvertible-2004,
  title = {Conservativity of Time-Flow Invertible and Boundary Control Systems},
  author = {Malinen, J.},
  year = 2004,
  annotation = {Published: Helsinki University of Technology, Institute of Mathematics Research Reports, A479}
}

@article{MalinenEtAl-WhenLinearSystem-2006,
  title = {When Is a Linear System Conservative?},
  author = {Malinen, J. and Staffans, O. J. and Weiss, G.},
  year = 2006,
  journal = {Quart. Appl. Math.},
  volume = {64},
  pages = {31--91}
}

@inproceedings{MalinenPalo-RecordingSpeechMRI-2009,
  title = {Recording Speech during {{MRI}}: {{Part II}}},
  booktitle = {{{MAVEBA}} 2009},
  author = {Malinen, J. and Palo, P.},
  year = 2009,
  pages = {211--214},
  address = {Florence, Italy}
}

@article{MalinenStaffans-ConservativeBoundaryControl-2006,
  title = {Conservative Boundary Control Systems},
  author = {Malinen, J. and Staffans, O. J.},
  year = 2006,
  journal = {J. Diff. Eq.},
  volume = {231},
  number = {1},
  pages = {290--312}
}

@article{MalinenStaffans-ImpedancePassiveConservative-2007,
  title = {Impedance {{Passive}} and {{Conservative Boundary Control Systems}}},
  author = {Malinen, J. and Staffans, O. J.},
  year = 2007,
  journal = {Compl. Anal. Oper. Theory},
  volume = {2},
  number = {1}
}

@incollection{Manuel-CrosslanguageStudiesRelating-1999,
  title = {Cross-Language Studies: Relating Language-Particular Coarticulation Patterns to Other Language-Particular Facts},
  booktitle = {Coarticulation: {{Theory}}, {{Data}} and {{Techniques}}},
  author = {Manuel, S.},
  editor = {Hardcastle, W. J. and Hewlett, N.},
  year = 1999,
  pages = {179--198},
  publisher = {Cambridge University Press}
}

@misc{Marshall-RichardFeynmansNotebook-2017,
  title = {Richard {{Feynman}}'s "{{Notebook Technique}}" {{Will Help You Learn Any Subject--at School}}, at {{Work}}, or in {{Life}} \textbar{} {{Open Culture}}},
  author = {Marshall, Colin},
  year = 2017,
  url = {https://www.openculture.com/2017/04/richard-feynmans-notebook-technique-will-help-you-learn-any-subject.html},
  urldate = {2024-11-01},
  abstract = {Richard Feynman knew his stuff. Had he not, he probably wouldn't have won the Nobel Prize in Physics, let alone his various other prestigious scientific awards. But his reputation for learning all his life long with a special depth and rigor survives him, and in a sense accounts for his fame --- of a degree that ensures his stern yet playful face will gaze out from dorm-room posters for generations to come --- even more than does his ``real'' work. Many students of physics still, understandably, want to be like Feynman, but everyone else, even those of us with no interest in physics whatsoever, could also do well to learn from him: not from what he thought about, but from how he thought about it.},
  langid = {american},
  file = {/home/jpalo/Zotero/storage/C5MEXNMP/richard-feynmans-notebook-technique-will-help-you-learn-any-subject.html}
}

@book{MATLAB-VersionR2013b-2013,
  title = {Version 8.2 ({{R2013b}})},
  author = {{MATLAB}},
  year = 2013,
  publisher = {The MathWorks Inc.},
  address = {Natick, Massachusetts}
}

@inproceedings{McAuliffeEtAl-MontrealForcedAligner-2017,
  title = {Montreal {{Forced Aligner}}: {{Trainable Text-Speech Alignment Using Kaldi}}},
  shorttitle = {Montreal {{Forced Aligner}}},
  booktitle = {Proc. {{Interspeech}} 2017},
  author = {McAuliffe, Michael and Socolof, Michaela and Mihuc, Sarah and Wagner, Michael and Sonderegger, Morgan},
  year = 2017,
  pages = {498--502},
  doi = {10.21437/Interspeech.2017-1386},
  url = {https://www.isca-archive.org/interspeech_2017/mcauliffe17_interspeech.html},
  urldate = {2025-05-17},
  file = {/home/jpalo/Zotero/storage/CNZSLLHZ/McAuliffe et al. - 2017 - Montreal Forced Aligner Trainable Text-Speech Alignment Using Kaldi.pdf}
}

@inproceedings{McAuliffeEtAl-MontrealForcedAligner-2017a,
  title = {Montreal {{Forced Aligner}}: {{Trainable Text-Speech Alignment Using Kaldi}}},
  shorttitle = {Montreal {{Forced Aligner}}},
  booktitle = {Proc. {{Interspeech}} 2017},
  author = {McAuliffe, Michael and Socolof, Michaela and Mihuc, Sarah and Wagner, Michael and Sonderegger, Morgan},
  year = 2017,
  pages = {498--502},
  doi = {10.21437/Interspeech.2017-1386},
  url = {https://www.isca-archive.org/interspeech_2017/mcauliffe17_interspeech.html},
  urldate = {2025-05-17}
}

@book{McIntosh-PoacherPilgrimageIsland-2016,
  title = {Poacher's {{Pilgrimage}}: An {{Island Journey}}},
  shorttitle = {Poacher's {{Pilgrimage}}},
  author = {McIntosh, Alastair},
  year = 2016,
  publisher = {Birlinn Books},
  address = {Edinburgh}
}

@article{McMillanCorley-CascadingInfluencesProduction-2010,
  title = {Cascading Influences on the Production of Speech: {{Evidence}} from Articulation},
  author = {McMillan, C. T. and Corley, M.},
  year = 2010,
  journal = {Cognition},
  volume = {117},
  number = {3},
  pages = {243--260}
}

@article{MenardEtAl-MeasuringTongueShapes-2011,
  title = {Measuring {{Tongue Shapes}} and {{Positions}} with {{Ultrasound Imaging}}: {{A Validation Experiment Using}} an {{Articulatory Model}}},
  shorttitle = {Measuring {{Tongue Shapes}} and {{Positions}} with {{Ultrasound Imaging}}},
  author = {M{\'e}nard, Lucie and Aubin, J{\'e}r{\^o}me and Thibeault, M{\'e}lanie and Richard, Gabrielle},
  year = 2011,
  journal = {Folia Phoniatrica et Logopaedica},
  volume = {64},
  number = {2},
  pages = {64--72},
  issn = {1021-7762},
  doi = {10.1159/000331997},
  url = {https://doi.org/10.1159/000331997},
  urldate = {2025-02-07},
  abstract = {Objective: The goal of this paper is to assess the validity of various metrics developed to characterize tongue shapes and positions collected through ultrasound imaging in experimental setups where the probe is not constrained relative to the subject's head. Patients and Methods: Midsagittal contours were generated using an articulatory-acoustic model of the vocal tract. Sections of the tongue were extracted to simulate ultrasound imaging. Various transformations were applied to the tongue contours in order to simulate ultrasound probe displacements: vertical displacement, horizontal displacement, and rotation. The proposed data analysis method reshapes tongue contours into triangles and then extracts measures of angles, x and y coordinates of the highest point of the tongue, curvature degree, and curvature position. Results: Parameters related to the absolute tongue position (tongue height and front/back position) are more sensitive to horizontal and vertical displacements of the probe, whereas parameters related to tongue curvature are less sensitive to such displacements. Conclusion: Because of their robustness to probe displacements, parameters related to tongue shape (especially curvature) are particularly well suited to cases where the transducer is not constrained relative to the head (studies with clinical populations or children).},
  file = {/home/jpalo/Zotero/storage/52U2QYIH/Measuring-Tongue-Shapes-and-Positions-with.html}
}

@article{MenardEtAl-MeasuringTongueShapes-2012,
  title = {Measuring Tongue Shapes and Positions with Ultrasound Imaging: A Validation Experiment Using an Articulatory Model},
  shorttitle = {Measuring Tongue Shapes and Positions with Ultrasound Imaging},
  author = {M{\'e}nard, Lucie and Aubin, J{\'e}r{\^o}me and Thibeault, M{\'e}lanie and Richard, Gabrielle},
  year = 2012,
  journal = {Folia phoniatrica et logopaedica: official organ of the International Association of Logopedics and Phoniatrics (IALP)},
  volume = {64},
  number = {2},
  pages = {64--72},
  issn = {1421-9972},
  doi = {10.1159/000331997},
  abstract = {OBJECTIVE: The goal of this paper is to assess the validity of various metrics developed to characterize tongue shapes and positions collected through ultrasound imaging in experimental setups where the probe is not constrained relative to the subject's head. PATIENTS AND METHODS: Midsagittal contours were generated using an articulatory-acoustic model of the vocal tract. Sections of the tongue were extracted to simulate ultrasound imaging. Various transformations were applied to the tongue contours in order to simulate ultrasound probe displacements: vertical displacement, horizontal displacement, and rotation. The proposed data analysis method reshapes tongue contours into triangles and then extracts measures of angles, x and y coordinates of the highest point of the tongue, curvature degree, and curvature position. RESULTS: Parameters related to the absolute tongue position (tongue height and front/back position) are more sensitive to horizontal and vertical displacements of the probe, whereas parameters related to tongue curvature are less sensitive to such displacements. CONCLUSION: Because of their robustness to probe displacements, parameters related to tongue shape (especially curvature) are particularly well suited to cases where the transducer is not constrained relative to the head (studies with clinical populations or children).},
  langid = {english},
  pmid = {22212175},
  keywords = {Adult,Algorithms,Analysis of Variance,Anthropometry,Cephalometry,Humans,Models Anatomic,Models Biological,Movement,Phonetics,Reference Values,Speech Production Measurement,Tongue,Ultrasonography},
  file = {/home/jpalo/Zotero/storage/CRBIVWRI/Ménard et al. - 2012 - Measuring tongue shapes and positions with ultrasound imaging a validation experiment using an arti.pdf}
}

@article{Mermelstein-ArticulatoryModelStudy-1973,
  title = {Articulatory Model for the Study of Speech Production},
  author = {Mermelstein, P.},
  year = 1973,
  journal = {Journal of the Acoustical Society of America},
  volume = {53},
  number = {4},
  pages = {1070--1082}
}

@article{MeyerEtAl-FunctionalOrganisationCorticonuclear-1994,
  title = {Functional Organisation of Corticonuclear Pathways to Motoneurones of Lower Facial Muscles in Man},
  author = {Meyer, B. U. and Werhahn, K. and Rothwell, J. C. and Roericht, S. and Fauth, C.},
  year = 1994,
  journal = {Experimental Brain Research},
  volume = {101},
  pages = {465--472}
}

@article{Mielke-UltrasoundStudyCanadian-2015,
  title = {An Ultrasound Study of {{Canadian French}} Rhotic Vowels with Polar Smoothing Spline Comparisons},
  author = {Mielke, J.},
  year = 2015,
  journal = {Journal of the Acoustical Society of America},
  volume = {137},
  number = {5},
  pages = {2858--2869}
}

@article{MielkeEtAl-PalatronTechniqueAligning-2005,
  title = {Palatron: A Technique for Aligning Ultrasound Images of the Tongue and Palate},
  shorttitle = {Palatron},
  author = {Mielke, Jeff and Baker, Adam and Archangeli, D. and Racy, Sumayya},
  year = 2005,
  journal = {Coyote Papers},
  volume = {14},
  pages = {96--107},
  url = {https://www.semanticscholar.org/paper/Palatron:-a-technique-for-aligning-ultrasound-of-Mielke-Baker/58a9c826bd67861db82df304571f1a68d18f8666},
  urldate = {2024-09-04},
  abstract = {A variety of articulatory imaging methods have been employed in the study of language, all with their own advantages and disadvantages. This paper describes a technique for addressing some of the serious challenges presented by ultrasound imaging, namely that ultrasound images offer no fixed point of reference, and that passive articulators such as the palate and velum are invisible under normal circumstances. Ultrasound is advantageous in many ways over other imaging methods, so addressing its challenges is a worthwhile pursuit.}
}

@misc{Miles-RaftingMostPolluted-2023,
  title = {Rafting the Most Polluted River in {{Australia}}},
  author = {Miles, Beau},
  year = 2023,
  publisher = {youtube},
  url = {https://www.youtube.com/watch?v=kFFSFxjg-TY}
}

@inproceedings{Miller-EstimatingLingualCavity-2013,
  title = {Estimating {{Lingual Cavity Volume}} in {{Click Consonant Production}} from {{Combined Lingual Ultrasound}} and {{Palatographic Data}}},
  booktitle = {Proceedings of {{Ultrafest VI}}},
  author = {Miller, A. L.},
  year = 2013
}

@article{MinifieEtAl-UltrasonicScansDorsal-1971,
  title = {Ultrasonic {{Scans}} of the {{Dorsal Surface}} of the {{Tongue}}},
  author = {Minifie, F. and Kelsey, C. and Zagzebski, J.},
  year = 1971,
  journal = {Journal of the Acoustical Society of America},
  volume = {49},
  number = {6},
  pages = {1857--1860}
}

@misc{minutephysics-PhysicsDissonance-2025,
  title = {The {{Physics Of Dissonance}}},
  author = {{minutephysics}},
  year = 2025,
  month = jul,
  url = {https://www.youtube.com/watch?v=tCsl6ZcY9ag},
  urldate = {2025-08-11},
  abstract = {Thanks to the Acoustical Society of America for sponsoring this video! Start your career in acoustics today with the ASA career toolkit: https://exploresound.org/acoustics-ca...}
}

@article{MitsuyaEtAl-CrosslanguageStudyCompensation-2011,
  title = {A Cross-Language Study of Compensation in Response to Real-Time Formant Perturbation},
  author = {Mitsuya, T. and MacDonald, E. N. and Purcell, D. W. and Munhall, K. G.},
  year = 2011,
  journal = {The Journal of the Acoustical Society of America},
  volume = {130},
  number = {5},
  pages = {2978--2986}
}

@phdthesis{Moisik-EpilarynxSpeech-2013,
  title = {The Epilarynx in Speech},
  author = {Moisik, S. R.},
  year = 2013,
  address = {Victoria, British Columbia, Canada},
  school = {University of Victoria}
}

@inproceedings{Moisik-LaryngealUltrasoundAssessment-2010,
  title = {Laryngeal Ultrasound Assessment of Retracted and Constricted Articulations by Phoneticians},
  booktitle = {Ultrafest {{V}}},
  author = {Moisik, S. R.},
  year = 2010,
  month = mar,
  address = {Ultrafest V, Haskins Lab, New Haven, Connecticut}
}

@mastersthesis{Moisik-ThreeDimensionalModelLarynx-2008,
  title = {A {{Three-Dimensional Model}} of the {{Larynx}} and the {{Laryngeal Constrictor Mechanism}}: {{Visually Synthesizing Pharyngeal}} and {{Epiglottal Articulations Observed}} in {{Laryngoscopy}}},
  author = {Moisik, S.},
  year = 2008,
  address = {Victoria},
  school = {University of Victoria}
}

@inproceedings{MoisikEtAl-EVALUATINGLARYNGEALULTRASOUND-2011,
  title = {{{EVALUATING LARYNGEAL ULTRASOUND TO STUDY LARYNX STATE AND HEIGHT}}},
  booktitle = {{{ICPhS}} 2011},
  author = {Moisik, Scott and Esling, John and Bird, Sonya and Lin, Hua},
  year = 2011,
  address = {Hong Kong},
  abstract = {In this work, we evaluate laryngeal ultrasound as a technique to study the state and height of the larynx. We first discuss general structural registration in laryngeal ultrasound (LUS) and illustrate the articulation of glottal stop. We then present a method for quantifying change in larynx height using optical flow analysis on laryngeal ultrasound video data. The analysis is quantitatively validated for accuracy by using independent control data. Then we qualitatively validate the method by performing simultaneous laryngoscopy and laryngeal ultrasound (SLLUS) on canonical productions of Mandarin tones. We conclude that laryngeal ultrasound is best suited for quantification of larynx height but can also provide limited information about larynx state.},
  file = {/home/jpalo/Zotero/storage/F3HNY6I6/Moisik et al. - 2011 - EVALUATING LARYNGEAL ULTRASOUND TO STUDY LARYNX ST.pdf}
}

@article{MoisikEtAl-StudyLaryngealGestures-2014,
  title = {A Study of Laryngeal Gestures in {{Mandarin}} Citation Tones Using Simultaneous Laryngoscopy and Laryngeal Ultrasound ({{SLLUS}})},
  author = {Moisik, S. R. and Lin, H. and Esling, J. H.},
  year = 2014,
  journal = {Journal of the International Phonetic Association},
  volume = {44},
  number = {01},
  pages = {21--58}
}

@article{MoisikGick-QuantalLarynxStable-2017,
  title = {The {{Quantal Larynx}}: {{The Stable Regions}} of {{Laryngeal Biomechanics}} and {{Implications}} for {{Speech Production}}},
  shorttitle = {The {{Quantal Larynx}}},
  author = {Moisik, Scott Reid and Gick, Bryan},
  year = 2017,
  journal = {Journal of Speech, Language, and Hearing Research},
  volume = {60},
  number = {3},
  pages = {540--560},
  publisher = {American Speech-Language-Hearing Association},
  doi = {10.1044/2016_JSLHR-S-16-0019},
  url = {https://pubs.asha.org/doi/abs/10.1044/2016_JSLHR-S-16-0019},
  urldate = {2025-05-28},
  abstract = {Purpose Recent proposals suggest that (a) the high dimensionality of speech motor control may be reduced via modular neuromuscular organization that takes advantage of intrinsic biomechanical regions of stability and (b) computational modeling provides a means to study whether and how such modularization works. In this study, the focus is on the larynx, a structure that is fundamental to speech production because of its role in phonation and numerous articulatory functions. Method A 3-dimensional model of the larynx was created using the ArtiSynth platform (http://www.artisynth.org). This model was used to simulate laryngeal articulatory states, including inspiration, glottal fricative, modal prephonation, plain glottal stop, vocal--ventricular stop, and aryepiglotto--epiglottal stop and fricative. Results Speech-relevant laryngeal biomechanics is rich with ``quantal'' or highly stable regions within muscle activation space. Conclusions Quantal laryngeal biomechanics complement a modular view of speech control and have implications for the articulatory--biomechanical grounding of numerous phonetic and phonological phenomena.},
  file = {/home/jpalo/Zotero/storage/5UZC7HRE/Moisik and Gick - 2017 - The Quantal Larynx The Stable Regions of Laryngeal Biomechanics and Implications for Speech Product.pdf}
}

@incollection{Monsell-GenerationModulationAction-1986,
  title = {Generation and {{Modulation}} of {{Action Patterns}}},
  author = {Monsell, S.},
  year = 1986,
  publisher = {Springer-Verlag},
  chapter = {Programming of complex sequences: Evidence from the timing of rapid speech and other productions}
}

@article{MooshammerEtAl-BridgingPlanningExecution-2012,
  title = {Bridging Planning and Execution: {{Temporal}} Planning of Syllables},
  author = {Mooshammer, C. and Goldstein, L. and Nam, H. and McClure, S. and Saltzman, E. and Tiede, M.},
  year = 2012,
  journal = {Journal of Phonetics},
  volume = {40},
  pages = {374--389}
}

@article{MrayatiEtAl-DistinctiveRegionsModes-1988,
  title = {Distinctive Regions and Modes: A New Theory of Speech Production},
  author = {Mrayati, M. and Carr{\'e}, R. and Guerin, B.},
  year = 1988,
  journal = {Speech Communication},
  number = {7},
  pages = {257--286}
}

@article{MullenEtAl-WaveguidePhysicalModeling-2006,
  title = {Waveguide {{Physical Modeling}} of {{Vocal Tract Acoustics}}: {{Flexible Formant Bandwith Control}} from {{Increased Model Dimensionality}}},
  author = {Mullen, J. and Howard, D. and Murphy, D.},
  year = 2006,
  journal = {IEEE Transactions on Audio, Speech and Language Processing},
  volume = {14},
  number = {3},
  pages = {964--971}
}

@book{Muller-InformationRetrievalMusic-2007,
  title = {Information {{Retrieval}} for {{Music}} and {{Motion}}},
  author = {M{\"u}ller, Meinard},
  year = 2007,
  publisher = {Springer},
  address = {Berlin Heidelberg New York}
}

@article{MurtolaEtAl-AnalysisPhonationOnsets-2019,
  title = {Analysis of Phonation Onsets in Vowel Production, Using Information from Glottal Area and Flow Estimate},
  author = {Murtola, T. and Malinen, J. and Geneid, A. and Alku, P.},
  year = 2019,
  journal = {Speech Communication},
  volume = {109},
  pages = {55--65}
}

@incollection{NairEtAl-AnalysisbysynthesisLearningInvert-2008,
  title = {Analysis-by-Synthesis by Learning to Invert Generative Black Boxes},
  booktitle = {Artificial {{Neural Networks-ICANN}} 2008},
  author = {Nair, Vinod and Susskind, Josh and Hinton, Geoffrey E},
  year = 2008,
  pages = {971--981},
  publisher = {Springer}
}

@misc{NawakArts&Crafts-DIYMadeMy-2024,
  title = {{{DIY}} - {{I}} Made My Own {{Midori}} Style {{TRAVELER}}'{{S NOTEBOOK}}},
  author = {{Nawak Arts \& Crafts}},
  year = 2024,
  month = jun,
  url = {https://www.youtube.com/watch?v=1WRFGv7jnA0},
  urldate = {2025-08-14},
  abstract = {Recently, I found out about the Midori Traveler's Notebook an I was instantly in love with the simplicity of this design and many customization possibilities, so I made my own (which is apparently called Fauxdori)!  I urge you to make one yourself too - it's really easy and fun! :D}
}

@incollection{Nearey-VowelInherentSpectral-2013,
  title = {Vowel {{Inherent Spectral Change}} in the {{Vowels}} of {{North American English}}},
  booktitle = {Vowel {{Inherent Spectral Change}}},
  author = {Nearey, Terrance M.},
  editor = {Morrison, Geoffrey Stewart and Assmann, Peter F.},
  year = 2013,
  pages = {49--85},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-14209-3_4},
  url = {https://doi.org/10.1007/978-3-642-14209-3_4},
  urldate = {2025-11-12},
  abstract = {Nearey and Assmann (1986) coined the term `vowel inherent spectral change' (VISC) to refer to change in spectral properties inherent to the phonetic specification of vowels. Although such change includes the relatively large formant changes associated with acknowledged diphthongs, the term was explicitly intended to include reliable (but possibly more subtle) spectral change associated with vowel categories of North American English typically regarded as monophthongs. This chapter reviews statistical and graphical evidence of dynamic formant patterns in vowels of several CV and CVC syllable types in three regional dialects of English: Dallas, Texas (Assmann and Katz, 2000), Western Michigan (Hillenbrand et al., 1995) and Northern Alberta (Thomson 2007). Evidence is reviewed for the importance of VISC in vowel perception. While certain apparent VISC patterns show up across dialects, both dialect differences and differences in context make it clear that more sophisticated methods will be required to fully separate several factors affecting formant change in vowels. Promising preliminary results are presented using a new non-linear regression method that extends compositional models of Broad and Clermont (1987, 2002, 2010) to include dual vowel targets.},
  isbn = {978-3-642-14209-3},
  langid = {english},
  keywords = {Formant Movement,Formant Trajectory,Front Vowel,Locus Equation,Vowel Duration},
  file = {/home/jpalo/Zotero/storage/LN26HERB/Nearey - 2013 - Vowel Inherent Spectral Change in the Vowels of North American English.pdf}
}

@article{Nieto-CastanonEtAl-ModelingInvestigationArticulatory-2005,
  title = {A Modeling Investigation of Articulatory Variability and Acoustic Stability during {{American English}} /r/ Production},
  author = {{Nieto-Castanon}, A. and Guenther, F. H. and Perkell, J. S. and Curtin, H. D.},
  year = 2005,
  journal = {The Journal of the Acoustical Society of America},
  volume = {117},
  number = {5},
  pages = {3196--3212}
}

@inproceedings{NiikawaEtAl-FrequencyDependenceVocaltract-2002,
  title = {Frequency Dependence of Vocal-Tract Length},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Spoken Language Processing}}},
  author = {Niikawa, T. and Ando, T. and Matsumura, M.},
  year = 2002,
  pages = {1525--1528}
}

@inproceedings{NishimotoEtAl-EstimationTransferFunction-2004,
  title = {Estimation of Transfer Function of Vocal Tract Extracted from {{MRI}} Data by {{FEM}}},
  booktitle = {The 18th {{International Congress}} on {{Acoustics}}},
  author = {Nishimoto, H. and Akagi, M. and Kitamura, T. and Suzuki, N.},
  year = 2004,
  volume = {II},
  pages = {1473--1476},
  address = {Kyoto, Japan}
}

@article{NiziolekGuenther-VowelCategoryBoundaries-2013,
  title = {Vowel {{Category Boundaries Enhance Cortical}} and {{Behavioral Responses}} to {{Speech Feedback Alterations}}},
  author = {Niziolek, C. A. and Guenther, F. H.},
  year = 2013,
  journal = {Journal of Neuroscience},
  volume = {33},
  number = {29},
  pages = {12090--12098}
}

@book{OConnor-Phonetics-1973,
  title = {Phonetics},
  author = {O'Connor, J. D.},
  year = 1973,
  publisher = {Penguin},
  address = {Harmondsworth}
}

@inproceedings{ODellNieminen-SuomenKonsonanttiyhtymassaOlevan-2016,
  title = {Suomen Konsonanttiyhtym\"ass\"a Olevan Lateraalin Akustinen Variaatio Ja Murteiden Epenteettinen Vokaali},
  booktitle = {Fonetiikan P\"aiv\"at 2016},
  author = {O'Dell, M. and Nieminen, T.},
  year = 2016,
  address = {Oulu}
}

@article{Öhman-CoarticulationVCVUtterances-1966,
  title = {Coarticulation in {{VCV Utterances}}: {{Spectrographic Measurements}}},
  shorttitle = {Coarticulation in {{VCV Utterances}}},
  author = {{\"O}hman, S. E. G.},
  year = 1966,
  month = jan,
  journal = {The Journal of the Acoustical Society of America},
  volume = {39},
  number = {1},
  pages = {151--168},
  issn = {0001-4966},
  doi = {10.1121/1.1909864},
  url = {https://doi.org/10.1121/1.1909864},
  urldate = {2024-10-24},
  abstract = {In this paper, the formant transitions in vowel + stop consonant + vowel utterances spoken by Swedish, American, and Russian talkers are studied spectrographically. The data suggest a physiological model in terms of which the VCV articulations are represented by a basic diphthongal gesture with an independent stop-consonant gesture superimposed on its transitional portion. This interpretation necessitates reevaluation of the locus theory proposed by the workers of the Haskins Laboratories. Some conclusions about the general nature of the neural instructions behind the VCV gestures are discussed.},
  file = {/home/jpalo/Zotero/storage/LPRPQM2N/Coarticulation-in-VCV-Utterances-Spectrographic.html}
}

@inproceedings{OjalaPalo-SignAnnotationTool-2012,
  title = {A Sign Annotation Tool for Phoneticians},
  booktitle = {{{XXVII}} Fonetiikan P\"aiv\"at 2012},
  author = {Ojala, S. and Palo, P.},
  year = 2012,
  pages = {41--44},
  address = {Tallinn, Estonia}
}

@article{Oller-EffectPositionUtterance-1973,
  title = {The Effect of Position in Utterance on Speech Segment Duration in {{English}}},
  author = {Oller, D. K.},
  year = 1973,
  journal = {The Journal of the Acoustical Society of America},
  volume = {54},
  number = {5},
  pages = {1235--1247}
}

@inproceedings{OrlandoPalo-IdentifyingArticulatoryGestures-2023,
  title = {Towards Identifying Articulatory Gestures with Pixel Difference and Audio Segmentation},
  booktitle = {International Conference of Phonetic Sciences ({{ICPhS}} 2023)},
  author = {Orlando, G. and Palo, P.},
  year = 2023,
  address = {Prague}
}

@inproceedings{Palo-CanWeDetect-2020,
  title = {Can We Detect Initiation of Tongue Internal Changes before Overt Movement Onset in Ultrasound?},
  booktitle = {Proceedings of the 12th International Seminar on Speech Production ({{ISSP}} 2020)},
  author = {Palo, P.},
  year = 2020,
  pages = {242--245},
  address = {Online / New Haven, CT}
}

@misc{Palo-CASTComputerAssisted-2024,
  title = {{{CAST}}: {{Computer Assisted Segmentation Tools}} [{{Python}} Software Package]},
  author = {Palo, P.},
  year = 2024,
  howpublished = {Available in a public software repository, accessed 31 October 2024}
}

@article{Palo-ComputerAssistedSegmentation-2021,
  title = {Computer Assisted Segmentation of Tongue Ultrasound and Lip Videos},
  author = {Palo, P.},
  year = 2021,
  journal = {Journal of the Canadian Acoustical Association},
  volume = {49},
  number = {3},
  pages = {44--45}
}

@unpublished{Palo-HolisticChangeMeasures-2021,
  title = {Holistic Change Measures and Automated Reaction Times Based on Tongue Ultrasound},
  author = {Palo, P.},
  year = 2021
}

@unpublished{Palo-LongWalksThoughts-2021,
  title = {Long {{Walks}} -- {{Thoughts}} on Wandering and on Trails},
  shorttitle = {Long {{Walks}}},
  author = {Palo, Pertti},
  year = 2021
}

@phdthesis{Palo-MeasuringPrespeechArticulation-2019,
  title = {Measuring Pre-Speech Articulation},
  author = {Palo, P.},
  year = 2019,
  address = {Edinburgh},
  school = {Queen Margaret University}
}

@misc{Palo-RandomiseAAAStimulus-2024,
  title = {Randomise {{AAA}} Stimulus List [{{R}} and {{Matlab}} Software Package]},
  author = {Palo, P.},
  year = 2024,
  howpublished = {Available in a public software repository, accessed 31 October 2024}
}

@mastersthesis{Palo-ReviewArticulatorySpeech-2006,
  title = {A Review of Articulatory Speech Synthesis},
  author = {Palo, P.},
  year = 2006,
  address = {Helsinki},
  url = {http://taurlin.org/?page_id=8},
  school = {TKK}
}

@phdthesis{Palo-WaveEquationModel-2011,
  type = {Licentiate Thesis},
  title = {A Wave Equation Model for Vowels: {{Measurements}} for Validation},
  author = {Palo, P.},
  year = 2011,
  address = {Helsinki},
  school = {Institute of Mathematics, Aalto University}
}

@misc{PaloCowen-SimultaneousRecordingTongue-2017,
  type = {Poster},
  title = {Simultaneous Recording of Tongue Ultrasound and Oral Airflow},
  author = {Palo, P. and Cowen, S.},
  year = 2017
}

@inproceedings{PaloEtAl-AcousticArticulatorySpeech-2015,
  title = {Acoustic and Articulatory Speech Reaction Times with Tongue Ultrasound: {{What}} Moves First?},
  booktitle = {Ultrafest 2015},
  author = {Palo, P. and Schaeffler, S. and Scobbie, J. M.},
  year = 2015,
  address = {Hong Kong}
}

@inproceedings{PaloEtAl-AnalysingSpeechData-2023,
  title = {Analysing Speech Data with {{SATKIT}}},
  booktitle = {International Conference of Phonetic Sciences ({{ICPhS}} 2023)},
  author = {Palo, P. and Moisik, S. R. and Faytak, M.},
  year = 2023,
  address = {Prague}
}

@inproceedings{PaloEtAl-ArticulatingFinnishVowels-2012,
  title = {Articulating Finnish Vowels: {{Results}} from {{MRI}} and Sound Data},
  booktitle = {{{XXVII}} Fonetiikan P\"aiv\"at 2012},
  author = {Palo, P. and Aalto, D. and Aaltonen, O. and Happonen, R.-P. and Malinen, J. and Vainio, M.},
  year = 2012,
  pages = {45--47},
  address = {Tallinn, Estonia}
}

@article{PaloEtAl-ArticulatingFinnishVowels-2012a,
  title = {Articulating Finnish Vowels: {{Results}} from {{MRI}} and Sound Data},
  author = {Palo, P. and Aalto, D. and Aaltonen, O. and Happonen, R.-P. and Malinen, J. and Vainio, M.},
  year = 2012,
  journal = {Linguistica Uralica},
  volume = {48},
  number = {3},
  pages = {194--199}
}

@unpublished{PaloEtAl-ArticulatoryAcousticTiming-2021,
  title = {Articulatory and Acoustic Timing of Finnish Utterances},
  author = {Palo, P. and Dahlgren, S. and Moisik, S. R. and {\v S}imko, J.},
  year = 2021
}

@unpublished{PaloEtAl-DeterminationArticulatoryReaction-2015,
  type = {Presentation at {{XXIX}} Fonetiikan P\"aiv\"at 2015},
  title = {Determination of Articulatory Reaction Time by Automatic Segmentation of Tongue Ultrasound},
  author = {Palo, Pertti and Schaeffler, Sonja and Scobbie, James M.},
  year = 2015,
  address = {Helsinki}
}

@unpublished{PaloEtAl-DoesEMAProvide-2017,
  title = {Does {{EMA}} Provide Unbiased Articulatory Reaction Times? {{Evidence}} from Comparison with Ultrasound. (Provisional Title)},
  author = {Palo, P. and Lehtinen, M. and Schaeffler, S. and Scobbie, J. M. and {\v S}imko, J.},
  year = 2017
}

@inproceedings{PaloEtAl-EffectPhoneticOnset-2015,
  title = {Effect of Phonetic Onset on Acoustic and Articulatory Speech Reaction Times Studied with Tongue Ultrasound},
  booktitle = {International Conference of Phonetic Sciences ({{ICPhS}} 2015)},
  author = {Palo, P. and Schaeffler, S. and Scobbie, J. M.},
  year = 2015,
  address = {Glasgow, Scotland, UK}
}

@article{PaloEtAl-MagnitudeUltrasoundProbe-2025,
  title = {Magnitude of Ultrasound Probe Misalignment Measures in Controlled Conditions - a Case Study},
  author = {Palo, P. and Lulich, S. M. and Aalto, D.},
  year = 2025,
  journal = {Besz\'edtudom\'any - Speech Science},
  volume = {5},
  number = {1},
  pages = {74--89}
}

@article{PaloEtAl-MagnitudeUltrasoundProbe-2025a,
  title = {Magnitude of Ultrasound Probe Misalignment Measures in Controlled Conditions - a Case Study},
  author = {Palo, P. and Lulich, S. M. and Aalto, D.},
  year = 2025,
  volume = {To appear in}
}

@misc{PaloEtAl-PATKITPhoneticAnalysis-2025,
  title = {{{PATKIT}}: {{Phonetic Analysis ToolKIT}} [{{Python}} Software Package]},
  author = {Palo, P. and Moisik, S. R. and Faytak, M.},
  year = 2025,
  howpublished = {Available in a public software repository, accessed 7 December 2025}
}

@inproceedings{PaloEtAl-PrespeechTongueMovements-2014,
  title = {Pre-Speech Tongue Movements Recorded with Ultrasound},
  booktitle = {10th International Seminar on Speech Production ({{ISSP}} 2014)},
  author = {Palo, P. and Schaeffler, S. and Scobbie, J. M.},
  year = 2014,
  pages = {304--307}
}

@unpublished{PaloEtAl-RoleDifferentArticulators-2021,
  title = {Role of Different Articulators and Syllable Structure in Speech Initiation in the Delayed Naming Task (Provisional Title)},
  author = {Palo, P. and Schaeffler, S. and Scobbie, J. M. and {\v S}imko, J.},
  year = 2021
}

@unpublished{PaloEtAl-TimingStartingStopping-2022,
  title = {Timing the Starting and Stopping of Speech -- {{An}} Ultrasound Study},
  author = {Palo, P. and Lulich, S. and Orlando, G.},
  year = 2022
}

@inproceedings{PaloLulich-AssessingStabilitySensitivity-2024,
  title = {Toward Assessing Stability and Sensitivity of Tongue Contour Metrics},
  booktitle = {Proceedings of {{Ultrafest}} 2024},
  author = {Palo, Pertti and Lulich, Steven M.},
  year = 2024,
  address = {Aizu-Wakamatsu, Japan}
}

@unpublished{PaloLulich-CharacterisingEndUtterance-2023,
  title = {Characterising End of Utterance Momevent with Tongue Ultrasound and Pixel Difference},
  author = {Palo, P. and Lulich, S. M.},
  year = 2023
}

@misc{PaloLulich-CharacterizingEndUtterance-2021,
  type = {Poster},
  title = {Characterizing End of Utterance Movements},
  author = {Palo, P. and Lulich, S.},
  year = 2021
}

@inproceedings{PaloLulich-EffectsFrameRate-2024,
  title = {Some {{Effects}} of {{Frame Rate}} on {{Gesture Detection}} in {{Tongue Ultrasound}}},
  booktitle = {Proceedings of {{ISSP}} 2024},
  author = {Palo, Pertti and Lulich, Steven M.},
  year = 2024,
  address = {Autrans, France}
}

@article{PaloLulich-ExtendingPixelDifference-2022,
  title = {Extending the Pixel Difference Metric from {{2D}} to {{3D}}/{{4D}} Ultrasound},
  author = {Palo, Pertti and Lulich, Steven M.},
  year = 2022,
  month = oct,
  journal = {The Journal of the Acoustical Society of America},
  volume = {152},
  number = {4\_Supplement},
  pages = {A58-A58},
  issn = {0001-4966},
  doi = {10.1121/10.0015537},
  url = {https://doi.org/10.1121/10.0015537},
  urldate = {2023-09-13},
  abstract = {It is frequently of interest to examine differences (e.g., the 1st or 2nd difference) in signals and images. Differencing can be employed for the detection of edges in images, or the detection of events in temporal sequences. One such event is the onset of articulatory movement in ultrasound image sequences of the tongue. It has been quantified using a form of Euclidean distance between successive frames of image scan line data, referred to as Pixel Difference. When applied to 3-D/4-D ultrasound data of the tongue (after scan conversion), Pixel Difference exhibited a surprisingly poor signal-to-noise ratio. In this presentation, we test the hypotheses that (1) scan conversion amplifies the effect of speckle noise near the top of the ultrasound image (i.e. above the tongue), and (2) the Pixel Difference signal-to-noise ratio in 3-D/4-D ultrasound differs from 2-D ultrasound by a power of 3/2 due to the added spatial dimension.}
}

@article{PaloLulich-ImprovingSignaltonoiseRatio-2023,
  title = {Improving Signal-to-Noise Ratio in Ultrasound Video Pixel Difference},
  author = {Palo, P. and Lulich, S. M.},
  year = 2023,
  journal = {The Journal of the Acoustical Society of America},
  volume = {153},
  number = {3\_supplement},
  pages = {A373},
  issn = {0001-4966},
  doi = {10.1121/10.0019222},
  abstract = {Pixel difference is a holistic change measure for ultrasound videos and other image sequences. It is based on Euclidean distance (l2 norm) which is calculated by interpreting each frame as a vector of pixels. Recently, we found that 3D/4D ultrasound data producesignificantly worse signal-to-noise ratio in Pixel Difference curves, which appears to be primarily due to the lower frame rate of 3D/4D ultrasound when compared with 2D ultrasound. We also found that the signal-to-noise ratio is improved when we limit the analysis to tongue-internal pixels. In this study, we seek to quantify the timing behavior of tongue gestures obtained from whole-image and tongue-internalPD. If the timing behavior is similar in both cases, we can increase the signal to noise ratio of 3D/4D ultrasound at the same time as we increase its frame rate, because imaging to a shallower depth requires less time per frame. We also examine the signal-to-noise behavior of PD curves calculated with other norms (e.g., l1, and l3 norms).}
}

@article{PaloLulich-LocalStabilityLong-2023,
  title = {Local Stability and Long Distance Reliability of Tongue Ultrasound Spline Metrics},
  author = {Palo, Pertti and Lulich, Steven M.},
  year = 2023,
  journal = {The Journal of the Acoustical Society of America},
  volume = {153},
  number = {3\_supplement},
  pages = {A373-A373},
  issn = {0001-4966},
  doi = {10.1121/10.0019223},
  url = {https://doi.org/10.1121/10.0019223},
  urldate = {2023-09-13},
  abstract = {Tongue ultrasound data arecommonly analyzed by using different metrics. In this study, we evaluate the local stability and long distance reliability of Average Nearest Neighbour Distance (Zharkova and Hewlett, 2009), Median Point-by-Point Distance (Palo, 2020), Procrustes analysis and Modified Curvature Index (Dawson et al., 2015). A metric which has good local stability should map a time-series of splines to a smooth scalar (or vector) time-series and not be very sensitive to small errors in the splining. Similarly, a metric with good long distance reliability should differentiate dissimilar tongue contours---such as different phoneme targets---reliably and without small splining errors significantly affecting the results. We test the metrics with simulated data generated from actual spline data. First, we select keyframes at phoneme target positions. Second, we vary the sampling frequency artificially by interpolating the splines between the keyframes. Third, we vary the level of splining noise by adding small perturbations to the splines at varying magnitudes and frequencies. Based on the simulation results, we will calculate the distributions of each of the metrics as conditioned by sampling frequency and noise level. The code and simulation data will be publicly available.}
}

@unpublished{PaloLulich-MethodologicalObservationsUse-2023,
  title = {Methodological Observations on the Use of Pixel Difference to Analyse Tongue Ultrasound},
  author = {Palo, P. and Lulich, S. M.},
  year = 2023
}

@article{PaloLulich-UltrasoundStudyEffect-2021,
  title = {An {{Ultrasound Study}} of the {{Effect}} of {{Rest Position}} on Timing of {{Pre-acoustic Speech Movements}}},
  author = {Palo, Pertti and Lulich, Steven M.},
  year = 2021,
  journal = {Proceedings of Meetings on Acoustics},
  volume = {45},
  number = {1},
  pages = {060011},
  issn = {1939-800X},
  doi = {10.1121/2.0001564},
  url = {https://doi.org/10.1121/2.0001564},
  abstract = {The position of the tongue during rest and its relationship to subsequent pre-acoustic speech movements have shed light on speech motor planning. They may find applications in technologies such as silent speech interfaces. Palo (2019) found that the duration of pre-acoustic speech movements are strongly correlated with acoustic utterance duration, but it remains an open question whether this correlation is modulated by different rest positions. This study investigates variability in rest position among children and adults, and examines whether rest position affects pre-acoustic speech movements and their correlation with acoustic utterance durations.}
}

@article{PaloLulich-UltrasoundStudyRest-2021,
  title = {An Ultrasound Study of Rest Position and Pre-Acoustic Articulation in Adults and Children},
  author = {Palo, Pertti and Lulich, Steven M.},
  year = 2021,
  month = oct,
  journal = {The Journal of the Acoustical Society of America},
  volume = {150},
  number = {4\_Supplement},
  pages = {A188-A188},
  issn = {0001-4966},
  doi = {10.1121/10.0008074},
  url = {https://doi.org/10.1121/10.0008074},
  urldate = {2023-09-13},
  abstract = {The position of the tongue during rest and its relationship to subsequent pre-acoustic speech movements have shed light on speech motor planning. They may find applications in technologies such as silent speech interfaces. Palo [``Measuring pre-speech articulation,'' Ph.D. thesis (Queen Margaret University, Edinburgh, 2019)] found that the duration of pre-acoustic speech movements are strongly correlated with acoustic utterance duration, but it remains an open question whether this correlation is modulated by different rest positions. This study investigates variability in rest position among children and adults, and examines whether rest position affects pre-acoustic speech movements and their correlation with acoustic utterance durations. [This work has been supported by a personal grant from The Emil Aaltonen Foundation (Palo) and by NSF Grant No. 1551131 (Lulich).]}
}

@unpublished{PaloOrlando-CharacterisingArticulatoryGestures-2023,
  title = {Characterising Articulatory Gestures with Tongue Ultrasound and Pixel Difference},
  author = {Palo, P. and Orlando, G.},
  year = 2023
}

@inproceedings{PeltolaEtAl-ShortTermStudy-2013,
  title = {A Short Term Study of Hungarians Learning Finnish Vowels},
  booktitle = {Fonetiikan P\"aiv\"at 2013},
  author = {Peltola, T. and Palo, P. and Aaltonen, O.},
  year = 2013,
  pages = {66--71},
  address = {Turku, Finland}
}

@phdthesis{Penttinen-SkillDevelopmentMusic-2013,
  title = {Skill Development in Music Reading: The Eye-Movement Approach},
  author = {Penttinen, M.},
  year = 2013,
  school = {University of Turku}
}

@article{PenttinenEtAl-ReadingAheadAdult-2015,
  title = {Reading Ahead: {{Adult}} Music Students' Eye Movements in Temporally Controlled Performances of a Children's Song},
  author = {Penttinen, M. and Huovinen, E. and Ylitalo, A.},
  year = 2015,
  journal = {International Journal of Music Education: Research},
  volume = {33},
  number = {1},
  pages = {36--50}
}

@article{PerkellOka-UseAlternatingMagnetic-1980,
  title = {Use of an Alternating Magnetic Field Device to Track Midsagittal Plane Movements of Multiple Points inside the Vocal Tract},
  author = {Perkell, J.S. and Oka, D.},
  year = 1980,
  journal = {Journal of the Acoustical Society of America},
  volume = {67},
  pages = {92}
}

@book{Pilgrim-PeacePilgrimHer-1981,
  title = {Peace {{Pilgrim}}: Her Life and Work in Her Own Words},
  author = {Pilgrim, Peace},
  year = 1981,
  publisher = {Ocean Tree Books},
  address = {Santa Fe, New Mexico}
}

@article{PossamaïEtAl-PartialAdvanceInformation-2002,
  title = {Partial Advance Information, Number of Alternatives, and Motor Processes: An Electromyographic Study},
  author = {Possama{\"i}, C.A. and Burle, B. and Osma, A. and Hasbroucq, T.},
  year = 2002,
  journal = {Acta Psychologica},
  volume = {111},
  number = {1},
  pages = {125--139}
}

@book{Poukka-KorkeammanMatematiikanPeruskurssi-1966,
  title = {Korkeamman Matematiikan Peruskurssi},
  author = {Poukka, K. A.},
  year = 1966,
  publisher = {Werner S\"oderstr\"om Osakeyhti\"o}
}

@article{PrestonEtAl-UltrasoundImagesTongue-2017,
  title = {Ultrasound {{Images}} of the {{Tongue}}: {{A Tutorial}} for {{Assessment}} and {{Remediation}} of {{Speech Sound Errors}}},
  shorttitle = {Ultrasound {{Images}} of the {{Tongue}}},
  author = {Preston, Jonathan L. and McAllister Byun, Tara and Boyce, Suzanne E. and Hamilton, Sarah and Tiede, Mark and Phillips, Emily and {Rivera-Campos}, Ahmed and Whalen, Douglas H.},
  year = 2017,
  journal = {Journal of Visualized Experiments : JoVE},
  number = {119},
  pages = {55123},
  issn = {1940-087X},
  doi = {10.3791/55123},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5351819/},
  urldate = {2024-04-09},
  abstract = {Diagnostic ultrasound imaging has been a common tool in medical practice for several decades. It provides a safe and effective method for imaging structures internal to the body. There has been a recent increase in the use of ultrasound technology to visualize the shape and movements of the tongue during speech, both in typical speakers and in clinical populations. Ultrasound imaging of speech has greatly expanded our understanding of how sounds articulated with the tongue (lingual sounds) are produced. Such information can be particularly valuable for speech-language pathologists. Among other advantages, ultrasound images can be used during speech therapy to provide (1) illustrative models of typical (i.e.~"correct") tongue configurations for speech sounds, and (2) a source of insight into the articulatory nature of deviant productions. The images can also be used as an additional source of feedback for clinical populations learning to distinguish their better productions from their incorrect productions, en route to establishing more effective articulatory habits., Ultrasound feedback is increasingly used by scientists and clinicians as both the expertise of the users increases and as the expense of the equipment declines. In this tutorial, procedures are presented for collecting ultrasound images of the tongue in a clinical context. We illustrate these procedures in an extended example featuring one common error sound, American English /r/. Images of correct and distorted /r/ are used to demonstrate (1) how to interpret ultrasound images, (2) how to assess tongue shape during production of speech sounds, (3), how to categorize tongue shape errors, and (4), how to provide visual feedback to elicit a more appropriate and functional tongue shape. We present a sample protocol for using real-time ultrasound images of the tongue for visual feedback to remediate speech sound errors. Additionally, example data are shown to illustrate outcomes with the procedure.},
  pmcid = {PMC5351819},
  pmid = {28117824},
  file = {/home/jpalo/Zotero/storage/VB3H4ETZ/Preston et al. - 2017 - Ultrasound Images of the Tongue A Tutorial for As.pdf}
}

@article{PrestonEtAl-VariablePracticeEnhance-2017,
  title = {Variable {{Practice}} to {{Enhance Speech Learning}} in {{Ultrasound Biofeedback Treatment}} for {{Childhood Apraxia}} of {{Speech}}: {{A Single Case Experimental Study}}},
  shorttitle = {Variable {{Practice}} to {{Enhance Speech Learning}} in {{Ultrasound Biofeedback Treatment}} for {{Childhood Apraxia}} of {{Speech}}},
  author = {Preston, Jonathan L. and Leece, Megan C. and McNamara, Kerry and Maas, Edwin},
  year = 2017,
  journal = {American Journal of Speech-Language Pathology},
  volume = {26},
  number = {3},
  pages = {840--852},
  issn = {1058-0360},
  doi = {10.1044/2017_AJSLP-16-0155},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5829796/},
  urldate = {2024-04-09},
  abstract = {Purpose The purpose of this study was to evaluate the role of practice variability, through prosodic variation during speech sound training, in biofeedback treatment for children with childhood apraxia of speech. It was hypothesized that variable practice would facilitate speech sound learning. Method Six children ages 8--16 years with persisting speech sound errors due to childhood apraxia of speech participated in a single-subject experimental design. For each participant, 2 speech sound targets were treated with ultrasound visual feedback training: one with prosodic variation (i.e., practicing sound targets in words and phrases spoken fast, slow, loud, as a question, command, and declarative), and one without prosodic variation. Each target was treated for half of the 1-hr session for 14 treatment sessions. Results As measured by standardized effect sizes, all participants showed greater change on generalization probes for sound targets treated under the prosodic variation condition with mean effect sizes (d 2) of 14.5 for targets treated with prosodic variation and 8.3 for targets treated without prosodic variation. The average increase in generalization scores was 38\% in the prosodic variation condition compared to 31\% without. Conclusions Ultrasound visual feedback may facilitate speech sound learning and learning may be enhanced by treating speech sounds with explicit prosodic variation. Supplemental Materials https://doi.org/10.23641/asha.5150119},
  pmcid = {PMC5829796},
  pmid = {28715554},
  file = {/home/jpalo/Zotero/storage/PW6XLCTB/Preston et al. - 2017 - Variable Practice to Enhance Speech Learning in Ul.pdf}
}

@book{Price-LazinessDoesNot-2021,
  title = {Laziness {{Does Not Exist}} - {{A Defense}} of the {{Exhausted}}, {{Exploited}}, and {{Overworked}}},
  shorttitle = {Laziness {{Does Not Exist}}},
  author = {Price, Devon},
  year = 2021,
  publisher = {Atria Books}
}

@article{PurcellMunhall-CompensationFollowingRealtime-2006,
  title = {Compensation Following Real-Time Manipulation of Formants in Isolated Vowels},
  author = {Purcell, D. W. and Munhall, K. G.},
  year = 2006,
  journal = {The Journal of the Acoustical Society of America},
  volume = {119},
  number = {4},
  pages = {2288--2297}
}

@book{PythonSoftwareFoundation-PythonLanguageReference-2017,
  title = {Python {{Language Reference}}, Version 2.7},
  author = {{Python Software Foundation}},
  year = 2017,
  publisher = {Python Software Foundation},
  url = {http://www.python.org}
}

@inproceedings{RaeesyEtAl-ParametrisingDegreeArticulator-2011,
  title = {Parametrising {{Degree}} of {{Articulator Movement}} from {{Dynamic MRI Data}}},
  booktitle = {12th {{Interspeech}}},
  author = {Raeesy, Z. and {Baghai-Ravary}, L. and Coleman, J.},
  year = 2011,
  pages = {2853--2856}
}

@article{RamanarayananEtAl-AreArticulatorySettings-2014,
  title = {Are {{Articulatory Settings Mechanically Advantageous}} for {{Speech Motor Control}}?},
  author = {Ramanarayanan, V. and Lammert, A. and Goldstein, L. and Narayanan, S.},
  year = 2014,
  journal = {PLOS ONE},
  volume = {9},
  number = {8},
  pages = {1--8}
}

@article{RamanarayananEtAl-RealtimeMRIInvestigation-2013,
  title = {A Real-Time {{MRI}} Investigation of Articulatory Setting across Different Speaking Styles},
  author = {Ramanarayanan, V. and Goldstein, L. and Byrd, D. and Narayanan, S. S.},
  year = 2013,
  journal = {Journal of the Acoustical Society of America},
  volume = {134},
  number = {1},
  pages = {510--519}
}

@mastersthesis{Ramirez-AcousticsVowelArticulation-2019,
  title = {Acoustics of {{Vowel Articulation}} in {{Flute Playing}}},
  author = {Ramirez, J. F.},
  year = 2019,
  school = {West Virginia University}
}

@article{RastleDavis-ComplexitiesMeasuringNaming-2002,
  title = {On the {{Complexities}} of {{Measuring Naming}}},
  author = {Rastle, K. and Davis, M. H.},
  year = 2002,
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {28},
  number = {2},
  pages = {307--314}
}

@article{RastleEtAl-CharacterizingMotorExecution-2005,
  title = {Characterizing the {{Motor Execution Stage}} of {{Speech Production}}: {{Consonantal Effects}} on {{Delayed Naming Latency}} and {{Onset Duration}}},
  author = {Rastle, K. and Harrington, J. M. and Croot, K. P. and Coltheart, M.},
  year = 2005,
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {31},
  number = {5},
  pages = {1083--1095}
}

@article{RastleEtAl-ReadingAloudBegins-2000,
  title = {Reading {{Aloud Begins When}} the {{Computation}} of {{Phonology Is Complete}}},
  author = {Rastle, K. and Harrington, J. and Coltheart, M. and Palethorpe, S.},
  year = 2000,
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {26},
  number = {3},
  pages = {1178--1191}
}

@article{Raudies-OpticFlow-2013,
  title = {Optic Flow},
  author = {Raudies, Florian},
  year = 2013,
  journal = {Scholarpedia},
  volume = {8},
  number = {7},
  pages = {30724},
  doi = {doi:10.4249/scholarpedia.30724}
}

@inproceedings{RaymondEtAl-AnalysisTranscriptionConsistency-2002,
  title = {An Analysis of Transcription Consistency in Spontaneous Speech from the Buckeye Corpus},
  booktitle = {Proc. {{ICSLP}} 2002},
  author = {Raymond, William D. and Pitt, Mark and Johnson, Keith and Hume, Elizabeth and Makashay, Matthew and Dautricourt, Robin and Hilts, Craig},
  year = 2002,
  pages = {1125--1128},
  doi = {10.21437/ICSLP.2002-371},
  url = {https://www.isca-archive.org/icslp_2002/raymond02_icslp.html},
  urldate = {2025-01-21},
  abstract = {We present a preliminary analysis of transcriber consistency in labeling and segmentation of words and phones in the Buckeye corpus of spontaneous, informal speech. We find that pairwise intertranscriber agreement on exact phone label match was 76\%, and segmentation agreement within 20\% of phone pair length was 75\%, though longer phones are more consistently segmented than shorter phones. Patterns of consistency variation in labeling are observed as a function of phonetic categories that are similar to patterns reported for read speech. More agreement is seen on consonants than on vowels, and on fricatives and labials than on other consonant classes. In general, we find that shorter, more reduced words and phones result in more transcriber disagreement.},
  file = {/home/jpalo/Zotero/storage/P9D3X9PG/Raymond et al. - 2002 - An analysis of transcription consistency in spontaneous speech from the buckeye corpus.pdf}
}

@book{RCoreTeam-LanguageEnvironmentStatistical-2013,
  title = {R: {{A Language}} and {{Environment}} for {{Statistical Computing}}},
  author = {{R Core Team}},
  year = 2013,
  publisher = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  url = {http://www.R-project.org/}
}

@inproceedings{RichmondRenals-UltraxAnimatedMidsagittal-2012,
  title = {Ultrax: {{An Animated Midsagittal Vocal Tract Display}} for {{Speech Therapy}}},
  booktitle = {Proceedings of {{Interspeech}}},
  author = {Richmond, Korin and Renals, Steve},
  year = 2012,
  month = sep,
  address = {Portland, Oregon, USA}
}

@article{RièsEtAl-CorrigendumWhyDoes-2014,
  title = {Corrigendum to ``{{Why}} Does Picture Naming Take Longer than Word Reading? {{The}} Contribution of Articulatory Processes''},
  author = {Ri{\`e}s, S. and Legou, T. and Burle, B. and Alario, F. X. and Malfait, N.},
  year = 2014,
  journal = {Psychonomic Bulletin \& Review},
  pages = {1--3}
}

@article{RièsEtAl-WhyDoesPicture-2012,
  title = {Why Does Picture Naming Take Longer than Word Reading? {{The}} Contribution of Articulatory Processes},
  author = {Ri{\`e}s, S. and Legou, T. and Burle, B. and Alario, F. X. and Malfait, N.},
  year = 2012,
  journal = {Psychonomic Bulletin \& Review},
  volume = {19},
  number = {5},
  pages = {955--961}
}

@article{RofskyEtAl-AbdominalMRImaging-1999,
  title = {Abdominal {{MR Imaging}} with a {{Volumetric Interpolated Breath-hold Examination}}},
  author = {Rofsky, N.M. and Lee, V.S. and Laub, G. and Pollack, M.A. and Krinsky, G.A. and Thomasson, D. and Ambrosino, M.M. and Weinreb, J.C.},
  year = 1999,
  journal = {Radiology},
  volume = {212},
  number = {3},
  pages = {876--884}
}

@article{RolandEtAl-VowelProductionPotential-2023,
  title = {Vowel Production: A Potential Speech Biomarker for Early Detection of Dysarthria in {{Parkinson}}'s Disease},
  shorttitle = {Vowel Production},
  author = {Roland, Virginie and Huet, Kathy and Harmegnies, Bernard and Piccaluga, Myriam and Verhaegen, Cl{\'e}mence and Delvaux, V{\'e}ronique},
  year = 2023,
  journal = {Frontiers in Psychology},
  volume = {14},
  pages = {1129830},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2023.1129830},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10493417/},
  urldate = {2024-07-16},
  abstract = {Objectives Our aim is to detect early, subclinical speech biomarkers of dysarthria in Parkinson's disease (PD), i.e., systematic atypicalities in speech that remain subtle, are not easily detectible by the clinician, so that the patient is labeled ``non-dysarthric.'' Based on promising exploratory work, we examine here whether vowel articulation, as assessed by three acoustic metrics, can be used as early indicator of speech difficulties associated with Parkinson's disease. Study design This is a prospective case--control study. Methods Sixty-three individuals with PD and 35 without PD (healthy controls-HC) participated in this study. Out of 63 PD patients, 43 had been diagnosed with dysarthria (DPD) and 20 had not (NDPD). Sustained vowels were recorded for each speaker and formant frequencies were measured. The analyses focus on three acoustic metrics: individual vowel triangle areas (tVSA), vowel articulation index (VAI) and the Phi index. Results tVSA were found to be significantly smaller for DPD speakers than for HC. The VAI showed significant differences between these two groups, indicating greater centralization and lower vowel contrasts in the DPD speakers with dysarhtria. In addition, DPD and NDPD speakers had lower Phi values, indicating a lower organization of their vowel system compared to the HC. Results also showed that the VAI index was the most efficient to distinguish between DPD and NDPD whereas the Phi index was the best acoustic metric to discriminate NDPD and HC. Conclusion This acoustic study identified potential subclinical vowel-related speech biomarkers of dysarthria in speakers with Parkinson's disease who have not been diagnosed with dysarthria.},
  pmcid = {PMC10493417},
  pmid = {37701868},
  file = {/home/jpalo/Zotero/storage/PYGPK5XU/Roland et al. - 2023 - Vowel production a potential speech biomarker for.pdf}
}

@phdthesis{Roon-DynamicsPhonologicalPlanning-2013,
  title = {The Dynamics of Phonological Planning},
  author = {Roon, K. D.},
  year = 2013,
  school = {New York University}
}

@inproceedings{RoonEtAl-DistractorEffectsResponse-2014,
  title = {Distractor Effects on Response Times in Fricative Production},
  booktitle = {10th {{International Seminar}} on {{Speech Production}} ({{ISSP}} 2014)},
  author = {Roon, K. D. and Klein, E. and Gafos, A. I.},
  year = 2014,
  pages = {360--363}
}

@misc{RosenfelderEtAl-FAVEForcedAlignment-2011,
  title = {{{FAVE}} ({{Forced Alignment}} and {{Vowel Extraction}}) {{Program Suite}}},
  author = {Rosenfelder, I. and Fruehwald, J. and Evanini, K. and Yuan, J.},
  year = 2011
}

@misc{RosenfelderEtAl-FAVEForcedAlignment-2022,
  title = {{{FAVE}} ({{Forced Alignment}} and {{Vowel Extraction}})},
  author = {Rosenfelder, Ingrid and Fruehwald, Josef and Brickhouse, Christian and Evanini, Keelan and Seyfarth, Scott and Gorman, Kyle and Prichard, Hilary and Yuan, Jiahong},
  year = 2022,
  url = {http://dx.doi.org/10.5281/zenodo.22281}
}

@book{RossingEtAl-ScienceSound-2002,
  title = {The {{Science}} of {{Sound}}},
  author = {Rossing, T. D. and Moore, F. R. and Wheeler, P. A.},
  year = 2002,
  edition = {3rd},
  publisher = {Addison Wesley},
  address = {San Francisco, California}
}

@article{RossionPourtois-RevisitingSnodgrassVanderwart-2004,
  title = {Revisiting {{Snodgrass}} and {{Vanderwart}}'s Object Pictorial Set: {{The}} Role of Surface Detail in Basic-Level Object Recognition},
  author = {Rossion, B. and Pourtois, G.},
  year = 2004,
  journal = {Perception},
  volume = {33},
  pages = {217--236}
}

@inproceedings{Russel-PalatalizationEpenthesisPlains-1992,
  title = {Palatalization and Epenthesis in {{Plains Cree}}},
  booktitle = {Proceedings of the {{Canadian Linguistics Association}}},
  author = {Russel, Kevin},
  year = 1992,
  file = {/home/jpalo/Zotero/storage/9AZ9JMNE/Russel - 1992 - Palatalization and epenthesis in Plains Cree.pdf}
}

@book{Saad-NumericalMethodsLarge-1992,
  title = {Numerical {{Methods}} for {{Large Eigenvalue Problems}}},
  author = {Saad, Y.},
  year = 1992,
  publisher = {Manchester University Press},
  address = {Manchester},
  mrnumber = {MR1177405 (93h:65052)}
}

@inproceedings{SaitoEtAl-UltrasoundStudyFrequency-2020,
  title = {An Ultrasound Study of Frequency and Co-Articulation},
  booktitle = {Proceedings of the 12th {{International Seminar}} on {{Speech Production}} ({{ISSP}} 2020)},
  author = {Saito, M. and Tomaschek, F. and Sun, C.-C. and Baayen, R. H.},
  year = 2020,
  pages = {206--209},
  address = {Online / New Haven, CT}
}

@article{SaltzmanMunhall-DynamicalApproachGestural-1989,
  title = {A {{Dynamical Approach}} to {{Gestural Patterning}} in {{Speech Production}}},
  author = {Saltzman, E. L. and Munhall, K. G.},
  year = 1989,
  journal = {Ecological Psychology},
  volume = {1},
  number = {4},
  pages = {333--382}
}

@misc{Sanderson-WhatFollowYour-2024,
  title = {What "{{Follow Your Dreams}}" {{Misses}} \textbar{} {{Harvey Mudd Commencement Speech}} 2024},
  author = {Sanderson, Grant},
  year = 2024,
  url = {https://www.youtube.com/watch?v=W3I3kAg2J7w}
}

@inproceedings{SasakiEtAl-FEMAnalysisBased-2003,
  title = {{{FEM}} Analysis Based on 3-{{D}} Time-Varying Vocal Tract Shape},
  booktitle = {{{EUROSPEECH-2003}}},
  author = {Sasaki, K. and Miki, N. and Miyanaga, Y.},
  year = 2003,
  pages = {2357--2360}
}

@article{SavariauxEtAl-ComparativeStudyPrecision-2017,
  title = {A {{Comparative Study}} of the {{Precision}} of {{Carstens}} and {{Northern Digital Instruments Electromagnetic Articulographs}}},
  author = {Savariaux, C. and Badin, P. andSamson, A. and Gerbera, S.},
  year = 2017,
  journal = {The Journal of the Acoustical Society of America},
  volume = {60},
  number = {2},
  pages = {322--340}
}

@inproceedings{SchaefflerEtAl-COMPLEXPATTERNSSILENT-2015,
  title = {{{COMPLEX PATTERNS IN SILENT SPEECH PREPARATION}}: {{PREPARING FOR FAST RESPONSE MIGHT BE DIFFERENT TO PREPARING FOR FAST SPEECH IN A REACTION TIME EXPERIMENT}}},
  booktitle = {Proceedings of {{ICPhS}} 2015},
  author = {Schaeffler, S. and Scobbie, J.M. and Schaeffler, F.},
  year = 2015,
  address = {Glasgow, UK}
}

@inproceedings{SchaefflerEtAl-EvaluationInterSpeechPostures-2008,
  title = {An {{Evaluation}} of {{Inter-Speech Postures}} for the {{Study}} of {{Language-Specific Articulatory Settings}}},
  booktitle = {8th {{International Seminar}} on {{Speech Production}} ({{ISSP}} 2008)},
  author = {Schaeffler, S. and Scobbie, J. M. and Mennen, I.},
  year = 2008,
  pages = {121--124}
}

@inproceedings{SchaefflerEtAl-MeasuringReactionTimes-2014,
  title = {Measuring {{Reaction Times}}: {{Vocalisation}} vs. {{Articulation}}},
  booktitle = {Proceedings of 10th {{ISSP}}},
  author = {Schaeffler, S. and Scobbie, J.M. and Schaeffler, F.},
  year = 2014,
  pages = {383--386}
}

@article{SchönleEtAl-ElectromagneticArticulographyUse-1987,
  title = {Electromagnetic Articulography: {{Use}} of Alternating Magnetic Fields for Tracking Movements of Multiple Points inside and Outside the Vocal Tract},
  author = {Sch{\"o}nle, P.W. and Gr{\"a}be, K. and Wenig, P. and H{\"o}hne, J. and Schrader, J. and Conrad, B.},
  year = 1987,
  journal = {Brain and Language},
  volume = {31},
  pages = {26--35}
}

@article{SchroederFoxe-TimingLaminarProfile-2002,
  title = {The Timing and Laminar Profile of Converging Inputs to Multisensory Areas of the Macaque Neocortex},
  author = {Schroeder, C. E. and Foxe, J. J.},
  year = 2002,
  journal = {Cognitive Brain Research},
  volume = {14},
  pages = {187--198}
}

@inproceedings{ScobbieEtAl-AudibleAspectsSpeech-2011,
  title = {Audible Aspects of Speech Preparation},
  booktitle = {Proceedings of 17th {{ICPhS}}},
  author = {Scobbie, J. M. and Schaeffler, S. and Mennen, I.},
  year = 2011,
  pages = {1782--1785},
  address = {Hong Kong}
}

@techreport{ScobbieEtAl-CommonCoordinateSystem-2011,
  title = {A Common Co-Ordinate System for Mid-Sagittal Articulatory Measurement},
  author = {Scobbie, J. M. and Lawson, E. and Cowen, S. and Cleland, J. and Wrench, A. A.},
  year = 2011,
  institution = {Queen Margaret University},
  file = {/home/jpalo/Zotero/storage/YN7Y7NPD/Scobbie et al. - 2011 - A common co-ordinate system for mid-sagittal articulatory measurement.pdf}
}

@incollection{ScobbieEtAl-ScottishEnglishSpeech-2007,
  title = {Scottish {{English Speech Acquisition}}},
  booktitle = {The {{International Guide}} to {{Speech Acquisition}}},
  author = {Scobbie, James M. and Gordeeva, Olga B. and Matthews, Benjamin},
  editor = {McLeod, Sharynne},
  year = 2007,
  pages = {221--240},
  publisher = {Thomson Delmar Learning},
  address = {Clifton Park, NY}
}

@article{SekiyamaTohkura-McGurkEffectNonEnglish-1991,
  title = {{{McGurk}} Effect in non-{{English}} Listeners: {{Few}} Visual Effects for {{Japanese}} Subjects Hearing {{Japanese}} Syllables of High Auditory Intelligibility},
  shorttitle = {{{McGurk}} Effect in non-{{English}} Listeners},
  author = {Sekiyama, Kaoru and Tohkura, Yoh'ichi},
  year = 1991,
  month = oct,
  journal = {The Journal of the Acoustical Society of America},
  volume = {90},
  number = {4},
  pages = {1797--1805},
  issn = {0001-4966},
  doi = {10.1121/1.401660},
  url = {https://doi.org/10.1121/1.401660},
  urldate = {2025-01-23},
  abstract = {The McGurk effect is a phenomenon that demonstrates a perceptual fusion between auditory and visual (lip-read) information in speech perception under the condition of audio-visual discrepancy, created by dubbed video tapes. This paper investigated whether or not the McGurk effect could be extended to Japanese subjects listening to Japanese syllables of different auditory intelligibility. The audio and video signal of a female talker's speech for ten Japanese syllables (/ba/, /pa/, /ma/, /wa/, /da/, /ta/, /na/, /ra/, /ga/, /ka/) were combined on videotapes, giving 100 audio-visual stimuli. These stimuli were presented to ten Japanese subjects who were required to identify the stimuli as heard speech in both noise-added and noise-free conditions. For both conditions, the intelligibility of the auditory stimuli was measured, by presenting the audio-alone stimuli. The results showed that, in the noise-free condition, the McGurk effect was small and almost limited to auditory stimuli of which the intelligibility was less than 100\%. In the noise-added condition, the McGurk effect was very strong and widespread. These results indicate that the ``Japanese McGurk effect'' is less easily induced than the English one, and that it depends on the auditory intelligibility of the speech signal.},
  file = {/home/jpalo/Zotero/storage/CB7HTU23/Sekiyama and Tohkura - 1991 - McGurk effect in non‐English listeners Few visual effects for Japanese subjects hearing Japanese sy.pdf;/home/jpalo/Zotero/storage/VVUUUL4W/McGurk-effect-in-non-English-listeners-Few-visual.html}
}

@article{Shannon-CommunicationPresenceNoise-1949,
  title = {Communication in the {{Presence}} of {{Noise}}},
  author = {Shannon, C.E.},
  year = 1949,
  journal = {Proceedings of the Institute of Radio Engineers},
  volume = {37},
  number = {1},
  pages = {10--21},
  issn = {2162-6634},
  doi = {10.1109/JRPROC.1949.232969},
  url = {https://ieeexplore.ieee.org/document/1697831},
  urldate = {2024-04-09},
  abstract = {A method is developed for representing any communication system geometrically. Messages and the corresponding signals are points in two "function spaces," and the modulation process is a mapping of one space into the other. Using this representation, a number of results in communication theory are deduced concerning expansion and compression of bandwidth and the threshold effect. Formulas are found for the maxmum rate of transmission of binary digits over a system when the signal is perturbed by various types of noise. Some of the properties of "ideal" systems which transmit at this maxmum rate are discussed. The equivalent number of binary digits per second for certain information sources is calculated.},
  keywords = {Bandwidth,Circuits,Communication systems,Electron tubes,Frequency measurement,Gain measurement,Klystrons,Shape,Telephony,Voltage},
  annotation = {Reprinted in Proc. IEEE, Vol. 86, No. 2, (Feb 1998)},
  file = {/home/jpalo/Zotero/storage/KGZEG8NM/1697831.html}
}

@article{Shannon-CommunicationPresenceNoise-1949a,
  title = {Communication in the {{Presence}} of {{Noise}}},
  author = {Shannon, C.E.},
  year = 1949,
  journal = {Proceedings of the IRE},
  volume = {37},
  number = {1},
  pages = {10--21},
  issn = {2162-6634},
  doi = {10.1109/JRPROC.1949.232969},
  url = {https://ieeexplore.ieee.org/document/1697831},
  urldate = {2023-12-21},
  abstract = {A method is developed for representing any communication system geometrically. Messages and the corresponding signals are points in two "function spaces," and the modulation process is a mapping of one space into the other. Using this representation, a number of results in communication theory are deduced concerning expansion and compression of bandwidth and the threshold effect. Formulas are found for the maxmum rate of transmission of binary digits over a system when the signal is perturbed by various types of noise. Some of the properties of "ideal" systems which transmit at this maxmum rate are discussed. The equivalent number of binary digits per second for certain information sources is calculated.},
  file = {/home/jpalo/Zotero/storage/NEA79DCN/1697831.html}
}

@article{ShawkerSonies-UltrasoundBiofeedbackSpeech-1985,
  title = {Ultrasound Biofeedback for Speech Training. {{Instrumentation}} and Preliminary Results},
  author = {Shawker, T. H. and Sonies, B. C.},
  year = 1985,
  journal = {Investigative Radiology},
  volume = {20},
  number = {1},
  pages = {90--93},
  issn = {0020-9996},
  doi = {10.1097/00004424-198501000-00022},
  abstract = {Real-time ultrasound scanning was employed as biofeedback therapy to correct a persistent articulatory speech defect in a subject. The subject continuously imaged her tongue with a transducer placed submentally. During a speech exercise, the subject could compare her tongue's positioning and movement with an ultrasound image showing the correct tongue placement prerecorded onto video tape by a speech therapist. Preliminary results suggest that this technique could be a valuable addition to speech therapy.},
  langid = {english},
  pmid = {3980183},
  keywords = {Articulation Disorders,Biofeedback Psychology,Child,Female,Humans,Speech Therapy,Ultrasonics,Videotape Recording}
}

@misc{SheffieldMadePlants-WhyEVERYBasil-2025,
  title = {Why {{EVERY Basil Plant You Buy Dies In}} 1 {{Week}}},
  author = {{Sheffield Made Plants}},
  year = 2025,
  month = jul,
  url = {https://www.youtube.com/watch?v=nK7uPpYBkpA},
  urldate = {2025-08-13},
  abstract = {▸ Download my FREE Plant Parent's Troubleshooting Handbook 👉 https://resources.sheffieldmadeplants...}
}

@book{Shepherd-LivingMountain-2011,
  title = {The {{Living Mountain}}},
  author = {Shepherd, Nan},
  year = 2011
}

@article{ShettyEtAl-DevelopmentVowelAcoustics-2022,
  title = {Development of Vowel Acoustics and Subglottal Resonances in {{American English-speaking}} Children: {{A}} Longitudinal {{Study}}},
  author = {Shetty, Vishwas and Lulich, Steven M. and Palo, Pertti and Alwan, Abeer},
  year = 2022,
  month = oct,
  journal = {The Journal of the Acoustical Society of America},
  volume = {152},
  number = {4\_Supplement},
  pages = {A286-A286},
  issn = {0001-4966},
  doi = {10.1121/10.0016294},
  url = {https://doi.org/10.1121/10.0016294},
  urldate = {2023-09-13},
  abstract = {Acoustic analysis of typically developing elementary school-aged (prepubertal) children's speech has been primarily performed on cross-sectional data in the past. Few studies have examined longitudinal data in this age group. For this presentation, we analyze the developmental changes in the acoustic properties of children's speech using data collected longitudinally over four years (from first grade to fourth grade). Four male and four female children participated in this study. Data were collected once every year for each child. Using these data, we measured the four-year development of subglottal acoustics (first two subglottal resonances) and vowel acoustics (first four formants and fundamental frequency). Subglottal acoustic measurements are relatively independent of context, and average values were obtained for each child in each year. Vowel acoustics measurements were made for seven vowels (i, ɪ, {$\varepsilon$}, \ae, ʌ, ɑ, u), each occurring in two different words in the stressed syllable. We investigated the correlations between the children's subglottal acoustics, vowel acoustics, and growth-related variables such as standing height, sitting height, and chronological age. Gender-, vowel-, and child-specific analyses were carried out in order to shed light on how typically developing speech acoustics depend on such variables. [Work supported, in part, by the NSF.]}
}

@article{SiegenthalerHochberg-ReactionTimeTongue-1965,
  title = {Reaction Time of the Tongue to Auditory and Tactile Stimulation},
  author = {Siegenthaler, B. M. and Hochberg, I.},
  year = 1965,
  journal = {Perceptual and Motor Skills},
  volume = {21},
  pages = {387--393}
}

@misc{SierraWalden-ThankYourPartner-2025,
  title = {Thank {{Your Partner}}},
  author = {{Sierra Walden}},
  year = 2025,
  month = may,
  url = {https://www.youtube.com/watch?v=wFF8Z745ZKw},
  urldate = {2025-08-05},
  abstract = {Documentary on the topic of Contra Dance Community. I traveled across a dozen different states where I visited Contra Dances and conducted oral interviews. This highlights many of the things that I-- and many other love about Contra Dance.}
}

@phdthesis{Simko-EmbodiedModellingGestural-2009,
  title = {The {{Embodied Modelling}} of {{Gestural Sequencing}} in {{Speech}}},
  author = {Simko, J.},
  year = 2009,
  school = {UCD School of Computer Science and Informatics}
}

@article{SimkoCummins-EmbodiedTaskDynamics-2010,
  title = {Embodied {{Task Dynamics}}},
  author = {Simko, J. and Cummins, F.},
  year = 2010,
  journal = {Psychological Review},
  volume = {117},
  number = {4},
  pages = {1229--1246}
}

@article{SimkoCummins-SequencingOptimizationEmbodied-2011,
  title = {Sequencing and Optimization within an {{Embodied Task Dynamic}} Model},
  author = {Simko, J. and Cummins, F.},
  year = 2011,
  journal = {Cognitive Science},
  volume = {35},
  number = {3},
  pages = {527--562}
}

@article{ŠimkoEtAl-EmergentConsonantalQuantity-2014,
  title = {Emergent Consonantal Quantity Contrast and Context-Dependence of Gestural Phasing},
  author = {{\v S}imko, J. and O'Dell, M. and Vainio, M.},
  year = 2014,
  journal = {Journal of Phonetics},
  volume = {44},
  pages = {130--151}
}

@article{ŠimkoEtAl-HyperarticulationLombardSpeech-2016,
  title = {Hyperarticulation in {{Lombard}} Speech: {{Global}} Coordination of the Jaw, Lips and the Tongue},
  author = {{\v S}imko, J. and Be{\v n}u{\v s}, {\v S}. and Vainio, M.},
  year = 2016,
  journal = {Journal of the Acoustical Society of America},
  volume = {139},
  number = {1},
  pages = {151--162}
}

@inproceedings{SmithEtAl-ExploratoryAnalysisBrainstem-2025,
  title = {Exploratory {{Analysis}} of {{Brainstem fMRI Data During Sustained Phonation}}},
  booktitle = {Proc. {{Interspeech}} 2025},
  author = {Smith, Carey and Cheng, Hu and Palo, Pertti and Aalto, Daniel and Lulich, Steven M.},
  year = 2025,
  pages = {1063--1067},
  doi = {10.21437/Interspeech.2025-2444},
  langid = {english}
}

@article{SnodgrassVanderwart-StandardizedSet260-1980,
  title = {A {{Standardized Set}} of 260 {{Pictures}}: {{Norms}} for {{Name Agreement}}, {{Image Agreement}}, {{Familiarity}}, and {{Visual Complexity}}},
  author = {Snodgrass, J. G. and Vanderwart, M.},
  year = 1980,
  journal = {Journal of Experimental Psychology: Human Learning and Memory},
  volume = {6},
  number = {2},
  pages = {174--215}
}

@article{Sondhi-ResonancesBentVocal-1986,
  title = {Resonances of a Bent Vocal Tract},
  author = {Sondhi, M. M.},
  year = 1986,
  journal = {J. Acoust. Soc. Am.},
  volume = {79},
  number = {4},
  pages = {1113--1116}
}

@article{Sonoda-ObservationTongueMovements-1974,
  title = {Observation of Tongue Movements Employing Magnetometer Sensor},
  author = {Sonoda, Y.},
  year = 1974,
  journal = {IEEE Trans. Magn.},
  volume = {10},
  pages = {954--957}
}

@article{Sovijärvi-RontgenogrammejaSuomenYleiskielen-1938,
  title = {R\"ontgenogrammeja Suomen Yleiskielen Vokaalien \"A\"antym\"aasennoista},
  author = {Sovij{\"a}rvi, A.},
  year = 1938,
  journal = {Viritt\"aj\"a}
}

@book{Sovijärvi-SuomenKielenAannekuvasto-1963,
  title = {Suomen Kielen \"A\"annekuvasto},
  author = {Sovij{\"a}rvi, A.},
  year = 1963,
  publisher = {Gummerus}
}

@inproceedings{SpreaficoEtAl-UltraFitSpeakerfriendlyHeadset-2018,
  title = {{{UltraFit}}: {{A Speaker-friendly Headset}} for {{Ultrasound Recordings}} in {{Speech Science}}},
  shorttitle = {{{UltraFit}}},
  booktitle = {Proc. {{Interspeech}} 2018},
  author = {Spreafico, Lorenzo and Pucher, Michael and Matosova, Anna},
  year = 2018,
  pages = {1517--1520},
  doi = {10.21437/Interspeech.2018-995},
  url = {https://www.isca-archive.org/interspeech_2018/spreafico18_interspeech.html#},
  urldate = {2024-09-04},
  file = {/home/jpalo/Zotero/storage/R4RKEWTH/Spreafico et al. - 2018 - UltraFit A Speaker-friendly Headset for Ultrasound Recordings in Speech Science.pdf}
}

@article{StavnessEtAl-CoupledHardsoftTissue-2011,
  title = {Coupled Hard-Soft Tissue Simulation with Contact and Constraints Applied to Jaw-Tongue-Hyoid Dynamics},
  author = {Stavness, I. and Lloyd, J. E. and Payan, Y. and Fels, S.},
  year = 2011,
  journal = {INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN BIOMEDICAL ENGINEERING},
  volume = {27},
  number = {3},
  pages = {367--390}
}

@article{SteinerEtAl-MagneticResonanceImaging-2012,
  title = {The Magnetic Resonance Imaging Subset of the Mngu0 Articulatory Corpus},
  author = {Steiner, I. and Richmond, K. and Marshall, I. and Gray, C. D.},
  year = 2012,
  month = jan,
  journal = {The Journal of the Acoustical Society of America},
  volume = {131},
  number = {2},
  pages = {EL106 -- EL111},
  publisher = {ASA}
}

@incollection{SternbergEtAl-LatencyDurationRapid-1978,
  title = {The {{Latency}} and {{Duration}} of {{Rapid Movement Sequences}}: {{Comparisons}} of {{Speech}} and {{Typewriting}}},
  booktitle = {Information {{Processing}} in {{Motor Control}} and {{Learning}}},
  author = {Sternberg, S. and Monsell, S. and Knoll, R. L. and Wright, C. E.},
  editor = {Stelmach, G. E.},
  year = 1978,
  publisher = {Academic Press}
}

@article{SternbergEtAl-MotorProgramsHierarchical-1988,
  title = {Motor Programs and Hierarchical Organization in the Control of Rapid Speech},
  author = {Sternberg, S. and Knoll, R. L. and Monsell, S. and Wright, C. E.},
  year = 1988,
  journal = {Phonetica},
  volume = {45},
  pages = {175--197}
}

@book{Stevens-AcousticPhonetics-1998,
  title = {Acoustic {{Phonetics}}},
  author = {Stevens, K. N.},
  year = 1998,
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts}
}

@book{Stevens-QuantalNatureSpeech-1968,
  title = {The {{Quantal Nature}} of {{Speech}}: {{Evidence}} from {{Articulatory-acoustic Data}}},
  shorttitle = {The {{Quantal Nature}} of {{Speech}}},
  author = {Stevens, Kenneth N.},
  year = 1968,
  googlebooks = {6KYeAQAAMAAJ},
  langid = {english}
}

@article{StevensKeyser-QuantalTheoryEnhancement-2010,
  title = {Quantal Theory, Enhancement and Overlap},
  author = {Stevens, Kenneth Noble and Keyser, Samuel Jay},
  year = 2010,
  month = jan,
  journal = {Journal of Phonetics},
  series = {Phonetic {{Bases}} of {{Distinctive Features}}},
  volume = {38},
  number = {1},
  pages = {10--19},
  issn = {0095-4470},
  doi = {10.1016/j.wocn.2008.10.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0095447008000600},
  urldate = {2025-05-31},
  abstract = {This paper explores three aspects of a theory of speech production and perception: quantal theory, enhancement, and overlap. The section on quantal theory makes the claim that every phonological feature or contrast is associated with its own quantal footprint. This footprint for a given feature is a discontinuous (or quantal) relation between the displacement of an articulatory parameter and the acoustical attribute that results from this articulatory movement. The second and third sections address the question of how a listener might extract the underlying distinctive features in running speech. The second section shows that for a given quantally defined feature, the featural specification during speech production may be embellished with other gestures that enhance the quantally defined base. These enhancing gestures, together with the defining gestures, provide a set of acoustic cues that are potentially available to a listener who must use these cues to aid the identification of features, segments, and words. The third section shows that even though rapid speech phenomena can obliterate defining quantal information from the speech stream, nonetheless that information is recoverable from the enhancement history of the segment. We provide examples and discussion in each of these sections of the paper.},
  file = {/home/jpalo/Zotero/storage/JJ4G3BXU/S0095447008000600.html}
}

@article{StockardEtAl-DetectionLocalizationOccult-1977,
  title = {Detection and Localization of Occult Lesions with Brain-Stem Auditory Responses},
  author = {Stockard, J. J. and Stockard, J. E. and Sharbrough, F. W.},
  year = 1977,
  journal = {Mayo Clinic Proceedings},
  volume = {52},
  pages = {761--769}
}

@article{StolarGick-IndexQuantifyingTongue-2013,
  title = {An {{Index}} for {{Quantifying Tongue Curvature}}},
  author = {Stolar, Stade and Gick, Bryan},
  year = 2013,
  journal = {Canadian Acoustics},
  volume = {41},
  number = {1},
  issn = {2291-1391},
  url = {https://jcaa.caa-aca.ca/index.php/jcaa/article/view/2598},
  urldate = {2024-06-17},
  abstract = {This study develops a method of quantifying tongue curvature by modeling the shape of the tongue surface in any anatomical plane using a polynomial approximation. In a validation experiment, the curvature indices of English vowel and consonant sounds were calculated across ten native speakers' productions based on midsagittal ultrasound images of the tongue. Indices confirm substantially higher curvature values for liquids /r/ and /l/ than for all other sounds in the inventory. This method is both more generalized and less dependent upon fixed locations than previous methods, and provides a simple, powerful metric for evaluating shape complexity with applications in areas such as motor development and aeroacoustics.},
  copyright = {Copyright (c)},
  langid = {english},
  file = {/home/jpalo/Zotero/storage/PFZEG258/Stolar and Gick - 2013 - An Index for Quantifying Tongue Curvature.pdf}
}

@article{Stone-GuideAnalyzingTongue-2005,
  title = {A {{Guide}} to {{Analyzing Tongue Motion}} from {{Ultrasound Images}}},
  author = {Stone, M.},
  year = 2005,
  journal = {Clinical Linguistics and Phonetics},
  volume = {19},
  number = {6--7},
  pages = {455--502}
}

@article{StoneBirkholz-AngleCorrectionOptopalatographic-2017,
  title = {Angle {{Correction}} in {{Optopalatographic Tongue Distance Measurements}}},
  author = {Stone, S. and Birkholz, P.},
  year = 2017,
  month = jan,
  journal = {IEEE Sensors Journal},
  volume = {17},
  number = {2},
  pages = {459--468}
}

@article{StoneDavis-HeadTransducerSupport-1995,
  title = {A Head and Transducer Support System for Making Ultrasound Images of Tongue/Jaw Movement},
  author = {Stone, Maureen and Davis, Edward P.},
  year = 1995,
  month = dec,
  journal = {The Journal of the Acoustical Society of America},
  volume = {98},
  number = {6},
  pages = {3107--3112},
  issn = {0001-4966},
  doi = {10.1121/1.413799},
  url = {https://doi.org/10.1121/1.413799},
  urldate = {2024-09-06},
  abstract = {A head and transducer support system (HATS) was developed for use in ultrasound imaging of tongue movement. Ultrasound is an imaging technique that captures tongue motion during speech and thus has great appeal as a tool for speech research. However, because ultrasound systems are designed for clinical use, the transducer is hand-held and it is almost impossible to hold it completely steady under the chin when collecting tongue data. A system was needed to fix the head and support the transducer under the chin in a known position without disturbing speech. The HATS system was designed, constructed, and modified to provide valid, reliable tongue movement data by (1) immobilizing the head and (2) positioning the ultrasound transducer in a known relationship to the head.},
  file = {/home/jpalo/Zotero/storage/PK4FCLQC/A-head-and-transducer-support-system-for-making.html}
}

@article{StoneEtAl-ComparisonSpeechProduction-2007,
  title = {Comparison of Speech Production in Upright and Supine Position},
  author = {Stone, M. and Stock, G. and Bunin, K. and Kumar, K. and Epstein, M. and Kambhamettu, C. and Li, M. and Parthasarathy, V. and Prince, J.},
  year = 2007,
  journal = {Journal of the Acoustical Society of America},
  volume = {122},
  number = {1},
  pages = {532--541}
}

@article{SumbyPollack-VisualContributionSpeech-1954,
  title = {Visual {{Contribution}} to {{Speech Intelligibility}} in {{Noise}}},
  author = {Sumby, W. H. and Pollack, Irwin},
  year = 1954,
  journal = {The Journal of the Acoustical Society of America},
  volume = {26},
  number = {2},
  pages = {212--215}
}

@book{SuomiEtAl-FinnishSoundStructure-2008,
  title = {Finnish {{Sound Structure}} -- {{Phonetics}}, Phonology, Phonotactics and Prosody},
  author = {Suomi, K. and Toivanen, J. and Ylitalo, R.},
  year = 2008,
  series = {{{STUDIA HUMANIORA OULUENSIA}}},
  publisher = {University of Oulu}
}

@article{SuzukiEtAl-SimulationVocalTract-1993,
  title = {Simulation of Vocal Tract with Three-Dimensional {{Finite Element Method}}},
  author = {Suzuki, H. and Nakai, T. and Takahashi, N. and Ishida, A.},
  year = 1993,
  journal = {Tech. Rep. IEICE},
  volume = {EA93-8},
  pages = {17--24}
}

@inproceedings{ŠvancaraEtAl-NumericalModellingProduction-2004,
  title = {Numerical Modelling of Production of {{Czech Wovel}} /a/ Based on {{FE}} Model of the Vocal Tract},
  booktitle = {Proceedings of {{International Conference}} on {{Voice Physiology}} and {{Biomechanics}}},
  author = {{\v S}vancara, P. and Hor{\'a}{\v c}ek, J. and Pe{\v s}ek, L.},
  year = 2004
}

@article{ŠvancaraHoráček-NumericalModellingEffect-2006,
  title = {Numerical {{Modelling}} of {{Effect}} of {{Tonsillectomy}} on {{Production}} of {{Czech Vowels}}},
  author = {{\v S}vancara, P. and Hor{\'a}{\v c}ek, J.},
  year = 2006,
  journal = {Acta Acustica united with Acustica},
  volume = {92},
  pages = {681--688}
}

@article{Tabain-VariabilityFricativeProduction-2001,
  title = {Variability in {{Fricative Production}} and {{Spectra}}: {{Implications}} for the {{Hyper-}} and {{Hypo-and Quantal Theories}} of {{Speech Production}}},
  shorttitle = {Variability in {{Fricative Production}} and {{Spectra}}},
  author = {Tabain, Marija},
  year = 2001,
  month = mar,
  journal = {Language and Speech},
  volume = {44},
  number = {1},
  pages = {57--93},
  publisher = {SAGE Publications Ltd},
  issn = {0023-8309},
  doi = {10.1177/00238309010440010301},
  url = {https://doi.org/10.1177/00238309010440010301},
  urldate = {2025-05-28},
  abstract = {Fricative spectral data are compared with articulatory data from electropalatographic (EPG) recordings in an investigation of coarticulatory effects on the acoustic signal. Data were taken from CV tokens produced by four female speakers of Australian English. Results are presented for the coronal fricatives /\texttheta{} s {$\int$} \dh{} z 3/in seven monophthong vowel contexts. The analysis consists of a comparison of spectral centre of gravity (COG) with EPG centre of gravity measured along the horizontal dimension. The correlation between the articulatory and the acoustic datais quitehigh. Overall, the sibilant fricatives show very little variability in production, while the nonsibilant dental shows a good deal of variability. This is reflected in the spectral output. It is also shown that the alveolar sibilants show more effectfrom vowel context than do the postalveolar sibilants. These results are interpreted as showing that coarticulatory resistance is indeed greater for sibilant fricatives, but that degree of tongue body raising inherent in the fricative's production must also be taken into account. The results for overall variability are discussed with reference to the Hyper- and Hypo- and Quantal Theories of speech production.It is suggested that sibilant fricatives do not lend themselves to the articulatory imprecision which, according to these theories, characterizes perceptually salient, and typologically common, speech sounds.},
  langid = {english},
  file = {/home/jpalo/Zotero/storage/R9QCVY26/Tabain - 2001 - Variability in Fricative Production and Spectra Implications for the Hyper- and Hypo-and Quantal Th.pdf}
}

@article{TakemotoEtAl-MethodToothSuperimposition-2004,
  title = {A Method of Tooth Superimposition on {{MRI}} Data for Accurate Measurement of Vocal Tract Shape and Dimensions},
  author = {Takemoto, H. and Kitamura, T. and Nishimoto, H. and Honda, K.},
  year = 2004,
  journal = {Acoustic Science and Technology},
  number = {25},
  pages = {468--474}
}

@article{TheoisAmrhein-TheoreticalAnalysisCognitive-1989,
  title = {Theoretical {{Analysis}} of the {{Cognitive Processing}} of {{Lexical}} and {{Pictorial Stimuli}}: {{Reading}}, {{Naming}}, and {{Visual}} and {{Conceptual Comparisons}}},
  author = {Theois, J. and Amrhein, P. C.},
  year = 1989,
  journal = {Psychological Review},
  volume = {96},
  number = {1},
  pages = {5--24}
}

@article{Tiippana-WhatMcGurkEffect-2014,
  title = {What Is the {{McGurk}} Effect?},
  author = {Tiippana, Kaisa},
  year = 2014,
  month = jul,
  journal = {Frontiers in Psychology},
  volume = {5},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.00725},
  url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2014.00725/full},
  urldate = {2025-01-23},
  abstract = {McGurk and MacDonald (1976) reported a powerful multisensory illusion occurring with audiovisual speech. They recorded a voice articulating a consonant and dubbed it with a face articulating another consonant. Even though the acoustic speech signal was well recognized alone, it was heard as another consonant after dubbing with incongruent visual speech. The illusion has been termed the McGurk effect. It has been replicated many times, and it has sparked an abundance of research. The reason for the great impact is that this is a striking demonstration of multisensory integration. It shows that auditory and visual information is merged into a unified, integrated percept. It is a very useful research tool since the strength of the McGurk effect can be taken to reflect the strength of audiovisual integration.Here I shall make two main claims regarding the definition and interpretation of the McGurk effect since they bear relevance to its use as a measure of multisensory integration. First, the McGurk effect should be defined as a categorical change in auditory perception induced by incongruent visual speech, resulting in a single percept of hearing something other than what the voice is saying. Second, when interpreting the McGurk effect, it is crucial to take into account the perception of the unisensory acoustic and visual stimulus components.There are many variants of the McGurk effect (McGurk \&amp; MacDonald, 1976; MacDonald \&amp; McGurk, 1978) . The best-known case is when dubbing a voice saying [b] onto a face articulating [g] results in hearing [d]. This is called the fusion effect since the percept differs from the acoustic and visual components. Many researchers have defined the McGurk effect exclusively as the fusion effect because here integration results in the perception of a third consonant, obviously merging information from audition and vision (Keil, Muller, Ihssen, \&amp; Weisz, 2012; Setti, Burke, Kenny, \&amp; Newell, 2013; van Wassenhove, Grant, \&amp; Poeppel, 2007). This definition ignores the fact that other incongruent audiovisual stimuli produce different types of percepts. For example, a reverse combination of these consonants, A[g]V[b], is heard as [bg], i.e. the visual and auditory components one after the other. There are other pairings, which result in hearing according to the visual component, e.g. acoustic [b] presented with visual [d] is heard as [d]. Here my first claim is that the definition of the McGurk effect should be that an acoustic utterance is heard as another utterance when presented with discrepant visual articulation. This definition includes all variants of the illusion, and it has been used by MacDonald and McGurk (1978) themselves, as well as by several others (e.g. Brancazio, Miller, \&amp; Par\&\#233;, 2003; Rosenblum \&amp; Salda\&\#241;a, 1996). The different variants of the McGurk effect represent the outcome of audiovisual integration. When integration takes place, it results in a unified percept, without access to the individual components that contributed to the percept. Thus, when the McGurk effect occurs, the observer has the subjective experience of hearing a certain utterance, even though another utterance is presented acoustically.One challenge with this interpretation of the McGurk effect is that it is impossible to be certain that the responses the observer gives correspond to the actual percepts. The real McGurk effect arises due to multisensory integration, resulting in an altered auditory percept. However, if integration does not occur, the observer can perceive the components separately and may choose to respond either according to what he heard or according to what he saw. This is one reason why the fusion effect is so attractive: If the observer reports a percept that differs from both stimulus components, he does not seem to rely on either modality alone, but instead really fuse the information from both. However, this approach does not guarantee a straightforward measure of integration any more than the other variants of the illusion, as is argued below.The second main claim here is that the perception of the acoustic and visual stimulus components has to be taken into account when interpreting the McGurk effect. This issue has been elaborated previously in the extensive work by Massaro and colleagues (Massaro, 1987, 1998) and others (Green \&amp; Norrix, 1997; Jiang \&amp; Bernstein, 2011; Sekiyama \&amp; Tohkura, 1991). It is important because the identification accuracy of unisensory components is reflected into audiovisual speech perception.In general, the strength of the McGurk effect is taken to increase when the proportion of responses according to the acoustic component decreases and/or when the proportion of fusion responses increases. That is, the McGurk effect for stimulus A[b]V[g] is considered stronger when fewer B responses and/or more D responses are given. This is often an adequate way to measure the strength of the McGurk effect -- if one keeps in mind that it implicitly assumes that perception of the acoustic and visual components is accurate (or at least constant across conditions that are compared). However, it can lead to erroneous conclusions if this assumption does not hold.The fusion effect provides a prime example of this caveat. It has been interpreted to mean that acoustic and visual information is integrated to produce a novel, intermediate percept. For example, when A[b]V[g] is heard as [d], the percept is thought to emerge due to fusion of the features (for the place of articulation) provided via audition (bilabial) and vision (velar), so that a different, intermediate consonant (alveolar) is perceived (van Wassenhove, 2013). However, already McGurk and MacDonald (1976) themselves wrote that ``lip movements for [ga] are frequently misread as [da]'', even though they did not measure speechreading performance, unfortunately. The omission of the unisensory visual condition in the original study is one factor that has contributed to the strong status of the fusion effect as the only real McGurk effect, reflecting true integration. Still, if visual [g] is confused with [d], it is not at all surprising or special if A[b]V[g] is perceived as [d].To demonstrate the contribution of the unisensory components more explicitly, I'll take two examples of my research, in which fusion-type stimuli produced different percepts depending on the clarity of the visual component. In one study, a McGurk stimulus A[epe]V[eke] was mainly heard as a fusion [ete] (Tiippana, Andersen, \&amp; Sams, 2004). This reflected the fact that in a visual-only identification task, the visual [eke] was confused with [ete] (42\% K responses and 45\% T responses to visual [eke]). In another study, a McGurk stimulus A[apa]V[aka] was mainly heard as [aka], and this could be traced back to the fact that in a visual-only identification task, the visual [aka] was clearly distinguishable from [ata], and thus recognized very accurately (100\% correct in typical adults; Saalasti et al., 2012; but note the deviant behaviour of individuals with Asperger syndrome). Thus, even though the McGurk stimuli were of a fusion type in both studies, their perception differed depending largely on the clarity of the visual components. These findings underscore the importance of knowing the perceptual qualities of the unisensory stimuli before making conclusions about multisensory integration.Exactly how to take the properties of the unisensory components into account in multisensory perception of speech is beyond this paper. Addressing this issue in detail requires carefully designed experimental studies (Alsius, Navarra, Campbell, \&amp; Soto-Faraco, 2005; Bertelson, Vroomen, \&amp; De Gelder, 2003), computational modelling (Massaro, 1987, 1998; Schwartz, 2010), and investigation of the underlying brain mechanisms (Sams et al., 1991; Skipper, van Wassenhove, Nusbaum, \&amp; Small, 2007). However, the main guideline is that unisensory perception of stimulus components is reflected into multisensory perception of the whole (Ernst \&amp; B\&\#252;lthoff, 2004).During experiments, when the task is to report what was heard, the observer reports the conscious auditory percept evoked by the audiovisual stimulus. If there is no multisensory integration or interaction, the percept is identical for the audiovisual stimulus and the auditory component presented alone. If there is audiovisual integration, the conscious auditory percept changes. To which extent visual input influences the percept depends on how coherent and reliable information each modality provides. Coherent information is integrated and weighted e.g. according to the reliability of each modality, which is reflected in unisensory discriminability.This perceptual process is the same for audiovisual speech -- be it natural, congruent audiovisual speech or artificial, incongruent McGurk speech stimuli. The outcome is the conscious auditory percept. Depending on the relative weighting of audition and vision, the outcome for McGurk stimuli can range from hearing according to the acoustic component (when audition is more reliable than vision) to fusion and combination percepts (when both modalities are informative to some extent) to hearing according to the visual component (when vision is more reliable than audition). Congruent audiovisual speech is treated no differently, showing visual influence when the auditory reliability decreases. The different variants of the McGurk effect are all results of this same perceptual process and reflect audiovisual integration.The McGurk effect is an excellent tool to investigate multisensory integration in speech perception. The main messages of this opinion paper are, first, that the McGurk effect should be defined as a change in auditory perception due to incongruent visual speech, so that observers hear another speech sound than what the voice uttered, and second, that the perceptual properties of the acoustic and visual stimulus components should be taken into account when interpreting the McGurk effect as reflecting integration.},
  langid = {english},
  keywords = {audiovisual,Illusion,integration,McGurk effect,multisensory,Perception,Speech},
  file = {/home/jpalo/Zotero/storage/5Q9395LJ/Tiippana - 2014 - What is the McGurk effect.pdf}
}

@article{TiippanaEtAl-InvestigationCrossLanguageStimulusDependent-2023,
  title = {Investigation of {{Cross-Language}} and {{Stimulus-Dependent Effects}} on the {{McGurk Effect}} with {{Finnish}} and {{Japanese Speakers}} and {{Listeners}}},
  author = {Tiippana, Kaisa and Ujiie, Yuta and Peromaa, Tarja and Takahashi, Kohske},
  year = 2023,
  month = aug,
  journal = {Brain Sciences},
  volume = {13},
  number = {8},
  pages = {1198},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3425},
  doi = {10.3390/brainsci13081198},
  url = {https://www.mdpi.com/2076-3425/13/8/1198},
  urldate = {2025-01-23},
  abstract = {In the McGurk effect, perception of a spoken consonant is altered when an auditory (A) syllable is presented with an incongruent visual (V) syllable (e.g., A/pa/V/ka/ is often heard as /ka/ or /ta/). The McGurk effect provides a measure for visual influence on speech perception, becoming stronger the lower the proportion of auditory correct responses. Cross-language effects are studied to understand processing differences between one's own and foreign languages. Regarding the McGurk effect, it has sometimes been found to be stronger with foreign speakers. However, other studies have shown the opposite, or no difference between languages. Most studies have compared English with other languages. We investigated cross-language effects with native Finnish and Japanese speakers and listeners. Both groups of listeners had 49 participants. The stimuli (/ka/, /pa/, /ta/) were uttered by two female and male Finnish and Japanese speakers and presented in A, V and AV modality, including a McGurk stimulus A/pa/V/ka/. The McGurk effect was stronger with Japanese stimuli in both groups. Differences in speech perception were prominent between individual speakers but less so between native languages. Unisensory perception correlated with McGurk perception. These findings suggest that stimulus-dependent features contribute to the McGurk effect. This may have a stronger influence on syllable perception than cross-language factors.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {audiovisual,cross-language,Finnish,Japanese,McGurk effect,speech perception,stimulus features},
  file = {/home/jpalo/Zotero/storage/NZ2PQF74/Tiippana et al. - 2023 - Investigation of Cross-Language and Stimulus-Dependent Effects on the McGurk Effect with Finnish and.pdf}
}

@article{TilsenGoldstein-ArticulatoryGesturesAre-2012,
  title = {Articulatory Gestures Are Individually Selected in Production},
  author = {Tilsen, S. and Goldstein, L.},
  year = 2012,
  journal = {Journal of Phonetics},
  volume = {40},
  pages = {764--779}
}

@article{Titze-CommentsMyoelasticAerodynamic-1980,
  title = {Comments on the Myoelastic - Aerodynamic Theory of Phonation},
  author = {Titze, I.R.},
  year = 1980,
  journal = {Journal of Speech and Hearing Research},
  volume = {23},
  number = {3},
  pages = {495--510}
}

@article{Titze-NonlinearSourcefilterCoupling-2008,
  title = {Nonlinear Source-Filter Coupling in Phonation: {{Theory}}},
  author = {Titze, I. R.},
  year = 2008,
  journal = {Journal of the Acoustical Society of America},
  volume = {123},
  number = {5},
  pages = {2733--2749}
}

@book{ToelkenWilson-AnguishSnails-2003,
  title = {The {{Anguish}} of {{Snails}}},
  editor = {Toelken, Barre and Wilson, William A.},
  year = 2003,
  series = {Folklife of the {{West}}},
  volume = {2},
  publisher = {Utah State University Press},
  address = {Logan, Utah}
}

@book{Townsend-OutThere-2015,
  title = {Out {{There}} -- {{A Voice From}} the {{Wild}}},
  shorttitle = {Out There},
  author = {Townsend, Chris},
  year = 2015,
  publisher = {Sandstone Press},
  address = {Dingwall, Scotland},
  isbn = {978-1-910124-72-7},
  langid = {english}
}

@article{TreebyCox-KWaveMATLABToolbox-2010,
  title = {K-{{Wave}}: {{MATLAB}} Toolbox for the Simulation and Reconstruction of Photoacoustic Wave-Fields},
  author = {Treeby, B. E. and Cox, B. T.},
  year = 2010,
  journal = {Journal of Biomedical Optics},
  volume = {15},
  number = {2},
  pages = {021314}
}

@article{TreebyEtAl-ModelingNonlinearUltrasound-2012,
  title = {Modeling Nonlinear Ultrasound Propagation in Heterogeneous Media with Power Law Absorption Using a K-Space Pseudospectral Method},
  author = {Treeby, B. E. and Jaros, J. and Rendell, A. P. and Cox, B. T.},
  year = 2012,
  journal = {Journal of the Acoustical Society of America},
  volume = {131},
  number = {6},
  pages = {4324--4336}
}

@book{TuckMcKenzie-PlaceResearchTheory-2015,
  title = {Place in {{Research Theory}}, {{Methodology}}, and {{Methods}}},
  shorttitle = {Place in {{Research}}},
  author = {Tuck, Eve and McKenzie, Marcia},
  year = 2015,
  publisher = {Routledge},
  isbn = {978-1-138-63968-3}
}

@article{TullerFowler-ArticulatoryCorrelatesPerceptual-1980,
  title = {Some Articulatory Correlates of Perceptual Isochrony},
  author = {Tuller, B. and Fowler, C.A.},
  year = 1980,
  journal = {Perception \& Psychophysics},
  volume = {27},
  number = {4},
  pages = {277--283}
}

@incollection{TurkEtAl-AcousticSegmentDurations-2006,
  title = {Acoustic Segment Durations in Prosodic Research: A Practical Guide},
  booktitle = {Methods in {{Empirical Prosody Research}}},
  author = {Turk, A. and Nakai, S. and Sugahara, M.},
  editor = {Sudhoff, S. and Lenertov{\'a}, D. and Meyer, R. and Pappert, S. and Augurzky, P. and Mleinek, I. and Richter, N. and Schliesser, J.},
  year = 2006,
  pages = {1--28},
  publisher = {De Gruyter},
  address = {Berlin, New York}
}

@article{TurkEtAl-EdinburghSpeechProduction-2010,
  title = {The {{Edinburgh Speech Production Facility}}'s Articulatory Corpus of Spontaneous Dialogue.},
  author = {Turk, A. and Scobbie, J. and Geng, C. and Macmartin, C. and Bard, E. and Campbell, B. and Dickie, C. and Dubourg, E. and Hardcastle, B. and Hoole, P. and Kanaida, E. and Lickley, R. and Nakai, S. and Pouplier, M. and King, S. and Renals, S. and Richmond, K. and Schaeffler, S. and Wiegand, R. and White, K. and Wrench, A.},
  year = 2010,
  journal = {The Journal of the Acoustical Society of America},
  volume = {128},
  number = {4},
  pages = {2429--2429},
  publisher = {ASA}
}

@article{TylerEtAl-DelayedTriggerVoice-2005,
  title = {The Delayed Trigger Voice Key: {{An}} Improved Analogue Voice Key for Psycholinguistic Research},
  author = {Tyler, M. D. and Tyler, L. and Burnham, D. K.},
  year = 2005,
  journal = {Behavior Research Methods},
  volume = {37},
  number = {1},
  pages = {139--147},
  publisher = {Springer-Verlag},
  issn = {1554-351X},
  doi = {10.3758/BF03206408},
  url = {http://dx.doi.org/10.3758/BF03206408},
  langid = {english}
}

@misc{Ultrax-ProjectWebsite-2014,
  title = {Project Website},
  author = {{Ultrax}},
  year = 2014,
  annotation = {Published: Accessed 23th May, 2014}
}

@incollection{VainioEtAl-LahdeJaSuodin-2009,
  title = {{L\"ahde ja suodin -- puheentuoton akustiikasta ja sen mallintamisesta}},
  booktitle = {{Puhuva ihminen}},
  author = {Vainio, M. and Palo, P. and Aalto, D. and Laine, U. K.},
  year = 2009,
  publisher = {Otava},
  address = {Helsinki},
  langid = {finnish}
}

@article{Valls-SoléEtAl-PatternedBallisticMovements-1999,
  title = {Patterned Ballistic Movements Triggered by a Startle in Healthy Humans},
  author = {{Valls-Sol{\'e}}, J. and Rothwell, J. C. and Goulart, F.R. and Cossu, G.},
  year = 1999,
  journal = {Journal of Physiology},
  volume = {516},
  pages = {931--938}
}

@article{VampolaEtAl-VocalTractChanges-2011,
  title = {Vocal Tract Changes Caused by Phonation into a Tube: {{A}} Case Study Using Computer Tomography and Finite-Element Modeling},
  author = {Vampola, T. and Laukkanen, A.-M. and Hor{\'a}cek, J. and {\v S}vec, J. G.},
  year = 2011,
  journal = {The Journal of the Acoustical Society of America},
  volume = {129},
  number = {1},
  pages = {310--315}
}

@incollection{VanBuuren-PosturaClearDark-1995,
  title = {Postura: {{Clear}} and Dark Consonants, Etcetera},
  booktitle = {Studies in {{General}} and {{English Phonetics}}: {{Essays}} in {{Honour}} of {{Professor J}}. {{D}}. {{O}}'{{Connor}}},
  author = {Van Buuren, L.},
  editor = {Lewis, J. W.},
  year = 1995,
  pages = {130--142},
  publisher = {Routledge},
  address = {New York}
}

@article{vanderLindenEtAl-ComparisonTwoProcedures-2014,
  title = {A Comparison of Two Procedures for Verbal Response Time Fractionation},
  author = {{van der Linden}, Lotje and Ries, Stephanie Kathleen and Legou, Thierry and Burle, Boris and Malfait, Nicole and Alario, F.-Xavier},
  year = 2014,
  journal = {Frontiers in Psychology},
  volume = {5},
  number = {1213},
  pages = {1--11}
}

@misc{vanderLindenEtAl-FractionationVerbalResponse-2014,
  title = {On the Fractionation of Verbal Response Times},
  author = {{van der Linden}, L. and Ri{\'e}s, S. and Legou, T. and Burle, B. and Malfait, N. and Alario, F.-X.},
  year = 2014,
  annotation = {Published: Poster presented at the International Workshop of Language Production}
}

@article{VasseljenEtAl-OnsetAbdominalMuscles-2009,
  title = {Onset in Abdominal Muscles Recorded Simultaneously by Ultrasound Imaging and Intramuscular Electromyography},
  author = {Vasseljen, O. and Fladmark, A. M. and Westad, C. and Torp, H. G.},
  year = 2009,
  journal = {Journal of Electromyography and Kinesiology},
  volume = {19},
  number = {2},
  pages = {e23 -- e31},
  issn = {1050-6411}
}

@inproceedings{VogtEtAl-EfficientBiomechanicalTongue-2006,
  title = {An Efficient Biomechanical Tongue Model for Speech Research},
  booktitle = {In {{Proc}}. 7th {{International Seminar}} on {{Speech Production}} ({{ISSP}})},
  author = {Vogt, F. and Lloyd, J. E. and Perrier, P. and Chabanas, M. Payan, Y. and Fels, S.},
  year = 2006,
  pages = {51--58}
}

@article{WaaramaaEtAl-EmotionsFreelyVarying-2014,
  title = {Emotions in Freely Varying and Mono-Pitched Vowels, Acoustic and {{EGG}} Analyses},
  author = {Waaramaa, T. and Palo, P. and Kankare, E.},
  year = 2014,
  journal = {Logopedics Phoniatrics Vocology},
  pages = {1--15}
}

@book{Wallis-GrammaticaLinguaeAgnlicanae-1653,
  title = {Grammatica Linguae Agnlicanae},
  author = {Wallis, J.},
  year = 1653,
  publisher = {London: Longman}
}

@article{WangBovik-MeanSquaredError-2009,
  title = {Mean Squared Error: {{Love}} It or Leave It? {{A}} New Look at {{Signal Fidelity Measures}}},
  shorttitle = {Mean Squared Error},
  author = {Wang, Zhou and Bovik, Alan C.},
  year = 2009,
  month = jan,
  journal = {IEEE Signal Processing Magazine},
  volume = {26},
  number = {1},
  pages = {98--117},
  issn = {1558-0792},
  doi = {10.1109/MSP.2008.930649},
  url = {https://ieeexplore.ieee.org/document/4775883},
  urldate = {2024-09-12},
  abstract = {In this article, we have reviewed the reasons why we (collectively) want to love or leave the venerable (but perhaps hoary) MSE. We have also reviewed emerging alternative signal fidelity measures and discussed their potential application to a wide variety of problems. The message we are trying to send here is not that one should abandon use of the MSE nor to blindly switch to any other particular signal fidelity measure. Rather, we hope to make the point that there are powerful, easy-to-use, and easy-to-understand alternatives that might be deployed depending on the application environment and needs. While we expect (and indeed, hope) that the MSE will continue to be widely used as a signal fidelity measure, it is our greater desire to see more advanced signal fidelity measures being used, especially in applications where perceptual criteria might be relevant. Ideally, the performance of a new signal processing algorithm might be compared to other algorithms using several fidelity criteria. Lastly, we hope that we have given further motivation to the community to consider recent advanced signal fidelity measures as design criteria for optimizing signal processing algorithms and systems. It is in this direction that we believe that the greatest benefit eventually lies.},
  keywords = {Algorithm design and analysis,Distortion measurement,Dynamic range,Image processing,Pixel,Pollution measurement,PSNR,Signal design,Signal processing,Signal processing algorithms},
  file = {/home/jpalo/Zotero/storage/8PVMI9DB/Wang and Bovik - 2009 - Mean squared error Love it or leave it A new look at Signal Fidelity Measures.pdf;/home/jpalo/Zotero/storage/6JGT6SZH/4775883.html}
}

@book{Watts-Firefall-2014,
  title = {Firefall},
  author = {Watts, Peter},
  year = 2014,
  series = {Firefall},
  annotation = {Combination of Blindsight and Echopraxia}
}

@book{Wells-ComputercodingIPAProposed-1995,
  title = {Computer-Coding the {{IPA}}: A Proposed Extension of {{SAMPA}}},
  author = {Wells, John C.},
  year = 1995,
  publisher = {{UCL Phonetics and Linguistics}},
  address = {University College London}
}

@article{WhalenEtAl-HaskinsOpticallyCorrected-2005,
  title = {The {{Haskins Optically Corrected Ultrasound System}} ({{HOCUS}})},
  author = {Whalen, D. and Iskarous, K. and Tiede, M. K. and Ostry, D. J. and {Lehnert-Lehouillier}, H. and {Vatikiotis-Bateson}, E. and Hailey, D. S.},
  year = 2005,
  journal = {Journal of Speech, Language, and Hearing Research},
  volume = {48},
  pages = {543--553}
}

@book{Wickham-Ggplot2ElegantGraphics-2016,
  title = {Ggplot2: {{Elegant Graphics}} for {{Data Analysis}}},
  author = {Wickham, Hadley},
  year = 2016,
  publisher = {Springer-Verlag New York},
  url = {https://ggplot2.tidyverse.org},
  isbn = {978-3-319-24277-4}
}

@phdthesis{Wiik-FinnishEnglishVowels-1965,
  title = {Finnish and {{English Vowels}}. {{A}} Comparison with Special Reference to the Learning Problems Met by Native Speakers of {{Finnish}} Learning {{English}}},
  author = {Wiik, K.},
  year = 1965,
  school = {University of Turku}
}

@article{Wikipedia-Epistemology-2025,
  title = {Epistemology},
  author = {Wikipedia},
  year = 2025,
  month = dec,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Epistemology&oldid=1330312116},
  urldate = {2026-01-12},
  abstract = {Epistemology is the branch of philosophy that examines the nature, origin, and limits of knowledge. Also called the theory of knowledge, it explores different types of knowledge, such as propositional knowledge about facts, practical knowledge in the form of skills, and knowledge by acquaintance as a familiarity through experience. Epistemologists study the concepts of belief, truth, and justification to understand the nature of knowledge. To discover how knowledge arises, they investigate sources of justification, such as perception, introspection, memory, reason, and testimony. The school of skepticism questions the human ability to attain knowledge, while fallibilism says that knowledge is never certain. Empiricists hold that all knowledge comes from sense experience, whereas rationalists believe that some knowledge does not depend on it. Coherentists argue that a belief is justified if it is consistent with other beliefs. Foundationalists, by contrast, maintain that the justification of basic beliefs does not depend on other beliefs. Internalism and externalism debate whether justification is determined solely by mental states or also by external circumstances. Separate branches of epistemology focus on knowledge in specific fields, like scientific, mathematical, moral, and religious knowledge. Naturalized epistemology relies on empirical methods and discoveries, whereas formal epistemology uses formal tools from logic. Social epistemology investigates the communal aspect of knowledge, and historical epistemology examines its historical conditions. Epistemology is closely related to psychology, which infers the beliefs people hold from their words and actions, while epistemology studies the norms governing the evaluation of beliefs. It also intersects with fields such as decision theory, education, and anthropology. Early reflections on the nature, sources, and scope of knowledge are found in ancient Greek, Indian, and Chinese philosophy. The relation between reason and faith was a central topic in the medieval period. The modern era was characterized by the contrasting perspectives of empiricism and rationalism. Epistemologists in the 20th century examined the components, structure, and value of knowledge while integrating insights from the natural sciences and linguistics.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1330312116},
  file = {/home/jpalo/Zotero/storage/YEHYRVGR/index.html}
}

@misc{Wikipedia-EravaelluksenSuomenmestaruuskilpailutWikipedia-,
  title = {Er\"avaelluksen {{Suomen-mestaruuskilpailut}} --- {{Wikipedia}},},
  author = {{Wikipedia}},
  url = {http://fi.wikipedia.org/w/index.php?title=Er%C3%A4vaelluksen_Suomen-mestaruuskilpailut&oldid=18737407},
  urldate = {2020-06-03}
}

@misc{Wikipedia-ObsoleteFinnishUnits-,
  title = {Obsolete {{Finnish}} Units of Measurement},
  author = {{Wikipedia}},
  url = {https://en.wikipedia.org/wiki/Obsolete_Finnish_units_of_measurement},
  urldate = {2021-01-11}
}

@article{Wikipedia-Ontology-2026,
  title = {Ontology},
  author = {Wikipedia},
  year = 2026,
  month = jan,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Ontology&oldid=1330758796},
  urldate = {2026-01-12},
  abstract = {Ontology is the philosophical study of being. It is traditionally understood as the subdiscipline of metaphysics focused on the most general features of reality. As one of the most fundamental concepts, being encompasses all of reality and every entity within it. To articulate the basic structure of being, ontology examines the commonalities among all things and investigates their classification into basic types, such as the categories of particulars and universals. Particulars are unique, non-repeatable entities, such as the person Socrates, whereas universals are general, repeatable entities, like the color green. Another distinction exists between concrete objects existing in space and time, such as a tree, and abstract objects existing outside space and time, like the number 7. Systems of categories aim to provide a comprehensive inventory of reality by employing categories such as substance, property, relation, state of affairs, and event. Ontologists disagree regarding which entities exist at the most basic level. Platonic realism asserts that universals have objective existence, while conceptualism maintains that universals exist only in the mind, and nominalism denies their existence altogether. Similar disputes pertain to mathematical objects, unobservable objects assumed by scientific theories, and moral facts. Materialism posits that fundamentally only matter exists, whereas dualism asserts that mind and matter are independent principles. According to some ontologists, objective answers to ontological questions do not exist, with perspectives shaped by differing linguistic practices. Ontology employs diverse methods of inquiry, including the analysis of concepts and experience, the use of intuitions and thought experiments, and the integration of findings from natural science. Formal ontology investigates the most abstract features of objects, while applied ontology utilizes ontological theories and principles to study entities within specific domains. For example, social ontology examines basic concepts used in the social sciences. Applied ontology is particularly relevant to information and computer science, which develop conceptual frameworks of limited domains. These frameworks facilitate the structured storage of information, such as in a college database tracking academic activities. Ontology is also pertinent to the fields of logic, theology, and anthropology. The origins of ontology lie in the ancient period with speculations about the nature of being and the source of the universe, including ancient Indian, Chinese, and Greek philosophy. In the modern period, philosophers conceived ontology as a distinct academic discipline and coined its name.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1330758796},
  file = {/home/jpalo/Zotero/storage/NKILVPLS/index.html}
}

@phdthesis{Wilson-ArticulatorySettingsFrench-2006,
  title = {Articulatory {{Settings}} of {{French}} and {{English}} Monolingual and Bilingual Speakers},
  author = {Wilson, I.},
  year = 2006,
  school = {University of British Colombia}
}

@inproceedings{WinklerEtAl-BiomechanicalTongueModels-2011,
  title = {Biomechanical {{Tongue Models}}: {{An Approach}} to {{Studying Inter-speaker Variability}}},
  booktitle = {Interspeech 2011},
  author = {Winkler, Ralf and Fuchs, Susanne and Perrier, Pascal and Tiede, Mark},
  year = 2011,
  address = {Italy}
}

@article{WrenchBalch-Tomes-EdgeMarkerlessPose-2022,
  title = {Beyond the {{Edge}}: {{Markerless Pose Estimation}} of {{Speech Articulators}} from {{Ultrasound}} and {{Camera Images Using DeepLabCut}}},
  shorttitle = {Beyond the {{Edge}}},
  author = {Wrench, Alan and {Balch-Tomes}, Jonathan},
  year = 2022,
  journal = {Sensors},
  volume = {22},
  number = {3},
  pages = {1133},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s22031133},
  url = {https://www.mdpi.com/1424-8220/22/3/1133},
  urldate = {2024-11-12},
  abstract = {Automatic feature extraction from images of speech articulators is currently achieved by detecting edges. Here, we investigate the use of pose estimation deep neural nets with transfer learning to perform markerless estimation of speech articulator keypoints using only a few hundred hand-labelled images as training input. Midsagittal ultrasound images of the tongue, jaw, and hyoid and camera images of the lips were hand-labelled with keypoints, trained using DeepLabCut and evaluated on unseen speakers and systems. Tongue surface contours interpolated from estimated and hand-labelled keypoints produced an average mean sum of distances (MSD) of 0.93, s.d. 0.46 mm, compared with 0.96, s.d. 0.39 mm, for two human labellers, and 2.3, s.d. 1.5 mm, for the best performing edge detection algorithm. A pilot set of simultaneous electromagnetic articulography (EMA) and ultrasound recordings demonstrated partial correlation among three physical sensor positions and the corresponding estimated keypoints and requires further investigation. The accuracy of the estimating lip aperture from a camera video was high, with a mean MSD of 0.70, s.d. 0.56 mm compared with 0.57, s.d. 0.48 mm for two human labellers. DeepLabCut was found to be a fast, accurate and fully automatic method of providing unique kinematic data for tongue, hyoid, jaw, and lips.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {keypoints,landmarks,lip reading,multimodal speech,pose estimation,speech kinematics,ultrasound tongue imaging},
  file = {/home/jpalo/Zotero/storage/KEFVFPLR/Wrench and Balch-Tomes - 2022 - Beyond the Edge Markerless Pose Estimation of Speech Articulators from Ultrasound and Camera Images.pdf}
}

@inproceedings{WrenchScobbie-HighspeedCineloopUltrasound-2008,
  title = {High-Speed {{Cineloop Ultrasound}} vs. {{Video Ultrasound Tongue Imaging}}: {{Comparison}} of {{Front}} and {{Back Lingual Gesture Location}} and {{Relative Timing}}.},
  shorttitle = {High-Speed {{Cineloop Ultrasound}} vs. {{Video Ultrasound Tongue Imaging}}},
  booktitle = {Proceedings of {{ISSP}} 2008 - 8th {{International Seminar}} on {{Speech Production}}},
  author = {Wrench, A. and Scobbie, J. M.},
  year = 2008,
  abstract = {We compare two methods of acquiring ultrasound tongue images. A new system capable of recording directly from the cineloop image buffer at a high frame rate and which is more accurately synchronized with audio is compared with an optimised method of recording images via the NTSC video output of an ultrasound machine. As a focus for this comparison we gathered representative data on English /l from a single speaker, using a headset restraint system. Both systems performed well, but while the video system is at its limits, the cineloop system is inherently more accurate and offers greater opportunity for development.},
  file = {/home/jpalo/Zotero/storage/FM228UT5/Wrench and Scobbie - 2008 - High-speed Cineloop Ultrasound vs. Video Ultrasoun.pdf}
}

@techreport{WrenchScobbie-QueenMargaretUniversity-2016,
  title = {Queen {{Margaret University}} Ultrasound, Audio and Video Multichannel Recording Facility (2008-2016)},
  author = {Wrench, A. A. and Scobbie, J. M.},
  year = 2016,
  institution = {Queen Margaret University}
}

@inproceedings{WrenchScobbie-SpatiotemporalInaccuraciesVideobased-2006,
  title = {Spatio-Temporal Inaccuracies of Video-Based Ultrasound Images of the Tongue},
  booktitle = {Proceedings of the 7th {{International Seminar}} on {{Speech Production}}},
  author = {Wrench, A. A. and Scobbie, J. M.},
  year = 2006,
  month = dec,
  pages = {451--458},
  address = {Ubatuba, Brazil}
}

@inproceedings{WrenchScobbie-VeryHighFrame-2011,
  title = {Very High Frame Rate Ultrasound Tongue Imaging},
  booktitle = {Proceedings of {{ISSP}} 9},
  author = {Wrench, A. A. and Scobbie, J. M.},
  year = 2011,
  pages = {155--162},
  address = {Montreal}
}

@article{Xu-DefenseLabSpeech-2010,
  title = {In Defense of Lab Speech},
  author = {Xu, Yi},
  year = 2010,
  journal = {Journal of Phonetics},
  volume = {38},
  pages = {329--336}
}

@misc{Xu-ResearchPhilosophies-2014,
  title = {Research Philosophies},
  author = {Xu, Y.},
  year = 2014,
  annotation = {Published: Webpage. Accessed 16th May, 2014}
}

@article{XuEtAl-ComparativeStudyContour-2016,
  title = {A Comparative Study on the Contour Tracking Algorithms in Ultrasound Tongue Images with Automatic Re-Initialization},
  author = {Xu, K. and Csapo, T. G. and Roussel, P. and Denby, B.},
  year = 2016,
  journal = {Journal of the Acoustical Society of America Express Letters},
  volume = {139},
  pages = {EL1154}
}

@article{XuLiu-TonalAlignmentSyllable-2006,
  title = {Tonal Alignment, Syllable Structure and Coarticulation: {{Toward}} an Integrated Model},
  author = {Xu, Y. and Liu, F.},
  year = 2006,
  journal = {Italian Journal of Linguistics},
  volume = {18},
  pages = {125--159}
}

@article{YamadaTamaoka-MeasurementErrorsVoicekey-2003,
  title = {Measurement Errors in Voice-Key Naming Latency for Hiragana},
  author = {Yamada, J. and Tamaoka, K.},
  year = 2003,
  journal = {Perceptual and motor skills},
  volume = {97},
  number = {3f},
  pages = {1100--1106},
  publisher = {Ammons Scientific}
}

@article{YanagiharaKoike-RegulationSustainedPhonation-1967,
  title = {The {{Regulation}} of {{Sustained Phonation}}},
  author = {Yanagihara, N. and Koike, Y.},
  year = 1967,
  journal = {Folia phoniatrica},
  volume = {19},
  pages = {1--18}
}

@article{YunusovaEtAl-AccuracyAssessmentAG500-2009,
  title = {Accuracy Assessment for {{AG500}}, Electromagnetic Articulograph},
  author = {Yunusova, Y. and Green, J.R. and Mefferd, A.},
  year = 2009,
  journal = {Journal of Speech, Language and Hearing Research},
  volume = {52},
  pages = {547--555}
}

@article{ZharkovaEtAl-QuantifyingLingualCoarticulation-2015,
  title = {Quantifying Lingual Coarticulation Using Ultrasound Imaging Data Collected with and without Head Stabilisation},
  author = {Zharkova, N. and Gibbon, F. E. and Hardcastle, W. J.},
  year = 2015,
  journal = {Clinical Linguistics \& Phonetics},
  volume = {29},
  number = {4},
  pages = {249--265}
}

@inproceedings{ZharkovaEtAl-UltrasoundStudyLingual-2008,
  title = {An {{Ultrasound Study}} of {{Lingual Coarticulation}} in {{Children}} and {{Adults}}},
  booktitle = {Proceedings of the 8th {{International Seminar}} on {{Speech Production}}},
  author = {Zharkova, N. and Hewlett, N. and Hardcastle, W. J.},
  year = 2008,
  address = {Strasbourg, France}
}

@article{ZharkovaEtAl-UsingUltrasoundTongue-2017,
  title = {Using Ultrasound Tongue Imaging to Identify Covert Contrasts in Children's Speech},
  author = {Zharkova, Natalia and Gibbon, Fiona E. and Lee, Alice},
  year = 2017,
  journal = {Clinical Linguistics \& Phonetics},
  volume = {31},
  number = {1},
  pages = {21--34},
  issn = {1464-5076},
  doi = {10.1080/02699206.2016.1180713},
  abstract = {Ultrasound tongue imaging has become a promising technique for detecting covert contrasts, due to the developments in data analysis methods that allow for processing information on tongue shape from young children. An important feature concerning analyses of ultrasound data from children who are likely to produce covert contrasts is that the data are likely to be collected without head-to-transducer stabilisation, due to the speakers' age. This article is a review of the existing methods applicable in analysing data from non-stabilised recordings. The article describes some of the challenges of ultrasound data collection from children, and analysing these data, as well as possible ways to address those challenges. Additionally, there are examples from typical and disordered productions featuring covert contrasts, with illustrations of quantifying differences in tongue shape between target speech sounds.},
  langid = {english},
  pmid = {27322800},
  keywords = {child,Covert contrasts,Head Movements,Humans,lingual articulation,Speech Acoustics,Speech Disorders,speech production measurement,Speech Production Measurement,subtle differences,Tongue,Ultrasonography,ultrasound},
  file = {/home/jpalo/Zotero/storage/I66SDCRG/Zharkova et al. - 2017 - Using ultrasound tongue imaging to identify covert contrasts in children's speech.pdf}
}

@article{ZharkovaHewlett-MeasuringLingualCoarticulation-2009,
  title = {Measuring Lingual Coarticulation from Midsagittal Tongue Contours: Description and Example Calculations Using {{English}} /t/ and /ɑ/},
  author = {Zharkova, N. and Hewlett, N.},
  year = 2009,
  journal = {Journal of Phonetics},
  volume = {37},
  pages = {248--256}
}
